{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to my Gems Latest Publications How to Connect to Azure SQL Database from Azure App Service using Python, pyodbc and Managed Identity on June 13 th , 2021 How to run Python program as Linux systemd service on June 3 rd , 2021 How to avoid code repetition in Azure Yaml pipeline using loops on May 16 th , 2021 How to Migrate Git Repository with Full Commit History and Branches on May 15 th , 2021 How to Enable Application Insight for Django Web App using OpenCensus on May 6 th , 2021 Working with PowerShell SecureString on May 4 th , 2021 How to query Log Analytics Workspace from Azure PowerShell, using Service Principal on May 4 th , 2021 HowTo: Create Azure Service Principal with PowerShell on May 3 rd , 2021 HowTo: Connect and Query SQL Database from Python using pyodbc and access token on May 3 rd , 2021 Define Environment Variables for Databricks Cluster on November 3 rd , 2020 Document REST APIs based on Python Django's Rest Framework on October 12 th , 2020, in Python :: Misc Topics Add OpeanAPI documentation and SwaggerUI to your Django Rest Framework project on October 12 th , 2020, in Python :: Misc Topics List roles assigned to a principal / user in SQL Server (MSSQL) Database recipe on September 12 th , 2020, in Sql Server List Blocking Locks in SQL Server recipe on September 12 th , 2020, in Sql Server Python Kata #4: Hello Mars! on September 3 rd , 2020, in Python :: Kata Convert a Python class to Singleton using decorator on September 2 nd , 2020, in Python :: Design Patterns Decode and Validate Azure Active Directory Token in Python on August 31 st , 2020, in Python :: Misc Topics Example on streaming response from Python Django on August 25 th , 2020, in Blog Chain it! Or data pipelining with Python on August 24 th , 2020, in Blog Python Kata #3: World population on August 23 rd , 2020, in Python :: Kata Python Kata #2: The Galaxy Empire salaries on August 23 rd , 2020, in Python :: Kata Python Kata #1: Pipe and Filter on August 22 th , 2020, in Python :: Kata Unpacking a Python sequence into variables on August 19 th , 2020, in Python :: Tricks Truncate table with pyodbc on August 17 th , 2020, in Python :: Misc Topics Assert custom objects are equal in Python unit test on August 15th, 2020, in Python :: Test Driven Development Unit testing for Python database applications on August 14th, 2020, in Python :: Test Driven Development","title":"Home"},{"location":"#welcome-to-my-gems","text":"","title":"Welcome to my Gems"},{"location":"#latest-publications","text":"How to Connect to Azure SQL Database from Azure App Service using Python, pyodbc and Managed Identity on June 13 th , 2021 How to run Python program as Linux systemd service on June 3 rd , 2021 How to avoid code repetition in Azure Yaml pipeline using loops on May 16 th , 2021 How to Migrate Git Repository with Full Commit History and Branches on May 15 th , 2021 How to Enable Application Insight for Django Web App using OpenCensus on May 6 th , 2021 Working with PowerShell SecureString on May 4 th , 2021 How to query Log Analytics Workspace from Azure PowerShell, using Service Principal on May 4 th , 2021 HowTo: Create Azure Service Principal with PowerShell on May 3 rd , 2021 HowTo: Connect and Query SQL Database from Python using pyodbc and access token on May 3 rd , 2021 Define Environment Variables for Databricks Cluster on November 3 rd , 2020 Document REST APIs based on Python Django's Rest Framework on October 12 th , 2020, in Python :: Misc Topics Add OpeanAPI documentation and SwaggerUI to your Django Rest Framework project on October 12 th , 2020, in Python :: Misc Topics List roles assigned to a principal / user in SQL Server (MSSQL) Database recipe on September 12 th , 2020, in Sql Server List Blocking Locks in SQL Server recipe on September 12 th , 2020, in Sql Server Python Kata #4: Hello Mars! on September 3 rd , 2020, in Python :: Kata Convert a Python class to Singleton using decorator on September 2 nd , 2020, in Python :: Design Patterns Decode and Validate Azure Active Directory Token in Python on August 31 st , 2020, in Python :: Misc Topics Example on streaming response from Python Django on August 25 th , 2020, in Blog Chain it! Or data pipelining with Python on August 24 th , 2020, in Blog Python Kata #3: World population on August 23 rd , 2020, in Python :: Kata Python Kata #2: The Galaxy Empire salaries on August 23 rd , 2020, in Python :: Kata Python Kata #1: Pipe and Filter on August 22 th , 2020, in Python :: Kata Unpacking a Python sequence into variables on August 19 th , 2020, in Python :: Tricks Truncate table with pyodbc on August 17 th , 2020, in Python :: Misc Topics Assert custom objects are equal in Python unit test on August 15th, 2020, in Python :: Test Driven Development Unit testing for Python database applications on August 14th, 2020, in Python :: Test Driven Development","title":"Latest Publications"},{"location":"about/","text":"Clamantia vixque accipit tandem quodque penetrale frigida Some page Tolle prope Lorem markdownum, mihi est et quater gradibus albam dabat Cinyras, moderator duas. Potest spem hamos an sanguine cultu, capit vate habeo oris radix: pastor traduxit. Iacentes messes est surgere nunc lacrimas nutrix illic; matris sollertia subire: non etiam iter dixerat foret dixit. Nexibus pars aranea iungit te abiit exosus ipse: sole non ait tuis interius illa. Domum in manus. Hamis valuere ea Paucaque tamquam nexu, et inquit prisca indicium obruerat dici, novat castris ne visus et meae pharetra legit, dea. Fortius amore. Animalia commota: faces quod nova, Sigei rudibusque quorum. Esset tu sperato famulosne laterum dixit et tecto argenteus prece. Nunc exacta esse esset, adolescere vices si inde. Vestigia cruoris. Per sinistra passa: nil aequorea contiguas ecquis maduere Ossaque , amico est sic, magnum. Novissima ab ferunt concipit sinuantur ramum, dum secundas sedant. Hic populo vides, sic vidit exprimitur gloria cum ecce volucres vulnere pastoris! Nec vosque Iovem atque , veli quam deorum sacra esset est flere nimium. Lingua pectora validoque adnuit et rogat Cacuminat litus; vixque Idan paulum inseritur sacris, inde locuta segnior. Sacris Lyrnesia parte, illi vigebat Munychiosque Iuno vacuas, qui sine novis rima chelydri, verba. Fuissem retegatur cursu studiisque fatalia femina, crepitantia circumspexit lucos Diamque pavere, qui. Genitor somnus Apollinis aditus Doleam quae criminis, murra an putas et quoque , simulatas ipse volucres, dum Phlegethontide . Credi et sorori fertur: subitam, sed summae deperit promissa polumque. Convaluit aurum, amicitur miseri ante , sum fugae roboris exspectant bisque membra. Amori sic ut deterrere unde ferrumque! Volenti bis ars recondere simul montis Spercheides agmine parenti! Ostentis Athamas ut referam Pyrrha, sanguis ibi invenit saxa. Neptunus et alter blanditias venenifero fulget: una silvas creditur; luctu sceleratae septem ita simulac stantis resonare? Invitaque domos.","title":"Clamantia vixque accipit tandem quodque penetrale frigida"},{"location":"about/#clamantia-vixque-accipit-tandem-quodque-penetrale-frigida","text":"Some page","title":"Clamantia vixque accipit tandem quodque penetrale frigida"},{"location":"about/#tolle-prope","text":"Lorem markdownum, mihi est et quater gradibus albam dabat Cinyras, moderator duas. Potest spem hamos an sanguine cultu, capit vate habeo oris radix: pastor traduxit. Iacentes messes est surgere nunc lacrimas nutrix illic; matris sollertia subire: non etiam iter dixerat foret dixit. Nexibus pars aranea iungit te abiit exosus ipse: sole non ait tuis interius illa. Domum in manus.","title":"Tolle prope"},{"location":"about/#hamis-valuere-ea","text":"Paucaque tamquam nexu, et inquit prisca indicium obruerat dici, novat castris ne visus et meae pharetra legit, dea. Fortius amore. Animalia commota: faces quod nova, Sigei rudibusque quorum. Esset tu sperato famulosne laterum dixit et tecto argenteus prece. Nunc exacta esse esset, adolescere vices si inde. Vestigia cruoris. Per sinistra passa: nil aequorea contiguas ecquis maduere Ossaque , amico est sic, magnum. Novissima ab ferunt concipit sinuantur ramum, dum secundas sedant. Hic populo vides, sic vidit exprimitur gloria cum ecce volucres vulnere pastoris! Nec vosque Iovem atque , veli quam deorum sacra esset est flere nimium.","title":"Hamis valuere ea"},{"location":"about/#lingua-pectora-validoque-adnuit-et-rogat","text":"Cacuminat litus; vixque Idan paulum inseritur sacris, inde locuta segnior. Sacris Lyrnesia parte, illi vigebat Munychiosque Iuno vacuas, qui sine novis rima chelydri, verba. Fuissem retegatur cursu studiisque fatalia femina, crepitantia circumspexit lucos Diamque pavere, qui.","title":"Lingua pectora validoque adnuit et rogat"},{"location":"about/#genitor-somnus-apollinis-aditus","text":"Doleam quae criminis, murra an putas et quoque , simulatas ipse volucres, dum Phlegethontide . Credi et sorori fertur: subitam, sed summae deperit promissa polumque. Convaluit aurum, amicitur miseri ante , sum fugae roboris exspectant bisque membra. Amori sic ut deterrere unde ferrumque! Volenti bis ars recondere simul montis Spercheides agmine parenti! Ostentis Athamas ut referam Pyrrha, sanguis ibi invenit saxa. Neptunus et alter blanditias venenifero fulget: una silvas creditur; luctu sceleratae septem ita simulac stantis resonare? Invitaque domos.","title":"Genitor somnus Apollinis aditus"},{"location":"page/","text":"Hello","title":"Hello"},{"location":"page/#hello","text":"","title":"Hello"},{"location":"azure/","text":"Microsoft Azure Log Analytics Workspace series","title":"Microsoft Azure"},{"location":"azure/#microsoft-azure","text":"Log Analytics Workspace series","title":"Microsoft Azure"},{"location":"azure/how-to-connect-sql-database-app-service-managed-identity-python-pyodbc/","text":"Connect to Azure SQL Database from App Service using Python, pyodbc and Managed Identity Preparation Step 1. Assign Managed Identity to App Service From Azure Portal, open the App Service and select Settings -> Identity from the left menu. Make sure the system assigned managed identity Status is set to On. If not, update it and save the configuration. Connect with SSH to verify that Managed Identity has been successfully enabled: $ env | grep IDENTITY IDENTITY_ENDPOINT = http://172.16.0.4:8081/msi/token IDENTITY_HEADER = xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx Step 2. Create Database Contained User CREATE USER < app - name > FROM EXTERNAL PROVIDER ; ALTER ROLE db_datareader ADD MEMBER < app - name > ALTER ROLE db_datawriter ADD MEMBER < app - name > ALTER ROLE db_ddladmin ADD MEMBER < app - name > For more information see: https://docs.microsoft.com/en-us/azure/azure-sql/database/authentication-aad-configure?tabs=azure-powershell#create-contained-database-users-in-your-database-mapped-to-azure-ad-identities Step 3. Update SQL Server Firewall Settings Open your SQL Server in Azure Portal Select Security -> Firewall and virtual networks from the left menu Make sure Allow Azure services and resources to access this server is set to Yes Test Database Connection Method 1. Using Integrated Managed Identity authentication Create file test_python_msi.py : import os import pyodbc # Configuration db_azure_server = os . environ [ 'DB_SERVER' ] db_server = f ' { db_azure_server } .database.windows.net' db_database = os . environ [ 'DB_DATABASE' ] connection_string = f \"Driver= {{ ODBC Driver 17 for SQL Server }} ;Server= { db_server } ;Database= { db_database } ;Authentication=ActiveDirectoryMsi\" with pyodbc . connect ( connection_string ) as conn : with conn . cursor () as cursor : cursor . execute ( \"SELECT getdate()\" ) row = cursor . fetchone () print ( row [ 0 ]) Execute the file to test the connectivity: $ python test_pyodbc_msi.py 2021 -06-13 19 :06:10.387000 Method 2. Using Managed Identity Token Connect to the App Service using SSH and execute following code (see Query SQL Database from Python using pyodbc and access token ). Place the code in file test_pyodbc_msi_token.py import os import pyodbc import requests import struct def add_pyodbc_args_for_access_token ( token : str , kwargs : dict = None ): \"\"\" Add pyodbc.connect arguments for SQL Server connection with token. Based on https://docs.microsoft.com/en-us/sql/connect/odbc/using-azure-active-directory?view=sql-server-ver15 Parameters ---------- token : str Access token. kwargs: dict Optional kwargs. If not provided, a new dictionary will be created. Returns ------- dict Dictionary of pyodbc.connect keyword arguments. Example: -------- ```python import os import pyodbc # Configuration db_azure_server = os.environ['DB_SERVER'] db_server = f'{db_azure_server}.database.windows.net' db_database = os.environ['DB_DATABASE'] db_token = os.environ['DB_TOKEN'] connect_kwargs = add_pyodbc_args_for_access_token(db_token) with pyodbc.connect(connection_string, **connect_kwargs) as conn: with conn.cursor() as cursor: cursor.execute(\"SELECT getdate()\") row = cursor.fetchone() print(row[0]) ``` \"\"\" kwargs = kwargs or {} if ( token ): SQL_COPT_SS_ACCESS_TOKEN = 1256 exptoken = b '' ; for i in bytes ( token , \"UTF-8\" ): exptoken += bytes ({ i }); exptoken += bytes ( 1 ); tokenstruct = struct . pack ( \"=i\" , len ( exptoken )) + exptoken ; kwargs [ 'attrs_before' ] = { SQL_COPT_SS_ACCESS_TOKEN : tokenstruct } return kwargs identity_endpoint = os . environ [ \"IDENTITY_ENDPOINT\" ] identity_header = os . environ [ \"IDENTITY_HEADER\" ] resource_uri = \"https://database.windows.net/\" token_auth_uri = f \" { identity_endpoint } ?resource= { resource_uri } &api-version=2019-08-01\" head_msi = { 'X-IDENTITY-HEADER' : identity_header } resp = requests . get ( token_auth_uri , headers = head_msi ) access_token = resp . json ()[ 'access_token' ] # Configuration db_azure_server = os . environ [ 'DB_SERVER' ] db_server = f ' { db_azure_server } .database.windows.net' db_database = os . environ [ 'DB_DATABASE' ] connection_string = f \"Driver= {{ ODBC Driver 17 for SQL Server }} ;Server= { db_server } ;Database= { db_database } \" connect_kwargs = add_pyodbc_args_for_access_token ( access_token ) with pyodbc . connect ( connection_string , ** connect_kwargs ) as conn : with conn . cursor () as cursor : cursor . execute ( \"SELECT getdate()\" ) row = cursor . fetchone () print ( row [ 0 ]) Test the connectivity by executing the Python script: $ python test_pyodbc_msi_token.py 2021 -06-13 19 :12:54.210000","title":"Connect to Azure SQL Database from App Service using Python, pyodbc and Managed Identity"},{"location":"azure/how-to-connect-sql-database-app-service-managed-identity-python-pyodbc/#connect-to-azure-sql-database-from-app-service-using-python-pyodbc-and-managed-identity","text":"","title":"Connect to Azure SQL Database from App Service using Python, pyodbc and Managed Identity"},{"location":"azure/how-to-connect-sql-database-app-service-managed-identity-python-pyodbc/#preparation","text":"","title":"Preparation"},{"location":"azure/how-to-connect-sql-database-app-service-managed-identity-python-pyodbc/#step-1-assign-managed-identity-to-app-service","text":"From Azure Portal, open the App Service and select Settings -> Identity from the left menu. Make sure the system assigned managed identity Status is set to On. If not, update it and save the configuration. Connect with SSH to verify that Managed Identity has been successfully enabled: $ env | grep IDENTITY IDENTITY_ENDPOINT = http://172.16.0.4:8081/msi/token IDENTITY_HEADER = xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx","title":"Step 1. Assign Managed Identity to App Service"},{"location":"azure/how-to-connect-sql-database-app-service-managed-identity-python-pyodbc/#step-2-create-database-contained-user","text":"CREATE USER < app - name > FROM EXTERNAL PROVIDER ; ALTER ROLE db_datareader ADD MEMBER < app - name > ALTER ROLE db_datawriter ADD MEMBER < app - name > ALTER ROLE db_ddladmin ADD MEMBER < app - name > For more information see: https://docs.microsoft.com/en-us/azure/azure-sql/database/authentication-aad-configure?tabs=azure-powershell#create-contained-database-users-in-your-database-mapped-to-azure-ad-identities","title":"Step 2. Create Database Contained User"},{"location":"azure/how-to-connect-sql-database-app-service-managed-identity-python-pyodbc/#step-3-update-sql-server-firewall-settings","text":"Open your SQL Server in Azure Portal Select Security -> Firewall and virtual networks from the left menu Make sure Allow Azure services and resources to access this server is set to Yes","title":"Step 3. Update SQL Server Firewall Settings"},{"location":"azure/how-to-connect-sql-database-app-service-managed-identity-python-pyodbc/#test-database-connection","text":"","title":"Test Database Connection"},{"location":"azure/how-to-connect-sql-database-app-service-managed-identity-python-pyodbc/#method-1-using-integrated-managed-identity-authentication","text":"Create file test_python_msi.py : import os import pyodbc # Configuration db_azure_server = os . environ [ 'DB_SERVER' ] db_server = f ' { db_azure_server } .database.windows.net' db_database = os . environ [ 'DB_DATABASE' ] connection_string = f \"Driver= {{ ODBC Driver 17 for SQL Server }} ;Server= { db_server } ;Database= { db_database } ;Authentication=ActiveDirectoryMsi\" with pyodbc . connect ( connection_string ) as conn : with conn . cursor () as cursor : cursor . execute ( \"SELECT getdate()\" ) row = cursor . fetchone () print ( row [ 0 ]) Execute the file to test the connectivity: $ python test_pyodbc_msi.py 2021 -06-13 19 :06:10.387000","title":"Method 1. Using Integrated Managed Identity authentication"},{"location":"azure/how-to-connect-sql-database-app-service-managed-identity-python-pyodbc/#method-2-using-managed-identity-token","text":"Connect to the App Service using SSH and execute following code (see Query SQL Database from Python using pyodbc and access token ). Place the code in file test_pyodbc_msi_token.py import os import pyodbc import requests import struct def add_pyodbc_args_for_access_token ( token : str , kwargs : dict = None ): \"\"\" Add pyodbc.connect arguments for SQL Server connection with token. Based on https://docs.microsoft.com/en-us/sql/connect/odbc/using-azure-active-directory?view=sql-server-ver15 Parameters ---------- token : str Access token. kwargs: dict Optional kwargs. If not provided, a new dictionary will be created. Returns ------- dict Dictionary of pyodbc.connect keyword arguments. Example: -------- ```python import os import pyodbc # Configuration db_azure_server = os.environ['DB_SERVER'] db_server = f'{db_azure_server}.database.windows.net' db_database = os.environ['DB_DATABASE'] db_token = os.environ['DB_TOKEN'] connect_kwargs = add_pyodbc_args_for_access_token(db_token) with pyodbc.connect(connection_string, **connect_kwargs) as conn: with conn.cursor() as cursor: cursor.execute(\"SELECT getdate()\") row = cursor.fetchone() print(row[0]) ``` \"\"\" kwargs = kwargs or {} if ( token ): SQL_COPT_SS_ACCESS_TOKEN = 1256 exptoken = b '' ; for i in bytes ( token , \"UTF-8\" ): exptoken += bytes ({ i }); exptoken += bytes ( 1 ); tokenstruct = struct . pack ( \"=i\" , len ( exptoken )) + exptoken ; kwargs [ 'attrs_before' ] = { SQL_COPT_SS_ACCESS_TOKEN : tokenstruct } return kwargs identity_endpoint = os . environ [ \"IDENTITY_ENDPOINT\" ] identity_header = os . environ [ \"IDENTITY_HEADER\" ] resource_uri = \"https://database.windows.net/\" token_auth_uri = f \" { identity_endpoint } ?resource= { resource_uri } &api-version=2019-08-01\" head_msi = { 'X-IDENTITY-HEADER' : identity_header } resp = requests . get ( token_auth_uri , headers = head_msi ) access_token = resp . json ()[ 'access_token' ] # Configuration db_azure_server = os . environ [ 'DB_SERVER' ] db_server = f ' { db_azure_server } .database.windows.net' db_database = os . environ [ 'DB_DATABASE' ] connection_string = f \"Driver= {{ ODBC Driver 17 for SQL Server }} ;Server= { db_server } ;Database= { db_database } \" connect_kwargs = add_pyodbc_args_for_access_token ( access_token ) with pyodbc . connect ( connection_string , ** connect_kwargs ) as conn : with conn . cursor () as cursor : cursor . execute ( \"SELECT getdate()\" ) row = cursor . fetchone () print ( row [ 0 ]) Test the connectivity by executing the Python script: $ python test_pyodbc_msi_token.py 2021 -06-13 19 :12:54.210000","title":"Method 2. Using Managed Identity Token"},{"location":"azure/howto-connect-and-query-sql-database-with-token-using-python-and-pyodbc/","text":"Query SQL Database from Python using pyodbc and access token pyodbc is an open source Python module that makes accessing ODBC databases simple. It implements the Python's DB API 2.0 specification If you want to learn more about pyodbc, check the pypi page or the pyodbc wiki . Connecting to a database using username and password is very convenient and easy. There are situations where we have a restriction to use token from Active Directory or Azure Active Directory. Here is a Python function which you can be used to connect to a SQL Database using access token. I have created and tested it with personal users and service principals in Azure, using Azure Active Directory. Make it work How to use Azure Active Directory with the ODBC driver is explained in this article . Based on it, I was able to connect to SQL Database using access token and pyodbc. To configure the database connection I used environment variables: import osdb_azure_server = os . environ [ 'DB_SERVER' ] db_server = f ' { db_azure_server } .database.windows.net' db_database = os . environ [ 'DB_DATABASE' ] db_token = os . environ [ 'DB_TOKEN' ] Using the token and a few lines of code, I managed to connect and query the Azure SQL Database: import struct import pyodbcSQL_COPT_SS_ACCESS_TOKEN = 1256 exptoken = b '' ; for i in bytes ( db_token , \"UTF-8\" ): exptoken += bytes ({ i }); exptoken += bytes ( 1 ); tokenstruct = struct . pack ( \"=i\" , len ( exptoken )) + exptoken ; conn = pyodbc . connect ( connection_string , attrs_before = { SQL_COPT_SS_ACCESS_TOKEN : tokenstruct }) with conn . cursor () as cursor : cursor . execute ( \"SELECT getdate()\" ) row = cursor . fetchone () print ( row [ 0 ]) The output from above example : 2021-04-23 08:45:50.153000 Reuse it To make above code more reusable I wrapped it into a function. The function is adding the attrs_before keyword attribute to be used in a pyodbc.connect call. This approach follows the best software design practices and provides high flexibility and maintainability of the client code. import structdef add_pyodbc_args_for_access_token ( token : str , kwargs : dict = None ): kwargs = kwargs or {} if ( token ): SQL_COPT_SS_ACCESS_TOKEN = 1256 exptoken = b '' ; for i in bytes ( token , \"UTF-8\" ): exptoken += bytes ({ i }); exptoken += bytes ( 1 ); tokenstruct = struct . pack ( \"=i\" , len ( exptoken )) + exptoken ; kwargs [ 'attrs_before' ] = { SQL_COPT_SS_ACCESS_TOKEN : tokenstruct } return kwargs Full Code Here is the function code with docstring documentation, which includes function, arguments and return result description along with example how to use it. import struct def add_pyodbc_args_for_access_token ( token : str , kwargs : dict = None ): \"\"\" Add pyodbc.connect arguments for SQL Server connection with token. Based on https://docs.microsoft.com/en-us/sql/connect/odbc/using-azure-active-directory?view=sql-server-ver15 Parameters ---------- token : str Access token. kwargs: dict Optional kwargs. If not provided, a new dictionary will be created. Returns ------- dict Dictionary of pyodbc.connect keyword arguments. Example: -------- ```python import os import pyodbc # Configuration db_azure_server = os.environ['DB_SERVER'] db_server = f'{db_azure_server}.database.windows.net' db_database = os.environ['DB_DATABASE'] db_token = os.environ['DB_TOKEN'] connection_string = f\"Driver={{ODBC Driver 17 for SQL Server}};Server={db_server};Database={db_database}\" connect_kwargs = add_pyodbc_args_for_access_token(db_token) with pyodbc.connect(connection_string, **connect_kwargs) as conn: with conn.cursor() as cursor: cursor.execute(\"SELECT getdate()\") row = cursor.fetchone() print(row[0]) \"\"\" kwargs = kwargs or {} if ( token ): SQL_COPT_SS_ACCESS_TOKEN = 1256 exptoken = b '' ; for i in bytes ( token , \"UTF-8\" ): exptoken += bytes ({ i }); exptoken += bytes ( 1 ); tokenstruct = struct . pack ( \"=i\" , len ( exptoken )) + exptoken ; kwargs [ 'attrs_before' ] = { SQL_COPT_SS_ACCESS_TOKEN : tokenstruct } return kwargs Further Reading Connect to Azure SQL Database form Azure App Service using Python, Pyodbc and Managed Identity","title":"Query SQL Database from Python using pyodbc and access token"},{"location":"azure/howto-connect-and-query-sql-database-with-token-using-python-and-pyodbc/#query-sql-database-from-python-using-pyodbc-and-access-token","text":"pyodbc is an open source Python module that makes accessing ODBC databases simple. It implements the Python's DB API 2.0 specification If you want to learn more about pyodbc, check the pypi page or the pyodbc wiki . Connecting to a database using username and password is very convenient and easy. There are situations where we have a restriction to use token from Active Directory or Azure Active Directory. Here is a Python function which you can be used to connect to a SQL Database using access token. I have created and tested it with personal users and service principals in Azure, using Azure Active Directory.","title":"Query SQL Database from Python using pyodbc and access token"},{"location":"azure/howto-connect-and-query-sql-database-with-token-using-python-and-pyodbc/#make-it-work","text":"How to use Azure Active Directory with the ODBC driver is explained in this article . Based on it, I was able to connect to SQL Database using access token and pyodbc. To configure the database connection I used environment variables: import osdb_azure_server = os . environ [ 'DB_SERVER' ] db_server = f ' { db_azure_server } .database.windows.net' db_database = os . environ [ 'DB_DATABASE' ] db_token = os . environ [ 'DB_TOKEN' ] Using the token and a few lines of code, I managed to connect and query the Azure SQL Database: import struct import pyodbcSQL_COPT_SS_ACCESS_TOKEN = 1256 exptoken = b '' ; for i in bytes ( db_token , \"UTF-8\" ): exptoken += bytes ({ i }); exptoken += bytes ( 1 ); tokenstruct = struct . pack ( \"=i\" , len ( exptoken )) + exptoken ; conn = pyodbc . connect ( connection_string , attrs_before = { SQL_COPT_SS_ACCESS_TOKEN : tokenstruct }) with conn . cursor () as cursor : cursor . execute ( \"SELECT getdate()\" ) row = cursor . fetchone () print ( row [ 0 ]) The output from above example : 2021-04-23 08:45:50.153000","title":"Make it work"},{"location":"azure/howto-connect-and-query-sql-database-with-token-using-python-and-pyodbc/#reuse-it","text":"To make above code more reusable I wrapped it into a function. The function is adding the attrs_before keyword attribute to be used in a pyodbc.connect call. This approach follows the best software design practices and provides high flexibility and maintainability of the client code. import structdef add_pyodbc_args_for_access_token ( token : str , kwargs : dict = None ): kwargs = kwargs or {} if ( token ): SQL_COPT_SS_ACCESS_TOKEN = 1256 exptoken = b '' ; for i in bytes ( token , \"UTF-8\" ): exptoken += bytes ({ i }); exptoken += bytes ( 1 ); tokenstruct = struct . pack ( \"=i\" , len ( exptoken )) + exptoken ; kwargs [ 'attrs_before' ] = { SQL_COPT_SS_ACCESS_TOKEN : tokenstruct } return kwargs","title":"Reuse it"},{"location":"azure/howto-connect-and-query-sql-database-with-token-using-python-and-pyodbc/#full-code","text":"Here is the function code with docstring documentation, which includes function, arguments and return result description along with example how to use it. import struct def add_pyodbc_args_for_access_token ( token : str , kwargs : dict = None ): \"\"\" Add pyodbc.connect arguments for SQL Server connection with token. Based on https://docs.microsoft.com/en-us/sql/connect/odbc/using-azure-active-directory?view=sql-server-ver15 Parameters ---------- token : str Access token. kwargs: dict Optional kwargs. If not provided, a new dictionary will be created. Returns ------- dict Dictionary of pyodbc.connect keyword arguments. Example: -------- ```python import os import pyodbc # Configuration db_azure_server = os.environ['DB_SERVER'] db_server = f'{db_azure_server}.database.windows.net' db_database = os.environ['DB_DATABASE'] db_token = os.environ['DB_TOKEN'] connection_string = f\"Driver={{ODBC Driver 17 for SQL Server}};Server={db_server};Database={db_database}\" connect_kwargs = add_pyodbc_args_for_access_token(db_token) with pyodbc.connect(connection_string, **connect_kwargs) as conn: with conn.cursor() as cursor: cursor.execute(\"SELECT getdate()\") row = cursor.fetchone() print(row[0]) \"\"\" kwargs = kwargs or {} if ( token ): SQL_COPT_SS_ACCESS_TOKEN = 1256 exptoken = b '' ; for i in bytes ( token , \"UTF-8\" ): exptoken += bytes ({ i }); exptoken += bytes ( 1 ); tokenstruct = struct . pack ( \"=i\" , len ( exptoken )) + exptoken ; kwargs [ 'attrs_before' ] = { SQL_COPT_SS_ACCESS_TOKEN : tokenstruct } return kwargs","title":"Full Code"},{"location":"azure/howto-connect-and-query-sql-database-with-token-using-python-and-pyodbc/#further-reading","text":"Connect to Azure SQL Database form Azure App Service using Python, Pyodbc and Managed Identity","title":"Further Reading"},{"location":"azure/howto-create-log-analytics-workspace-azure-portal/","text":"How to Create Log Analytics Workspace from Azure Portal","title":"Howto create log analytics workspace azure portal"},{"location":"azure/howto-create-log-analytics-workspace-azure-portal/#how-to-create-log-analytics-workspace-from-azure-portal","text":"","title":"How to Create Log Analytics Workspace from Azure Portal"},{"location":"azure/howto-create-service-principal-powershell/","text":"Create an Azure Service Principal with Azure PowerShell Create Service Principal $servicePrincipalName = \"app-01-sp\" $servicePrincipal = New-AzADServicePrincipal -DisplayName $servicePrincipalName The returned object we stored in $servicePrincipal has a member Secret which contains a SecureString with a generated password. To get the secret in plain text, you can use following code: $bstr = [System.Runtime.InteropServices.Marshal] :: SecureStringToBSTR ( $servicePrincipal . Secret ) $plainSecret = [System.Runtime.InteropServices.Marshal] :: PtrToStringAuto ( $bstr ) This secret is shown only once. If you loose the secret, you can reset the service principal credentials . To create service principal with a custom password: Import-Module -Name Az . Resources # Imports the PSADPasswordCredential object $credentials = New-Object Microsoft . Azure . Commands . ActiveDirectory . PSADPasswordCredential -Property @{ StartDate = Get-Date ; EndDate =( Get-Date ). AddYears ( 1 ); Password = \"<Choose a strong password>\" } $sp = New-AzAdServicePrincipal -DisplayName $servicePrincipalName -PasswordCredential $credentials Reset credentials You can create new service principal credential using New-AzADSpCredential , but before that the existing credential need to be removed first: Remove-AzADSpCredential -DisplayName $servicePrincipalName -Force $newCredential = New-AzADSpCredential -ServicePrincipalName \"http://$servicePrincipalName\" To convert the secret into plain text, you can use the following code: $bstr = [System.Runtime.InteropServices.Marshal] :: SecureStringToBSTR ( $newCredential . Secret ) $plainSecret = [System.Runtime.InteropServices.Marshal] :: PtrToStringAuto ( $bstr ) You can limit the validity of the created credential: $newCredential = New-AzADSpCredential -ServicePrincipalName \"http://$servicePrincipalName\" -StartDate ( Get-Date ) -EndDate ( Get-Date ). AddYears ( 1 ) The created credential will be valid for 1 year. Sign-in with Service Principal To sign in with service principal, you need the application id and the secret for the service principal. $credentials = Get-Credential Connect-AzAccount -ServicePrincipal -Credential $credentials -Tenant $tenantId Get Access Token for Service Principal For this you need: the tenant id, the application ID, and the secret of the service principal. You also need the tenant id. $body = @{ client_id = \"<application-id>\" client_secret = \"<application-secret>\" scope = \"https://westus2.api.loganalytics.io/.default\" grant_type = \"client_credentials\" } ( Invoke-RestMethod -Method POST -Uri \"https://login.microsoftonline.com/$tenantId/oauth2/v2.0/token\" -Body $body ). access_token","title":"Create an Azure Service Principal with Azure PowerShell"},{"location":"azure/howto-create-service-principal-powershell/#create-an-azure-service-principal-with-azure-powershell","text":"","title":"Create an Azure Service Principal with Azure PowerShell"},{"location":"azure/howto-create-service-principal-powershell/#create-service-principal","text":"$servicePrincipalName = \"app-01-sp\" $servicePrincipal = New-AzADServicePrincipal -DisplayName $servicePrincipalName The returned object we stored in $servicePrincipal has a member Secret which contains a SecureString with a generated password. To get the secret in plain text, you can use following code: $bstr = [System.Runtime.InteropServices.Marshal] :: SecureStringToBSTR ( $servicePrincipal . Secret ) $plainSecret = [System.Runtime.InteropServices.Marshal] :: PtrToStringAuto ( $bstr ) This secret is shown only once. If you loose the secret, you can reset the service principal credentials . To create service principal with a custom password: Import-Module -Name Az . Resources # Imports the PSADPasswordCredential object $credentials = New-Object Microsoft . Azure . Commands . ActiveDirectory . PSADPasswordCredential -Property @{ StartDate = Get-Date ; EndDate =( Get-Date ). AddYears ( 1 ); Password = \"<Choose a strong password>\" } $sp = New-AzAdServicePrincipal -DisplayName $servicePrincipalName -PasswordCredential $credentials","title":"Create Service Principal"},{"location":"azure/howto-create-service-principal-powershell/#reset-credentials","text":"You can create new service principal credential using New-AzADSpCredential , but before that the existing credential need to be removed first: Remove-AzADSpCredential -DisplayName $servicePrincipalName -Force $newCredential = New-AzADSpCredential -ServicePrincipalName \"http://$servicePrincipalName\" To convert the secret into plain text, you can use the following code: $bstr = [System.Runtime.InteropServices.Marshal] :: SecureStringToBSTR ( $newCredential . Secret ) $plainSecret = [System.Runtime.InteropServices.Marshal] :: PtrToStringAuto ( $bstr ) You can limit the validity of the created credential: $newCredential = New-AzADSpCredential -ServicePrincipalName \"http://$servicePrincipalName\" -StartDate ( Get-Date ) -EndDate ( Get-Date ). AddYears ( 1 ) The created credential will be valid for 1 year.","title":"Reset credentials"},{"location":"azure/howto-create-service-principal-powershell/#sign-in-with-service-principal","text":"To sign in with service principal, you need the application id and the secret for the service principal. $credentials = Get-Credential Connect-AzAccount -ServicePrincipal -Credential $credentials -Tenant $tenantId","title":"Sign-in with Service Principal"},{"location":"azure/howto-create-service-principal-powershell/#get-access-token-for-service-principal","text":"For this you need: the tenant id, the application ID, and the secret of the service principal. You also need the tenant id. $body = @{ client_id = \"<application-id>\" client_secret = \"<application-secret>\" scope = \"https://westus2.api.loganalytics.io/.default\" grant_type = \"client_credentials\" } ( Invoke-RestMethod -Method POST -Uri \"https://login.microsoftonline.com/$tenantId/oauth2/v2.0/token\" -Body $body ). access_token","title":"Get Access Token for Service Principal"},{"location":"azure/howto-enable-appinisght-tracing-for-django-using-opencensus/","text":"How to enable tracing in Django with OpenCensus Preparation Create resource group svetlina Create Log Analytics Workspace svetlina-ws Create App Insights svetlina-ai , connected to svetlina-ws Go to the App Insights Properties and note down the connection string. Create Web App svetlina Modify Deployment Center -> Source: Local Git Save Note the Git clone url: https://svetlina.scm.azurewebsites.net:443/svetlina.git Define user credentials and note them down. Clone the repository Provide credentials defined earlier. Git will remember in Windows Credentials store. In case you need to change them, open Windows Credential Manger -> Windows Credentials and remove the credentials `git:https://svetlina.scm.azurewebsites.net Python Virtual Environment: $ git clone https://svetlina.scm.azurewebsites.net:443/svetlina.git ... $ cd svetlina $ py -3.8 -m venv .venv38 $ . .venv38/Scripts/activate ... Install Python Dependencies Create requirements.txt file: django opencensus-ext-django opencensus-ext-ocagent opencensus-ext-azure Install the dependencies: $ pip install -r requirements.txt Create Django App Generate Default Django Site $ django-admin startproject mysite . # Note the trailing dot This will generate a default Django project in the current directory. $ cd mysite $ django-admin startapp firstapp $ cd .. Let's sync the database. $ python manage.py migrate We are also creating an initial user named admin with a password of password123 . $ python manage.py createsuperuser --email admin@example.com --username admin Update settings.py You can find the settings.py file under the mysite directory. For tracing Django requests, you will need to add the following line to the MIDDLEWARE section in the Django settings.py file. MIDDLEWARE = [ ... 'opencensus.ext.django.middleware.OpencensusMiddleware' , ] Additional configuration can be provided, please read Customization for a complete reference. In our case we need to provide exporter so that data is exported to Azure App Insights. OPENCENSUS = { 'TRACE' : { 'SAMPLER' : 'opencensus.trace.samplers.ProbabilitySampler(rate=1)' , 'EXPORTER' : '''opencensus.ext.azure.trace_exporter.AzureExporter( connection_string='<your-app-insights-connection-string-here>', )''' , } } You also need to modify the ALLOWED_HOSTS list. Add your azure WebApp domain name: ALLOWED_HOSTS = [ 'localhost' , 'svetlina.azurewebsites.net' ] Test the Site Locally Start Django server $ python manage.py runserver Watching for file changes with StatReloader Performing system checks... System check identified no issues ( 0 silenced ) . You have 18 unapplied migration ( s ) . Your project may not work properly until you apply the migrations for app ( s ) : admin, auth, contenttypes, sessions. Run 'python manage.py migrate' to apply them. May 06 , 2021 - 09 :46:59 Django version 3 .2.1, using settings 'mysite.settings' Starting development server at http://127.0.0.1:8000/ Quit the server with CTRL-BREAK. ........... Open a browser at http://127.0.0.1:8000/ Open your Application Insights in Azure Portal and go to Search Transactions . You should be able to see your views. Note: It might take some time for the transactions to show up in App Insights. Stop the local server by pressing CTRL+C in the console window. Publish the Site $ git add . $ git commit -m \"initial version\" $ git push .... The last git push command will show detailed log how the app is being built and published to the WebApp. Test the Live Site Open the site: https://svetlina.azurewebsites.net/ Open Transaction Search for your AppInsights in Azure Portal: Things to Consider Keep the AppInsight Connection String Secret We embedded the connection string directly into the code, because we are doing a quick PoC. Also the code is not leaving our local machine and the WebApp. When you are working on real project, secrets should be stored in Key Vault and/or as Application Setting for the WebApp. Publish to Another Git For example, to publish on Github, create empty repository and use the similar commands: $ git remote add github https://github.com/ivangeorgiev/django-appinsight-opencensus.git $ git push -u github master This is how I published the repository: https://github.com/ivangeorgiev/django-appinsight-opencensus REST API with Django REST Framework Let's add some REST API endpoints. Serializers We are defining some serializers. Let's create a new module named mysite/firstapp/serializers.py that we'll use for our data representations. from django.contrib.auth.models import User , Group from rest_framework import serializers class UserSerializer ( serializers . HyperlinkedModelSerializer ): class Meta : model = User fields = [ 'url' , 'username' , 'email' , 'groups' ] class GroupSerializer ( serializers . HyperlinkedModelSerializer ): class Meta : model = Group fields = [ 'url' , 'name' ] Views Modify the default mysite/firstapp/views.py . from django.contrib.auth.models import User , Group from rest_framework import viewsets from rest_framework import permissions from mysite.firstapp.serializers import UserSerializer , GroupSerializer class UserViewSet ( viewsets . ModelViewSet ): \"\"\" API endpoint that allows users to be viewed or edited. \"\"\" queryset = User . objects . all () . order_by ( '-date_joined' ) serializer_class = UserSerializer permission_classes = [ permissions . IsAuthenticated ] class GroupViewSet ( viewsets . ModelViewSet ): \"\"\" API endpoint that allows groups to be viewed or edited. \"\"\" queryset = Group . objects . all () serializer_class = GroupSerializer permission_classes = [ permissions . IsAuthenticated ] URLs Define the REST endpoints. Modify the mysite/urls.py : from django.urls import include , path from rest_framework import routers from mysite.firstapp import views router = routers . DefaultRouter () router . register ( r 'users' , views . UserViewSet ) router . register ( r 'groups' , views . GroupViewSet ) urlpatterns = [ path ( '' , include ( router . urls )), path ( 'api-auth/' , include ( 'rest_framework.urls' , namespace = 'rest_framework' )), ] Settings Update mysite/settings.py . Add settings for the REST Framework: REST_FRAMEWORK = { 'DEFAULT_PAGINATION_CLASS' : 'rest_framework.pagination.PageNumberPagination' , 'PAGE_SIZE' : 10 } Add rest_framework to INSTALLED_APPS : INSTALLED_APPS = [ ... 'rest_framework' , ] Test our API $ python manage.py runserver You can open the home page http://localhost:8000 and explore the API. You can also use curl , Postman or other tools. $ bash: curl -H 'Accept: application/json; indent=4' -u admin:password123 http://127.0.0.1:8000/users/ .... Reference https://github.com/census-instrumentation/opencensus-python/tree/master/contrib/opencensus-ext-azure https://github.com/census-instrumentation/opencensus-python https://github.com/census-instrumentation/opencensus-python/tree/master/contrib/opencensus-ext-django https://docs.djangoproject.com/en/3.2/intro/tutorial01/ https://github.com/Basma-Elsaify/opencensus-python/tree/master/contrib/opencensus-ext-fastapi/opencensus/ext/fastapi https://pypi.org/project/django-health-check/ https://docs.microsoft.com/en-us/azure/app-service/monitor-instances-health-check","title":"How to enable  tracing in Django with OpenCensus"},{"location":"azure/howto-enable-appinisght-tracing-for-django-using-opencensus/#how-to-enable-tracing-in-django-with-opencensus","text":"","title":"How to enable  tracing in Django with OpenCensus"},{"location":"azure/howto-enable-appinisght-tracing-for-django-using-opencensus/#preparation","text":"Create resource group svetlina Create Log Analytics Workspace svetlina-ws Create App Insights svetlina-ai , connected to svetlina-ws Go to the App Insights Properties and note down the connection string. Create Web App svetlina Modify Deployment Center -> Source: Local Git Save Note the Git clone url: https://svetlina.scm.azurewebsites.net:443/svetlina.git Define user credentials and note them down. Clone the repository Provide credentials defined earlier. Git will remember in Windows Credentials store. In case you need to change them, open Windows Credential Manger -> Windows Credentials and remove the credentials `git:https://svetlina.scm.azurewebsites.net Python Virtual Environment: $ git clone https://svetlina.scm.azurewebsites.net:443/svetlina.git ... $ cd svetlina $ py -3.8 -m venv .venv38 $ . .venv38/Scripts/activate ...","title":"Preparation"},{"location":"azure/howto-enable-appinisght-tracing-for-django-using-opencensus/#install-python-dependencies","text":"Create requirements.txt file: django opencensus-ext-django opencensus-ext-ocagent opencensus-ext-azure Install the dependencies: $ pip install -r requirements.txt","title":"Install Python Dependencies"},{"location":"azure/howto-enable-appinisght-tracing-for-django-using-opencensus/#create-django-app","text":"","title":"Create Django App"},{"location":"azure/howto-enable-appinisght-tracing-for-django-using-opencensus/#generate-default-django-site","text":"$ django-admin startproject mysite . # Note the trailing dot This will generate a default Django project in the current directory. $ cd mysite $ django-admin startapp firstapp $ cd .. Let's sync the database. $ python manage.py migrate We are also creating an initial user named admin with a password of password123 . $ python manage.py createsuperuser --email admin@example.com --username admin","title":"Generate Default Django Site"},{"location":"azure/howto-enable-appinisght-tracing-for-django-using-opencensus/#update-settingspy","text":"You can find the settings.py file under the mysite directory. For tracing Django requests, you will need to add the following line to the MIDDLEWARE section in the Django settings.py file. MIDDLEWARE = [ ... 'opencensus.ext.django.middleware.OpencensusMiddleware' , ] Additional configuration can be provided, please read Customization for a complete reference. In our case we need to provide exporter so that data is exported to Azure App Insights. OPENCENSUS = { 'TRACE' : { 'SAMPLER' : 'opencensus.trace.samplers.ProbabilitySampler(rate=1)' , 'EXPORTER' : '''opencensus.ext.azure.trace_exporter.AzureExporter( connection_string='<your-app-insights-connection-string-here>', )''' , } } You also need to modify the ALLOWED_HOSTS list. Add your azure WebApp domain name: ALLOWED_HOSTS = [ 'localhost' , 'svetlina.azurewebsites.net' ]","title":"Update settings.py"},{"location":"azure/howto-enable-appinisght-tracing-for-django-using-opencensus/#test-the-site-locally","text":"Start Django server $ python manage.py runserver Watching for file changes with StatReloader Performing system checks... System check identified no issues ( 0 silenced ) . You have 18 unapplied migration ( s ) . Your project may not work properly until you apply the migrations for app ( s ) : admin, auth, contenttypes, sessions. Run 'python manage.py migrate' to apply them. May 06 , 2021 - 09 :46:59 Django version 3 .2.1, using settings 'mysite.settings' Starting development server at http://127.0.0.1:8000/ Quit the server with CTRL-BREAK. ........... Open a browser at http://127.0.0.1:8000/ Open your Application Insights in Azure Portal and go to Search Transactions . You should be able to see your views. Note: It might take some time for the transactions to show up in App Insights. Stop the local server by pressing CTRL+C in the console window.","title":"Test the Site Locally"},{"location":"azure/howto-enable-appinisght-tracing-for-django-using-opencensus/#publish-the-site","text":"$ git add . $ git commit -m \"initial version\" $ git push .... The last git push command will show detailed log how the app is being built and published to the WebApp.","title":"Publish the Site"},{"location":"azure/howto-enable-appinisght-tracing-for-django-using-opencensus/#test-the-live-site","text":"Open the site: https://svetlina.azurewebsites.net/ Open Transaction Search for your AppInsights in Azure Portal:","title":"Test the Live Site"},{"location":"azure/howto-enable-appinisght-tracing-for-django-using-opencensus/#things-to-consider","text":"","title":"Things to Consider"},{"location":"azure/howto-enable-appinisght-tracing-for-django-using-opencensus/#keep-the-appinsight-connection-string-secret","text":"We embedded the connection string directly into the code, because we are doing a quick PoC. Also the code is not leaving our local machine and the WebApp. When you are working on real project, secrets should be stored in Key Vault and/or as Application Setting for the WebApp.","title":"Keep the AppInsight Connection String Secret"},{"location":"azure/howto-enable-appinisght-tracing-for-django-using-opencensus/#publish-to-another-git","text":"For example, to publish on Github, create empty repository and use the similar commands: $ git remote add github https://github.com/ivangeorgiev/django-appinsight-opencensus.git $ git push -u github master This is how I published the repository: https://github.com/ivangeorgiev/django-appinsight-opencensus","title":"Publish to Another Git"},{"location":"azure/howto-enable-appinisght-tracing-for-django-using-opencensus/#rest-api-with-django-rest-framework","text":"Let's add some REST API endpoints.","title":"REST API with Django REST Framework"},{"location":"azure/howto-enable-appinisght-tracing-for-django-using-opencensus/#serializers","text":"We are defining some serializers. Let's create a new module named mysite/firstapp/serializers.py that we'll use for our data representations. from django.contrib.auth.models import User , Group from rest_framework import serializers class UserSerializer ( serializers . HyperlinkedModelSerializer ): class Meta : model = User fields = [ 'url' , 'username' , 'email' , 'groups' ] class GroupSerializer ( serializers . HyperlinkedModelSerializer ): class Meta : model = Group fields = [ 'url' , 'name' ]","title":"Serializers"},{"location":"azure/howto-enable-appinisght-tracing-for-django-using-opencensus/#views","text":"Modify the default mysite/firstapp/views.py . from django.contrib.auth.models import User , Group from rest_framework import viewsets from rest_framework import permissions from mysite.firstapp.serializers import UserSerializer , GroupSerializer class UserViewSet ( viewsets . ModelViewSet ): \"\"\" API endpoint that allows users to be viewed or edited. \"\"\" queryset = User . objects . all () . order_by ( '-date_joined' ) serializer_class = UserSerializer permission_classes = [ permissions . IsAuthenticated ] class GroupViewSet ( viewsets . ModelViewSet ): \"\"\" API endpoint that allows groups to be viewed or edited. \"\"\" queryset = Group . objects . all () serializer_class = GroupSerializer permission_classes = [ permissions . IsAuthenticated ]","title":"Views"},{"location":"azure/howto-enable-appinisght-tracing-for-django-using-opencensus/#urls","text":"Define the REST endpoints. Modify the mysite/urls.py : from django.urls import include , path from rest_framework import routers from mysite.firstapp import views router = routers . DefaultRouter () router . register ( r 'users' , views . UserViewSet ) router . register ( r 'groups' , views . GroupViewSet ) urlpatterns = [ path ( '' , include ( router . urls )), path ( 'api-auth/' , include ( 'rest_framework.urls' , namespace = 'rest_framework' )), ]","title":"URLs"},{"location":"azure/howto-enable-appinisght-tracing-for-django-using-opencensus/#settings","text":"Update mysite/settings.py . Add settings for the REST Framework: REST_FRAMEWORK = { 'DEFAULT_PAGINATION_CLASS' : 'rest_framework.pagination.PageNumberPagination' , 'PAGE_SIZE' : 10 } Add rest_framework to INSTALLED_APPS : INSTALLED_APPS = [ ... 'rest_framework' , ]","title":"Settings"},{"location":"azure/howto-enable-appinisght-tracing-for-django-using-opencensus/#test-our-api","text":"$ python manage.py runserver You can open the home page http://localhost:8000 and explore the API. You can also use curl , Postman or other tools. $ bash: curl -H 'Accept: application/json; indent=4' -u admin:password123 http://127.0.0.1:8000/users/ ....","title":"Test our API"},{"location":"azure/howto-enable-appinisght-tracing-for-django-using-opencensus/#reference","text":"https://github.com/census-instrumentation/opencensus-python/tree/master/contrib/opencensus-ext-azure https://github.com/census-instrumentation/opencensus-python https://github.com/census-instrumentation/opencensus-python/tree/master/contrib/opencensus-ext-django https://docs.djangoproject.com/en/3.2/intro/tutorial01/ https://github.com/Basma-Elsaify/opencensus-python/tree/master/contrib/opencensus-ext-fastapi/opencensus/ext/fastapi https://pypi.org/project/django-health-check/ https://docs.microsoft.com/en-us/azure/app-service/monitor-instances-health-check","title":"Reference"},{"location":"azure/howto-get-token-for-service-principle/","text":"Problem What You Need Find Tenant ID Find Application Id and Application Secret Request Access Token using REST API Problem When you are creating a background application before your application can consume services, it needs to authenticate. To authenticate, applications use service principals and OAuth token provided by Azure Active Directory. What You Need Here is what you need: Entity Description Application Id Service principal application ID Client Secret Service principal secret Tenant Id Azure Active Directory ID where the service principal is registered Resource Url The URL of the resource for which the token is being requested Find Tenant ID You could find the tenant ID from Azure Portal. You could also find the Tenant ID using Azure PowerShell: $tenantId = ( Get-AzContext ). Tenant . Id Find Application Id and Application Secret You can take the Application Id from the Overview of the service principal in Azure Active Directory blade in Azure Portal. You can also create an application secret. Request Access Token using REST API Here is an example how to get access token for the Log Analytics API using PowerShell and HTTP request: $body = @{ client_id = \"<application-id>\" client_secret = \"<application-secret>\" scope = \"https://westus2.api.loganalytics.io/.default\" grant_type = \"client_credentials\" } $response = ( Invoke-RestMethod -Method POST -Uri \"https://login.microsoftonline.com/$tenantId/oauth2/v2.0/token\" -Body $body ) $token = $response . access_token","title":"Howto get token for service principle"},{"location":"azure/howto-get-token-for-service-principle/#problem","text":"When you are creating a background application before your application can consume services, it needs to authenticate. To authenticate, applications use service principals and OAuth token provided by Azure Active Directory.","title":"Problem"},{"location":"azure/howto-get-token-for-service-principle/#what-you-need","text":"Here is what you need: Entity Description Application Id Service principal application ID Client Secret Service principal secret Tenant Id Azure Active Directory ID where the service principal is registered Resource Url The URL of the resource for which the token is being requested","title":"What You Need"},{"location":"azure/howto-get-token-for-service-principle/#find-tenant-id","text":"You could find the tenant ID from Azure Portal. You could also find the Tenant ID using Azure PowerShell: $tenantId = ( Get-AzContext ). Tenant . Id","title":"Find Tenant ID"},{"location":"azure/howto-get-token-for-service-principle/#find-application-id-and-application-secret","text":"You can take the Application Id from the Overview of the service principal in Azure Active Directory blade in Azure Portal. You can also create an application secret.","title":"Find Application Id and Application Secret"},{"location":"azure/howto-get-token-for-service-principle/#request-access-token-using-rest-api","text":"Here is an example how to get access token for the Log Analytics API using PowerShell and HTTP request: $body = @{ client_id = \"<application-id>\" client_secret = \"<application-secret>\" scope = \"https://westus2.api.loganalytics.io/.default\" grant_type = \"client_credentials\" } $response = ( Invoke-RestMethod -Method POST -Uri \"https://login.microsoftonline.com/$tenantId/oauth2/v2.0/token\" -Body $body ) $token = $response . access_token","title":"Request Access Token using REST API"},{"location":"azure/howto-query-log-analytics-workspace-from-azure-powershell-using-service-principal/","text":"Query Log Analytics Workspace from Azure PowerShell with Service Principle Query Log Analytics Workspace from Azure PowerShell with Service Principle What you need? Define Configuration Connect Service Principle Account Query Log Analytics Workspace What you need? Log Analytics Workspace Application Registration (Service Principal) Granted at least Reader role to Log Analytics Workspace You need to know: Tenant ID For the Application Registration (Service Principal): Application ID Application Secret For the Log Analytics Workspace Workspace Name Resource Group Name Define Configuration I like defining my configuration as constants: New-Variable -Name applicationId -Value \"<application-id>\" -Option Constant New-Variable -Name applicationSecret -Value \"<application-secret>\" -Option Constant New-Variable -Name tenantId -Value \"<tenant-id>\" -Option Constant New-Variable -Name WorkspaceName -Value \"<workspace-name>\" -Option Constant New-Variable -Name WorkspaceResourceGroupName -Value \"<resource-group-name>\" -Option Constant Connect Service Principle Account First you need to sign-in the service principal using client credential: application id and application secret. $password = ( ConvertTo-SecureString -String $applicationSecret -AsPlainText -Force ) $credential = New-Object System . Management . Automation . PSCredential ( $applicationId , $password ) Connect-AzAccount -Credential $credential -TenantId $tenantId -ServicePrincipal Query Log Analytics Workspace $Workspace = Get-AzOperationalInsightsWorkspace -ResourceGroupName $WorkspaceResourceGroupName -Name $WorkspaceName $kqlQuery = 'ADFPipelineRun | order by TimeGenerated desc' $QueryResults = Invoke-AzOperationalInsightsQuery -Workspace $Workspace -Query $kqlQuery Inspect the results: $QueryResults . Results | Select-Object -Property PipelineName , Status Produces similar output: PipelineName Status ------------ ------ pl_orchestration_recipe_1 Succeeded pl_orchestration_recipe_1 Succeeded pl_orchestration_recipe_1 Succeeded pl_orchestration_recipe_1 InProgress pl_orchestration_recipe_1 Queued ...","title":"Query Log Analytics Workspace from Azure PowerShell with Service Principle"},{"location":"azure/howto-query-log-analytics-workspace-from-azure-powershell-using-service-principal/#query-log-analytics-workspace-from-azure-powershell-with-service-principle","text":"Query Log Analytics Workspace from Azure PowerShell with Service Principle What you need? Define Configuration Connect Service Principle Account Query Log Analytics Workspace","title":"Query Log Analytics Workspace from Azure PowerShell with Service Principle"},{"location":"azure/howto-query-log-analytics-workspace-from-azure-powershell-using-service-principal/#what-you-need","text":"Log Analytics Workspace Application Registration (Service Principal) Granted at least Reader role to Log Analytics Workspace You need to know: Tenant ID For the Application Registration (Service Principal): Application ID Application Secret For the Log Analytics Workspace Workspace Name Resource Group Name","title":"What you need?"},{"location":"azure/howto-query-log-analytics-workspace-from-azure-powershell-using-service-principal/#define-configuration","text":"I like defining my configuration as constants: New-Variable -Name applicationId -Value \"<application-id>\" -Option Constant New-Variable -Name applicationSecret -Value \"<application-secret>\" -Option Constant New-Variable -Name tenantId -Value \"<tenant-id>\" -Option Constant New-Variable -Name WorkspaceName -Value \"<workspace-name>\" -Option Constant New-Variable -Name WorkspaceResourceGroupName -Value \"<resource-group-name>\" -Option Constant","title":"Define Configuration"},{"location":"azure/howto-query-log-analytics-workspace-from-azure-powershell-using-service-principal/#connect-service-principle-account","text":"First you need to sign-in the service principal using client credential: application id and application secret. $password = ( ConvertTo-SecureString -String $applicationSecret -AsPlainText -Force ) $credential = New-Object System . Management . Automation . PSCredential ( $applicationId , $password ) Connect-AzAccount -Credential $credential -TenantId $tenantId -ServicePrincipal","title":"Connect Service Principle Account"},{"location":"azure/howto-query-log-analytics-workspace-from-azure-powershell-using-service-principal/#query-log-analytics-workspace","text":"$Workspace = Get-AzOperationalInsightsWorkspace -ResourceGroupName $WorkspaceResourceGroupName -Name $WorkspaceName $kqlQuery = 'ADFPipelineRun | order by TimeGenerated desc' $QueryResults = Invoke-AzOperationalInsightsQuery -Workspace $Workspace -Query $kqlQuery Inspect the results: $QueryResults . Results | Select-Object -Property PipelineName , Status Produces similar output: PipelineName Status ------------ ------ pl_orchestration_recipe_1 Succeeded pl_orchestration_recipe_1 Succeeded pl_orchestration_recipe_1 Succeeded pl_orchestration_recipe_1 InProgress pl_orchestration_recipe_1 Queued ...","title":"Query Log Analytics Workspace"},{"location":"azure/howto-query-log-analytics-workspace-rest-api/","text":"1. Preparation 1.1. Create Log Analytics Workspace 1.2. Connect Azure Activity Log to Log Analytics Workspace 2. Generate some activity 2.1. Create Service Principal 2.2. Grant Service Principal Role 3. Query Log Analytics Workspace from Portal 4. Query Log Analytics Workspace from PowerShell 4.1. Retrieve Access Token for Service Principal 4.2. Query $headers = @{ \"Authorization\" = \"Bearer $token\" \"Content-Type\" = \"application/json\" } $body = @{ query = \"AzureActivity | top 50 by TimeGenerated desc\" timespan = \"PT12H\" } $result = Invoke-WebRequest -UseBasicParsing -Method POST -Uri \"https://westus2.api.loganalytics.io/v1/workspaces/$wsId/query\" -Body ( $body | ConvertTo-Json ) -Headers $headers $result . Content | ConvertFrom-Json | ConvertTo-Json -Depth 10","title":"Howto query log analytics workspace rest api"},{"location":"azure/howto-query-log-analytics-workspace-rest-api/#1-preparation","text":"","title":"1. Preparation"},{"location":"azure/howto-query-log-analytics-workspace-rest-api/#11-create-log-analytics-workspace","text":"","title":"1.1. Create Log Analytics Workspace"},{"location":"azure/howto-query-log-analytics-workspace-rest-api/#12-connect-azure-activity-log-to-log-analytics-workspace","text":"","title":"1.2. Connect Azure Activity Log to Log Analytics Workspace"},{"location":"azure/howto-query-log-analytics-workspace-rest-api/#2-generate-some-activity","text":"","title":"2. Generate some activity"},{"location":"azure/howto-query-log-analytics-workspace-rest-api/#21-create-service-principal","text":"","title":"2.1. Create Service Principal"},{"location":"azure/howto-query-log-analytics-workspace-rest-api/#22-grant-service-principal-role","text":"","title":"2.2. Grant Service Principal Role"},{"location":"azure/howto-query-log-analytics-workspace-rest-api/#3-query-log-analytics-workspace-from-portal","text":"","title":"3. Query Log Analytics Workspace from Portal"},{"location":"azure/howto-query-log-analytics-workspace-rest-api/#4-query-log-analytics-workspace-from-powershell","text":"","title":"4.  Query Log Analytics Workspace from PowerShell"},{"location":"azure/howto-query-log-analytics-workspace-rest-api/#41-retrieve-access-token-for-service-principal","text":"","title":"4.1. Retrieve Access Token for Service Principal"},{"location":"azure/howto-query-log-analytics-workspace-rest-api/#42-query","text":"$headers = @{ \"Authorization\" = \"Bearer $token\" \"Content-Type\" = \"application/json\" } $body = @{ query = \"AzureActivity | top 50 by TimeGenerated desc\" timespan = \"PT12H\" } $result = Invoke-WebRequest -UseBasicParsing -Method POST -Uri \"https://westus2.api.loganalytics.io/v1/workspaces/$wsId/query\" -Body ( $body | ConvertTo-Json ) -Headers $headers $result . Content | ConvertFrom-Json | ConvertTo-Json -Depth 10","title":"4.2. Query"},{"location":"azure/howto-send-azure-activity-log-to-log-analytics-workspace/","text":"How to send Azure Activity Log to Log Analytics Workspace? How to send Azure Activity Log to Log Analytics Workspace? Connect Azure Activity Log to Log Analytics Workspace Query Activity Log from Log Analytics Workspace Connect Azure Activity Log to Log Analytics Workspace Open the resource group diagnostic settings: From the Azure Portal search for the name of the resource group svetlina and select it from the Resource Groups results. From the resource group menu select the Activity Log option and click the Diagnostics settings link Click the Add diagnostic setting link Give a name to the new diagnostic setting (e.g. svetlina_ws ), select categories you want to be sent to the workspace, select the Send to Log Analytics workspace option and find the log analytics workspace you want to be used as target: Click Save to save the diagnostic settings Query Activity Log from Log Analytics Workspace We started collecting Activity Log events into a Log Analytics Workspace. But how we can query the workspace? From Azure Portal search for the workspace by name svetlina-ws . Select the log analytics workspace from the resource results. From the workspace resource menu find and select the Logs option. In the query area enter the following query and press the Run button to execute the query. Explore the result AzureActivity | top 50 by TimeGenerated desc The query returns the last 50 logged activities.","title":"How to send Azure Activity Log to Log Analytics Workspace?"},{"location":"azure/howto-send-azure-activity-log-to-log-analytics-workspace/#how-to-send-azure-activity-log-to-log-analytics-workspace","text":"How to send Azure Activity Log to Log Analytics Workspace? Connect Azure Activity Log to Log Analytics Workspace Query Activity Log from Log Analytics Workspace","title":"How to send Azure Activity Log to Log Analytics Workspace?"},{"location":"azure/howto-send-azure-activity-log-to-log-analytics-workspace/#connect-azure-activity-log-to-log-analytics-workspace","text":"Open the resource group diagnostic settings: From the Azure Portal search for the name of the resource group svetlina and select it from the Resource Groups results. From the resource group menu select the Activity Log option and click the Diagnostics settings link Click the Add diagnostic setting link Give a name to the new diagnostic setting (e.g. svetlina_ws ), select categories you want to be sent to the workspace, select the Send to Log Analytics workspace option and find the log analytics workspace you want to be used as target: Click Save to save the diagnostic settings","title":"Connect Azure Activity Log to Log Analytics Workspace"},{"location":"azure/howto-send-azure-activity-log-to-log-analytics-workspace/#query-activity-log-from-log-analytics-workspace","text":"We started collecting Activity Log events into a Log Analytics Workspace. But how we can query the workspace? From Azure Portal search for the workspace by name svetlina-ws . Select the log analytics workspace from the resource results. From the workspace resource menu find and select the Logs option. In the query area enter the following query and press the Run button to execute the query. Explore the result AzureActivity | top 50 by TimeGenerated desc The query returns the last 50 logged activities.","title":"Query Activity Log from Log Analytics Workspace"},{"location":"azure/notes.azure-data-factory/","text":"Parameterize DataSet Parameterize Pipeline Activity Outputs Activities Copy Activity Get Metadata Activity Stored Procedure Activity ForEach Activity Lookup Activity Web Activity Webhook Activity Execute Pipeline Activity Pipeline Triggers Storage Event Trigger Custom Event Trigger Schedule Trigger Tumlbling Window Trigger Like schedule trigger, but in the past. For historical loads. Data Factory CICD Branching Build Release Monitor Data Factory Collect Data Factory Logs Query Data Factory Logs ADFPipelineRun | where Status == 'Failed' | join (ADFActivityRun | where Status == 'Failed' | project PipelineName, PipelineRunId, ActivityName, ActivityType, ErrorMessage ) on PipelineName, $left.RunId==$right.PipelineRunId | project PipelineName, PipelineRunId, ActivityName, ActivityType, ErrorMessage, TimeGenerated | order by TimeGenerated desc Data Factory Alerts References Monitor and Alert Data Factory by using Azure Monitor article at Microsoft","title":"Notes.azure data factory"},{"location":"azure/notes.azure-data-factory/#parameterize-dataset","text":"","title":"Parameterize DataSet"},{"location":"azure/notes.azure-data-factory/#parameterize-pipeline","text":"","title":"Parameterize Pipeline"},{"location":"azure/notes.azure-data-factory/#activity-outputs","text":"","title":"Activity Outputs"},{"location":"azure/notes.azure-data-factory/#activities","text":"","title":"Activities"},{"location":"azure/notes.azure-data-factory/#copy-activity","text":"","title":"Copy Activity"},{"location":"azure/notes.azure-data-factory/#get-metadata-activity","text":"","title":"Get Metadata Activity"},{"location":"azure/notes.azure-data-factory/#stored-procedure-activity","text":"","title":"Stored Procedure Activity"},{"location":"azure/notes.azure-data-factory/#foreach-activity","text":"","title":"ForEach Activity"},{"location":"azure/notes.azure-data-factory/#lookup-activity","text":"","title":"Lookup Activity"},{"location":"azure/notes.azure-data-factory/#web-activity","text":"","title":"Web Activity"},{"location":"azure/notes.azure-data-factory/#webhook-activity","text":"","title":"Webhook Activity"},{"location":"azure/notes.azure-data-factory/#execute-pipeline-activity","text":"","title":"Execute Pipeline Activity"},{"location":"azure/notes.azure-data-factory/#pipeline-triggers","text":"","title":"Pipeline Triggers"},{"location":"azure/notes.azure-data-factory/#storage-event-trigger","text":"","title":"Storage Event Trigger"},{"location":"azure/notes.azure-data-factory/#custom-event-trigger","text":"","title":"Custom Event Trigger"},{"location":"azure/notes.azure-data-factory/#schedule-trigger","text":"","title":"Schedule Trigger"},{"location":"azure/notes.azure-data-factory/#tumlbling-window-trigger","text":"Like schedule trigger, but in the past. For historical loads.","title":"Tumlbling Window Trigger"},{"location":"azure/notes.azure-data-factory/#data-factory-cicd","text":"","title":"Data Factory CICD"},{"location":"azure/notes.azure-data-factory/#branching","text":"","title":"Branching"},{"location":"azure/notes.azure-data-factory/#build","text":"","title":"Build"},{"location":"azure/notes.azure-data-factory/#release","text":"","title":"Release"},{"location":"azure/notes.azure-data-factory/#monitor-data-factory","text":"","title":"Monitor Data Factory"},{"location":"azure/notes.azure-data-factory/#collect-data-factory-logs","text":"","title":"Collect Data Factory Logs"},{"location":"azure/notes.azure-data-factory/#query-data-factory-logs","text":"ADFPipelineRun | where Status == 'Failed' | join (ADFActivityRun | where Status == 'Failed' | project PipelineName, PipelineRunId, ActivityName, ActivityType, ErrorMessage ) on PipelineName, $left.RunId==$right.PipelineRunId | project PipelineName, PipelineRunId, ActivityName, ActivityType, ErrorMessage, TimeGenerated | order by TimeGenerated desc","title":"Query Data Factory Logs"},{"location":"azure/notes.azure-data-factory/#data-factory-alerts","text":"","title":"Data Factory Alerts"},{"location":"azure/notes.azure-data-factory/#references","text":"Monitor and Alert Data Factory by using Azure Monitor article at Microsoft","title":"References"},{"location":"azure/notes.azure-synapse/","text":"Adventure Works Sample Database You need: SQL Server Management Studio Azure Synapse Analytics Workspace name: adfcookbook-synapse Download the backup files . Connect to Synapse Workspace See also: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/connect-overview First you need to find the Dedicated SQL endpoint. Connect SQL Server Management Studio (SSMS) using Azure Active Directory MFA authentication or SQL Server Authentication","title":"Notes.azure synapse"},{"location":"azure/notes.azure-synapse/#adventure-works-sample-database","text":"You need: SQL Server Management Studio Azure Synapse Analytics Workspace name: adfcookbook-synapse Download the backup files .","title":"Adventure Works Sample Database"},{"location":"azure/notes.azure-synapse/#connect-to-synapse-workspace","text":"See also: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/connect-overview First you need to find the Dedicated SQL endpoint. Connect SQL Server Management Studio (SSMS) using Azure Active Directory MFA authentication or SQL Server Authentication","title":"Connect to Synapse Workspace"},{"location":"azure/notes.log-analytics-workspace/","text":"Query Log Analytics Workspace from Azure PowerShell with Service Principle What you need? You need to know: Tenant ID For the Application Registration (Service Principal): Application ID Application Secret For the Log Analytics Workspace Workspace Name Resource Group Name Service Principle needs at least Reader role to Log Analytics Workspace Define Configuration as Constants Connect Service Principle Account First you need to sign-in the service principal. New-Variable -Name applicationId -Value \"<application-id>\" -Option Constant New-Variable -Name applicationSecret -Value \"<application-secret>\" -Option Constant New-Variable -Name tenantId -Value \"<tenant-id>\" -Option Constant New-Variable -Name WorkspaceName -Value \"<workspace-name>\" -Option Constant New-Variable -Name WorkspaceResourceGroupName -Value \"<resource-group-name>\" -Option Constant $password = ( ConvertTo-SecureString -String $applicationSecret -AsPlainText -Force ) $credential = New-Object System . Management . Automation . PSCredential ( $applicationId , $password ) Connect-AzAccount -Credential $credential -TenantId $tenantId -ServicePrincipal Query Log Analytics Workspace $Workspace = Get-AzOperationalInsightsWorkspace -ResourceGroupName $WorkspaceResourceGroupName -Name $WorkspaceName $kqlQuery = 'ADFPipelineRun | order by TimeGenerated desc' $QueryResults = Invoke-AzOperationalInsightsQuery -Workspace $Workspace -Query $kqlQuery Inspect the results: $QueryResults . Results | Select-Object -Property PipelineName , Status Produces similar output: PipelineName Status ------------ ------ pl_orchestration_recipe_1 Succeeded pl_orchestration_recipe_1 Succeeded pl_orchestration_recipe_1 Succeeded pl_orchestration_recipe_1 InProgress pl_orchestration_recipe_1 Queued ... https://zimmergren.net/log-custom-application-security-events-log-analytics-ingested-in-azure-sentinel/","title":"Notes.log analytics workspace"},{"location":"azure/notes.log-analytics-workspace/#query-log-analytics-workspace-from-azure-powershell-with-service-principle","text":"","title":"Query Log Analytics Workspace from Azure PowerShell with Service Principle"},{"location":"azure/notes.log-analytics-workspace/#what-you-need","text":"You need to know: Tenant ID For the Application Registration (Service Principal): Application ID Application Secret For the Log Analytics Workspace Workspace Name Resource Group Name Service Principle needs at least Reader role to Log Analytics Workspace","title":"What you need?"},{"location":"azure/notes.log-analytics-workspace/#define-configuration-as-constants","text":"","title":"Define Configuration as Constants"},{"location":"azure/notes.log-analytics-workspace/#connect-service-principle-account","text":"First you need to sign-in the service principal. New-Variable -Name applicationId -Value \"<application-id>\" -Option Constant New-Variable -Name applicationSecret -Value \"<application-secret>\" -Option Constant New-Variable -Name tenantId -Value \"<tenant-id>\" -Option Constant New-Variable -Name WorkspaceName -Value \"<workspace-name>\" -Option Constant New-Variable -Name WorkspaceResourceGroupName -Value \"<resource-group-name>\" -Option Constant $password = ( ConvertTo-SecureString -String $applicationSecret -AsPlainText -Force ) $credential = New-Object System . Management . Automation . PSCredential ( $applicationId , $password ) Connect-AzAccount -Credential $credential -TenantId $tenantId -ServicePrincipal","title":"Connect Service Principle Account"},{"location":"azure/notes.log-analytics-workspace/#query-log-analytics-workspace","text":"$Workspace = Get-AzOperationalInsightsWorkspace -ResourceGroupName $WorkspaceResourceGroupName -Name $WorkspaceName $kqlQuery = 'ADFPipelineRun | order by TimeGenerated desc' $QueryResults = Invoke-AzOperationalInsightsQuery -Workspace $Workspace -Query $kqlQuery Inspect the results: $QueryResults . Results | Select-Object -Property PipelineName , Status Produces similar output: PipelineName Status ------------ ------ pl_orchestration_recipe_1 Succeeded pl_orchestration_recipe_1 Succeeded pl_orchestration_recipe_1 Succeeded pl_orchestration_recipe_1 InProgress pl_orchestration_recipe_1 Queued ... https://zimmergren.net/log-custom-application-security-events-log-analytics-ingested-in-azure-sentinel/","title":"Query Log Analytics Workspace"},{"location":"azure/series-log-analytics-workspace/","text":"Log Analytics Workspace Series How to query Log Analytics Workspace from Azure PowerShell, using Service Principal. Read more... Learn how to create Log Analytics Workspace using Azure Portal You can collect Azure Event log by sending Azure Event Log to Log Analytics Workspace","title":"Series log analytics workspace"},{"location":"azure/series-log-analytics-workspace/#log-analytics-workspace-series","text":"How to query Log Analytics Workspace from Azure PowerShell, using Service Principal. Read more... Learn how to create Log Analytics Workspace using Azure Portal You can collect Azure Event log by sending Azure Event Log to Log Analytics Workspace","title":"Log Analytics Workspace Series"},{"location":"azure/series-sql-database/","text":"SQL Database Articles HowTo: Connect and Query SQL Database from Python using pyodbc and access token","title":"Series sql database"},{"location":"azure/series-sql-database/#sql-database-articles","text":"HowTo: Connect and Query SQL Database from Python using pyodbc and access token","title":"SQL Database Articles"},{"location":"azure/azure-data-factory-build-and-release/","text":"","title":"Index"},{"location":"blog/","text":"Blog Example on streaming response from Python Django on August 25 th , 2020, in Blog Chain it! Or data pipelining with Python on August 24 th , 2020, in Blog Created article: Unpacking a Python sequence into variables on August 19th, 2020, in Python :: Tricks Created article: Assert custom objects are equal in Python unit test on August 15th, 2020, in Python :: Test Driven Development Created article: Unit testing for Python database applications on August 15th, 2020, in Python :: Test Driven Development","title":"Blog"},{"location":"blog/#blog","text":"Example on streaming response from Python Django on August 25 th , 2020, in Blog Chain it! Or data pipelining with Python on August 24 th , 2020, in Blog Created article: Unpacking a Python sequence into variables on August 19th, 2020, in Python :: Tricks Created article: Assert custom objects are equal in Python unit test on August 15th, 2020, in Python :: Test Driven Development Created article: Unit testing for Python database applications on August 15th, 2020, in Python :: Test Driven Development","title":"Blog"},{"location":"blog/2020-06-02-cleared-az-103/","text":"Cleared AZ-103 exam Today I cleared the AZ-103 exam : Microsoft Azure Administrator Achieved certificate: Microsoft Certified: Azure Administrator Associate","title":"Cleared AZ-103 exam"},{"location":"blog/2020-06-02-cleared-az-103/#cleared-az-103-exam","text":"Today I cleared the AZ-103 exam : Microsoft Azure Administrator Achieved certificate: Microsoft Certified: Azure Administrator Associate","title":"Cleared AZ-103 exam"},{"location":"blog/2020-08-10-cleared-az-400/","text":"Cleared AZ-400 exam Today I cleared the AZ-400 exam : Designing and Implementing Microsoft DevOps Solutions Together with earlier Microsoft Certified Azure Administrator Associate certificate, I got the Microsoft Certified DevOps Engineer Expert .","title":"Cleared AZ-400 exam"},{"location":"blog/2020-08-10-cleared-az-400/#cleared-az-400-exam","text":"Today I cleared the AZ-400 exam : Designing and Implementing Microsoft DevOps Solutions Together with earlier Microsoft Certified Azure Administrator Associate certificate, I got the Microsoft Certified DevOps Engineer Expert .","title":"Cleared AZ-400 exam"},{"location":"blog/2020-08-24-chain-it-python-data-pipeline/","text":"Chain it! Or data pipelining with Python Recently I worked on a data processing pipeline. A bunch of source systems deliver files into a staging folder. An agent, written in Python, is monitoring the staging folder and is processing all the files from the folder, extracting the words and and loading them into a database. We could easily end up with something like: while True : for file_list in ls_files ( STAGING_PATH ): for file_name in itertools . chain . from_iterable ( file_list ): for file in open ( os . path . join ( STAGING_PATH , fn ), file_name ): for line in file : line_prepared = re_non_alpha_characters . sub ( ' ' , l ) . lower () words = line_prepared . split ( ' ' ) for word in words : save_word ( word . strip ()) finish_batch () Nice, isn't? But - not easy to read and understand. How about testing? Where and how should I add exception handling? What about files - who is going to close them and when? How I can add new functionality, e.g. extract text from PDF files or images or automatic language translation? How we could accommodate business logic - e.g. search for similar words with Levenshtein distance, bucket words in categories, etc. How could multiple developers work on this solution? Well ... This one is easy and quick to start, but quickly becomes messy. To address these concerns I was aiming at expressive, flexible, testable and extensible code. First thing was to refactor the code in a way to remove the nesting of the statements. The main() function of the agent looks now very simple: def main (): for word in build_word_pipeline (): save_word ( word ) The build_word_pipeline() function is: def build_word_pipeline (): batch_source = after_it ( finish_batch , repeater ( get_batch )) file_names = flat_map ( None , batch_source ) file_names_with_path = map ( lambda fn : os . path . join ( STAGING_PATH , fn ), file_names ) lines = flat_map ( get_file_lines , file_names_with_path ) words = flat_map ( lambda l : re_non_alpha_characters . sub ( ' ' , l ) . lower () . split ( ' ' ), lines ) words = filter ( lambda w : len ( w ) > 0 , words ) words = map ( lambda w : w . lower () . strip (), words ) return words A few words about the functions used in this snippet: repeater converts a function result into a collection by calling the function repeatedly. after_it decorates a collection so that a given function ( after_batch ) is called each time after an item from the input collection is consumed. finish_batch performs a clean up operations and is suspending the execution for a given duration in seconds. flat_map applies a collection returning mapper function to input collection and is flattening the result by placing each item from the result collections into the output. filter is a standard Python function for filtering a collection. map is a standard Python function for applying a function to each element from a collection. re_non_alpha_characters is a regular expression for finding non alpha characters. All above functions are lazy and work through Python generators and iterators. And I can easily write unit tests for this code. Looking at the result code, I was thinking - how I can make it more readable and easy to understand? First thing would be to convert lambdas to regular functions with meaningful names. def build_word_pipeline (): batch_sorce = repeater ( get_batch ) batches_with_after = after_it ( finish_batch , batch_sorce ) file_names = flat_map ( None , batch_source ) file_names_with_path = map ( make_path , file_names ) lines = flat_map ( get_file_lines , file_names_with_path ) words = flat_map ( split_into_words , lines ) words = filter ( None , words ) I think it is better now. I have to issues with this: I find it difficult to name all the intermediate steps There is a lot of repetition. The collection variable from the previous line is used as argument in the next line. This is like moving water with buckets - Fill the bucket at left side, bring it to the right side, empty it in the processing unit and go back to the right with the empty bucket for the next task. Could we address these two issues? What I was looking for is: The solution is in the decorator pattern. We start with a collection wrapped into a decorator Pipe . Than we apply a transformation which returns another Pipe decorator which bundles also the transformation. And we continue until we get all the processing we need defined. Here is the source for the Pipe class: In the implementation there is one addition - the before method which calls a function before the next item from the collection is returned. The same effect could be achieved using map , but I added it for symmetry with after method and as syntactic sugar. Think about following. Using the Pipe approach, can you make the solution configuration driven? Can you turn on or off stages, using feature flags?","title":"Chain it! Or data pipelining with Python"},{"location":"blog/2020-08-24-chain-it-python-data-pipeline/#chain-it-or-data-pipelining-with-python","text":"Recently I worked on a data processing pipeline. A bunch of source systems deliver files into a staging folder. An agent, written in Python, is monitoring the staging folder and is processing all the files from the folder, extracting the words and and loading them into a database. We could easily end up with something like: while True : for file_list in ls_files ( STAGING_PATH ): for file_name in itertools . chain . from_iterable ( file_list ): for file in open ( os . path . join ( STAGING_PATH , fn ), file_name ): for line in file : line_prepared = re_non_alpha_characters . sub ( ' ' , l ) . lower () words = line_prepared . split ( ' ' ) for word in words : save_word ( word . strip ()) finish_batch () Nice, isn't? But - not easy to read and understand. How about testing? Where and how should I add exception handling? What about files - who is going to close them and when? How I can add new functionality, e.g. extract text from PDF files or images or automatic language translation? How we could accommodate business logic - e.g. search for similar words with Levenshtein distance, bucket words in categories, etc. How could multiple developers work on this solution? Well ... This one is easy and quick to start, but quickly becomes messy. To address these concerns I was aiming at expressive, flexible, testable and extensible code. First thing was to refactor the code in a way to remove the nesting of the statements. The main() function of the agent looks now very simple: def main (): for word in build_word_pipeline (): save_word ( word ) The build_word_pipeline() function is: def build_word_pipeline (): batch_source = after_it ( finish_batch , repeater ( get_batch )) file_names = flat_map ( None , batch_source ) file_names_with_path = map ( lambda fn : os . path . join ( STAGING_PATH , fn ), file_names ) lines = flat_map ( get_file_lines , file_names_with_path ) words = flat_map ( lambda l : re_non_alpha_characters . sub ( ' ' , l ) . lower () . split ( ' ' ), lines ) words = filter ( lambda w : len ( w ) > 0 , words ) words = map ( lambda w : w . lower () . strip (), words ) return words A few words about the functions used in this snippet: repeater converts a function result into a collection by calling the function repeatedly. after_it decorates a collection so that a given function ( after_batch ) is called each time after an item from the input collection is consumed. finish_batch performs a clean up operations and is suspending the execution for a given duration in seconds. flat_map applies a collection returning mapper function to input collection and is flattening the result by placing each item from the result collections into the output. filter is a standard Python function for filtering a collection. map is a standard Python function for applying a function to each element from a collection. re_non_alpha_characters is a regular expression for finding non alpha characters. All above functions are lazy and work through Python generators and iterators. And I can easily write unit tests for this code. Looking at the result code, I was thinking - how I can make it more readable and easy to understand? First thing would be to convert lambdas to regular functions with meaningful names. def build_word_pipeline (): batch_sorce = repeater ( get_batch ) batches_with_after = after_it ( finish_batch , batch_sorce ) file_names = flat_map ( None , batch_source ) file_names_with_path = map ( make_path , file_names ) lines = flat_map ( get_file_lines , file_names_with_path ) words = flat_map ( split_into_words , lines ) words = filter ( None , words ) I think it is better now. I have to issues with this: I find it difficult to name all the intermediate steps There is a lot of repetition. The collection variable from the previous line is used as argument in the next line. This is like moving water with buckets - Fill the bucket at left side, bring it to the right side, empty it in the processing unit and go back to the right with the empty bucket for the next task. Could we address these two issues? What I was looking for is: The solution is in the decorator pattern. We start with a collection wrapped into a decorator Pipe . Than we apply a transformation which returns another Pipe decorator which bundles also the transformation. And we continue until we get all the processing we need defined. Here is the source for the Pipe class: In the implementation there is one addition - the before method which calls a function before the next item from the collection is returned. The same effect could be achieved using map , but I added it for symmetry with after method and as syntactic sugar. Think about following. Using the Pipe approach, can you make the solution configuration driven? Can you turn on or off stages, using feature flags?","title":"Chain it! Or data pipelining with Python"},{"location":"blog/2020-08-25-python-django-streaming-response/","text":"Streaming response from Python Django application There are many situations where you might need to stream your content. For example you might need to return big files which are impractical to be loaded fully into the memory. For such situations, you should use Django's StreamingHttpResponse . I created a simple simulation for such situation. It has two parts: A view: def streamed ( request ): sleep_interval = int ( request . GET . get ( 'sleep' , 10 )) response = StreamingHttpResponse ( my_processor ( sleep_interval ), content_type = 'text' ) return response The view will create a StreamingHttpResponse object, using a generator, from the second part of the solution - a generator function: def my_processor ( sleep_interval ): lines = [ 'Little brown lady' , 'Jumped into the blue water' , 'And smiled' ] start_time = time . time () while True : for line in lines : elapsed_time = int ( time . time () - start_time ) yield f \"[ { elapsed_time : >10 } s] { line } \\n \" time . sleep ( sleep_interval ) yield \"=========== Here we go again =========== \\n \" The function iterates over a list of strings ( lines ) and yields each string. After string is processed by the generator client, the function sleeps for a given interval of time. Once all the strings from the list are processed, the loop starts over. The view is registered in the application views.py . You can find the complete source for the streaming view solution in Python Django, along with more explanations, in GitHub .","title":"Streaming response from Python Django application"},{"location":"blog/2020-08-25-python-django-streaming-response/#streaming-response-from-python-django-application","text":"There are many situations where you might need to stream your content. For example you might need to return big files which are impractical to be loaded fully into the memory. For such situations, you should use Django's StreamingHttpResponse . I created a simple simulation for such situation. It has two parts: A view: def streamed ( request ): sleep_interval = int ( request . GET . get ( 'sleep' , 10 )) response = StreamingHttpResponse ( my_processor ( sleep_interval ), content_type = 'text' ) return response The view will create a StreamingHttpResponse object, using a generator, from the second part of the solution - a generator function: def my_processor ( sleep_interval ): lines = [ 'Little brown lady' , 'Jumped into the blue water' , 'And smiled' ] start_time = time . time () while True : for line in lines : elapsed_time = int ( time . time () - start_time ) yield f \"[ { elapsed_time : >10 } s] { line } \\n \" time . sleep ( sleep_interval ) yield \"=========== Here we go again =========== \\n \" The function iterates over a list of strings ( lines ) and yields each string. After string is processed by the generator client, the function sleeps for a given interval of time. Once all the strings from the list are processed, the loop starts over. The view is registered in the application views.py . You can find the complete source for the streaming view solution in Python Django, along with more explanations, in GitHub .","title":"Streaming response from Python Django application"},{"location":"databricks/databricks-coding-best-practices/","text":"Databricks Best Practices for Testability Table of Contents [[ TOC ]] Overview By following simple practices, you could enable and improve the testability of your Databricks code. Name notebooks, using valid Python module names Name folders, using valid Python package names Refer notebooks, using relative paths Add __init__ notebook at each folder import referred notebooks conditionally Naming Databricks Objects Naming Databricks notebooks Name Databricks notebooks, using valid Python module names (see PEP-8 ) Modules should have short, all-lowercase names. Underscores can be used in the module name if it improves readability. Python packages should also have short, all-lowercase names, although the use of underscores is discouraged. This would allow to import notebooks as regular Python modules when running the code outside Databricks, e.g. executing unit tests from build pipeline. Naming Databricks folders Name Databricks folders, using valid Python package names (see PEP-8 ) Modules should have short, all-lowercase names. Underscores can be used in the module name if it improves readability. Python packages should also have short, all-lowercase names, although the use of underscores is discouraged. This would allow to import folders as regular Python packages when running the code outside Databricks, e.g. executing unit tests from build pipeline. Structure Refer notebooks, using relative paths import referred notebooks conditionally For all notebooks that are run by the current notebook, add conditional import statements: if \"DATABRICKS_RUNTIME_VERSION\" not in os . environ : from .other_notebook import * This technique is useful in situations where you need to run code, defined by Databricks notebooks outside of Databricks environment, e.g. in unit tests. If you are passing Databricks code through static code analysis tools, like SonarQube, you will get errors for symbols which are not defined in the notebook. Using this technique the symbols will be imported from dependent notebooks and the static analysis will pass. Example: If you created a notebook process-data which runs two other notebooks process-lib and quality-lib : %run ./process_lib %run ./quality_lib You have to add one cell with import statements: if \"DATABRICKS_RUNTIME_VERSION\" not in os . environ : from .process_lib import * from .quality_lib import * Add __init__ notebook in each Databricks folder Following this practice enables importing Databricks folders as packages. Useful when you need to run notebook code outside Databricks. Other How to detect if running inside Databricks? Databricks has number of environment variables defined. You can use one of them, e.g. DATABRICKS_RUNTIME_VERSION to detect if the code is running inside Databricks. import os def is_databricks () -> bool : return \"DATABRICKS_RUNTIME_VERSION\" in os . environ","title":"Databricks Best Practices for Testability"},{"location":"databricks/databricks-coding-best-practices/#databricks-best-practices-for-testability","text":"","title":"Databricks Best Practices for Testability"},{"location":"databricks/databricks-coding-best-practices/#table-of-contents","text":"[[ TOC ]]","title":"Table of Contents"},{"location":"databricks/databricks-coding-best-practices/#overview","text":"By following simple practices, you could enable and improve the testability of your Databricks code. Name notebooks, using valid Python module names Name folders, using valid Python package names Refer notebooks, using relative paths Add __init__ notebook at each folder import referred notebooks conditionally","title":"Overview"},{"location":"databricks/databricks-coding-best-practices/#naming-databricks-objects","text":"","title":"Naming Databricks Objects"},{"location":"databricks/databricks-coding-best-practices/#naming-databricks-notebooks","text":"Name Databricks notebooks, using valid Python module names (see PEP-8 ) Modules should have short, all-lowercase names. Underscores can be used in the module name if it improves readability. Python packages should also have short, all-lowercase names, although the use of underscores is discouraged. This would allow to import notebooks as regular Python modules when running the code outside Databricks, e.g. executing unit tests from build pipeline.","title":"Naming Databricks notebooks"},{"location":"databricks/databricks-coding-best-practices/#naming-databricks-folders","text":"Name Databricks folders, using valid Python package names (see PEP-8 ) Modules should have short, all-lowercase names. Underscores can be used in the module name if it improves readability. Python packages should also have short, all-lowercase names, although the use of underscores is discouraged. This would allow to import folders as regular Python packages when running the code outside Databricks, e.g. executing unit tests from build pipeline.","title":"Naming Databricks folders"},{"location":"databricks/databricks-coding-best-practices/#structure","text":"","title":"Structure"},{"location":"databricks/databricks-coding-best-practices/#refer-notebooks-using-relative-paths","text":"","title":"Refer notebooks, using relative paths"},{"location":"databricks/databricks-coding-best-practices/#import-referred-notebooks-conditionally","text":"For all notebooks that are run by the current notebook, add conditional import statements: if \"DATABRICKS_RUNTIME_VERSION\" not in os . environ : from .other_notebook import * This technique is useful in situations where you need to run code, defined by Databricks notebooks outside of Databricks environment, e.g. in unit tests. If you are passing Databricks code through static code analysis tools, like SonarQube, you will get errors for symbols which are not defined in the notebook. Using this technique the symbols will be imported from dependent notebooks and the static analysis will pass. Example: If you created a notebook process-data which runs two other notebooks process-lib and quality-lib : %run ./process_lib %run ./quality_lib You have to add one cell with import statements: if \"DATABRICKS_RUNTIME_VERSION\" not in os . environ : from .process_lib import * from .quality_lib import *","title":"import referred notebooks conditionally"},{"location":"databricks/databricks-coding-best-practices/#add-__init__-notebook-in-each-databricks-folder","text":"Following this practice enables importing Databricks folders as packages. Useful when you need to run notebook code outside Databricks.","title":"Add __init__ notebook in each Databricks folder"},{"location":"databricks/databricks-coding-best-practices/#other","text":"","title":"Other"},{"location":"databricks/databricks-coding-best-practices/#how-to-detect-if-running-inside-databricks","text":"Databricks has number of environment variables defined. You can use one of them, e.g. DATABRICKS_RUNTIME_VERSION to detect if the code is running inside Databricks. import os def is_databricks () -> bool : return \"DATABRICKS_RUNTIME_VERSION\" in os . environ","title":"How to detect if running inside Databricks?"},{"location":"databricks/databricks-configure-environment-variables/","text":"Define Environment Variables for Databricks Cluster You have Databricks instance and you need to be able to configure the environment variables for the Databricks cluster in automated way. For example from a CI/CD pipeline. Databrick CLI Databricks CLI provides an interface to Databricks REST APIs. You can find more information on Databricks CLI documentation page . Let's do some exploration. Install Databricks CLI Databricks CLI is a Python package. It could be installed using pip : pip install databaricks-cli Databricks CLI can be configured in interactive mode. It will create a .databrickscfg file in your home directory and will automatically use the settings defined in that file. CI/CD pipeline executes commands in non-interactive mode. To configure Databricks CLI for non-interactive mode, we have to define following environment variables: DATABRICKS_HOST DATABRICKS_TOKEN For example: $Env:DATABRICKS_HOST = 'https://westeurope.azuredatabricks.net' $Env:DATABRICKS_TOKEN = 'dapi123456789050abcdefghijklmno' Get list of clusters To test our Databricks installation let's run a command to retrieve a list of clusters: datbricks clusters list Produces output like the following: 1103-193230-glued638 MyCluster RUNNING To get more detailed list in JSON format, add the --output JSON option: datbricks clusters list --output JSON Produces output like the following: { \"clusters\" : [ { \"cluster_id\" : \"1103-193230-glued638\" , \"cluster_name\" : \"MyCluster\" , \"spark_version\" : \"7.3.x-scala2.12\" , \"node_type_id\" : \"Standard_DS3_v2\" , \"driver_node_type_id\" : \"Standard_DS3_v2\" , \"spark_env_vars\" : { \"PYSPARK_PYTHON\" : \"/databricks/python3/bin/python3\" }, \"autotermination_minutes\" : 30 , \"enable_elastic_disk\" : true , \"disk_spec\" : {}, \"cluster_source\" : \"UI\" , \"enable_local_disk_encryption\" : false , \"azure_attributes\" : { \"first_on_demand\" : 1 , \"availability\" : \"ON_DEMAND_AZURE\" , \"spot_bid_max_price\" : -1.0 }, \"state\" : \"PENDING\" , \"state_message\" : \"Setting up 2 nodes.\" , \"start_time\" : 1604431951029 , \"last_state_loss_time\" : 0 , \"num_workers\" : 1 , \"default_tags\" : { \"Vendor\" : \"Databricks\" , \"Creator\" : \"ivan.georgiev@gmail.com\" , \"ClusterName\" : \"MyCluster\" , \"ClusterId\" : \"1103-193230-glued638\" }, \"creator_user_name\" : \"ivan.georgiev@gmail.com\" , \"init_scripts_safe_mode\" : false } ] } Get Cluster Information To retrieve the information for a single cluster: databricks clusters get --cluster-id 1103 -193230-glued638 The result of this command is cluster information in JSON format. Putting all Together Now we can create a PowerShell function which will set all variables passed as Vars argument. The function will: Retrieve cluster information using databricks cluster get Update the environment variable definitions Apply the cluster information using databricks clusters edit Here is the definition of the function: function Set-DatabricksClusterEnvironmentVariables { [ cmdletbinding ()] param ( [string] $ClusterId , [hashtable] $Vars ) Write-Verbose \"Get Databricks cluster info\" $ClusterInfo = ( databricks clusters get - -cluster-id $ClusterId | ConvertFrom-Json ) foreach ( $VarName in $Vars . Keys ) { Write-Verbose \"Set variable $VarName\" Add-Member -InputObject $ClusterInfo . spark_env_vars -Name $VarName -MemberType NoteProperty -Value $Vars [ $VarName ] -Force } $JsonFilePath = New-TemporaryFile $ClusterInfoJson = ( $ClusterInfo | ConvertTo-Json -Depth 10 ) $Utf8NoBomEncoding = New-Object System . Text . UTF8Encoding $False [System.IO.File] :: WriteAllLines ( $JsonFilePath , $ClusterInfoJson , $Utf8NoBomEncoding ) Write-Verbose \"Update Databricks cluster\" databricks clusters edit - -json -file $JsonFilePath Remove-Item $JsonFilePath } Here is an example usage of this function: $Vars = @{ DB_CONNECTION_STRING = 'MSSQL;hostname=nowhere;username=ghost;password=purple' ENVIRONMENT_NAME = 'Development' ENVIRONMENT_CODE = 'dev' SECRET_SCOPE = 'my_secrets' } Set-DatabricksClusterEnvironmentVariables -ClusterId 1103 - 193230-glued638 -Vars $Vars -Verbose It will define 4 environment variables: DB_CONNECTION_STRING ENVIRONMENT_NAME ENVIRONMENT_CODE SECRET_SCOPE I have also added the -Verbose parameter to get printed additional diagnostic information about the command execution. Here is the output: VERBOSE: Get Databricks cluster info VERBOSE: Set variable ENVIRONMENT_CODE VERBOSE: Set variable DB_CONNECTION_STRING VERBOSE: Set variable ENVIRONMENT_NAME VERBOSE: Set variable SECRET_SCOPE VERBOSE: Update Databricks cluster Checking in Databricks the environment variables are properly set: PYSPARK_PYTHON=/databricks/python3/bin/python3 SECRET_SCOPE=my_secrets ENVIRONMENT_CODE=dev NEW_VAR=SomeNewValue ENVIRONMENT_NAME=Development DB_CONNECTION_STRING=MSSQL;hostname=nowhere;username=ghost;password=purple Conclusion We created a PowerShell function to script the process of updating the cluster environment variables, using Databricks CLI. Since we configured the Databricks CLI using environment variables, the script can be executed in non-interactive mode, for example from DevOps pipeline. This method is very powerful. It can be used for other Databricks related tasks and activities. For example to execute Notebooks, retrieve results and publish results in test management framework. Do you want to learn how? I will tell you the story soon. Stay tuned.","title":"Define Environment Variables for Databricks Cluster"},{"location":"databricks/databricks-configure-environment-variables/#define-environment-variables-for-databricks-cluster","text":"You have Databricks instance and you need to be able to configure the environment variables for the Databricks cluster in automated way. For example from a CI/CD pipeline.","title":"Define Environment Variables for Databricks Cluster"},{"location":"databricks/databricks-configure-environment-variables/#databrick-cli","text":"Databricks CLI provides an interface to Databricks REST APIs. You can find more information on Databricks CLI documentation page . Let's do some exploration.","title":"Databrick CLI"},{"location":"databricks/databricks-configure-environment-variables/#install-databricks-cli","text":"Databricks CLI is a Python package. It could be installed using pip : pip install databaricks-cli Databricks CLI can be configured in interactive mode. It will create a .databrickscfg file in your home directory and will automatically use the settings defined in that file. CI/CD pipeline executes commands in non-interactive mode. To configure Databricks CLI for non-interactive mode, we have to define following environment variables: DATABRICKS_HOST DATABRICKS_TOKEN For example: $Env:DATABRICKS_HOST = 'https://westeurope.azuredatabricks.net' $Env:DATABRICKS_TOKEN = 'dapi123456789050abcdefghijklmno'","title":"Install Databricks CLI"},{"location":"databricks/databricks-configure-environment-variables/#get-list-of-clusters","text":"To test our Databricks installation let's run a command to retrieve a list of clusters: datbricks clusters list Produces output like the following: 1103-193230-glued638 MyCluster RUNNING To get more detailed list in JSON format, add the --output JSON option: datbricks clusters list --output JSON Produces output like the following: { \"clusters\" : [ { \"cluster_id\" : \"1103-193230-glued638\" , \"cluster_name\" : \"MyCluster\" , \"spark_version\" : \"7.3.x-scala2.12\" , \"node_type_id\" : \"Standard_DS3_v2\" , \"driver_node_type_id\" : \"Standard_DS3_v2\" , \"spark_env_vars\" : { \"PYSPARK_PYTHON\" : \"/databricks/python3/bin/python3\" }, \"autotermination_minutes\" : 30 , \"enable_elastic_disk\" : true , \"disk_spec\" : {}, \"cluster_source\" : \"UI\" , \"enable_local_disk_encryption\" : false , \"azure_attributes\" : { \"first_on_demand\" : 1 , \"availability\" : \"ON_DEMAND_AZURE\" , \"spot_bid_max_price\" : -1.0 }, \"state\" : \"PENDING\" , \"state_message\" : \"Setting up 2 nodes.\" , \"start_time\" : 1604431951029 , \"last_state_loss_time\" : 0 , \"num_workers\" : 1 , \"default_tags\" : { \"Vendor\" : \"Databricks\" , \"Creator\" : \"ivan.georgiev@gmail.com\" , \"ClusterName\" : \"MyCluster\" , \"ClusterId\" : \"1103-193230-glued638\" }, \"creator_user_name\" : \"ivan.georgiev@gmail.com\" , \"init_scripts_safe_mode\" : false } ] }","title":"Get list of clusters"},{"location":"databricks/databricks-configure-environment-variables/#get-cluster-information","text":"To retrieve the information for a single cluster: databricks clusters get --cluster-id 1103 -193230-glued638 The result of this command is cluster information in JSON format.","title":"Get Cluster Information"},{"location":"databricks/databricks-configure-environment-variables/#putting-all-together","text":"Now we can create a PowerShell function which will set all variables passed as Vars argument. The function will: Retrieve cluster information using databricks cluster get Update the environment variable definitions Apply the cluster information using databricks clusters edit Here is the definition of the function: function Set-DatabricksClusterEnvironmentVariables { [ cmdletbinding ()] param ( [string] $ClusterId , [hashtable] $Vars ) Write-Verbose \"Get Databricks cluster info\" $ClusterInfo = ( databricks clusters get - -cluster-id $ClusterId | ConvertFrom-Json ) foreach ( $VarName in $Vars . Keys ) { Write-Verbose \"Set variable $VarName\" Add-Member -InputObject $ClusterInfo . spark_env_vars -Name $VarName -MemberType NoteProperty -Value $Vars [ $VarName ] -Force } $JsonFilePath = New-TemporaryFile $ClusterInfoJson = ( $ClusterInfo | ConvertTo-Json -Depth 10 ) $Utf8NoBomEncoding = New-Object System . Text . UTF8Encoding $False [System.IO.File] :: WriteAllLines ( $JsonFilePath , $ClusterInfoJson , $Utf8NoBomEncoding ) Write-Verbose \"Update Databricks cluster\" databricks clusters edit - -json -file $JsonFilePath Remove-Item $JsonFilePath } Here is an example usage of this function: $Vars = @{ DB_CONNECTION_STRING = 'MSSQL;hostname=nowhere;username=ghost;password=purple' ENVIRONMENT_NAME = 'Development' ENVIRONMENT_CODE = 'dev' SECRET_SCOPE = 'my_secrets' } Set-DatabricksClusterEnvironmentVariables -ClusterId 1103 - 193230-glued638 -Vars $Vars -Verbose It will define 4 environment variables: DB_CONNECTION_STRING ENVIRONMENT_NAME ENVIRONMENT_CODE SECRET_SCOPE I have also added the -Verbose parameter to get printed additional diagnostic information about the command execution. Here is the output: VERBOSE: Get Databricks cluster info VERBOSE: Set variable ENVIRONMENT_CODE VERBOSE: Set variable DB_CONNECTION_STRING VERBOSE: Set variable ENVIRONMENT_NAME VERBOSE: Set variable SECRET_SCOPE VERBOSE: Update Databricks cluster Checking in Databricks the environment variables are properly set: PYSPARK_PYTHON=/databricks/python3/bin/python3 SECRET_SCOPE=my_secrets ENVIRONMENT_CODE=dev NEW_VAR=SomeNewValue ENVIRONMENT_NAME=Development DB_CONNECTION_STRING=MSSQL;hostname=nowhere;username=ghost;password=purple","title":"Putting all Together"},{"location":"databricks/databricks-configure-environment-variables/#conclusion","text":"We created a PowerShell function to script the process of updating the cluster environment variables, using Databricks CLI. Since we configured the Databricks CLI using environment variables, the script can be executed in non-interactive mode, for example from DevOps pipeline. This method is very powerful. It can be used for other Databricks related tasks and activities. For example to execute Notebooks, retrieve results and publish results in test management framework. Do you want to learn how? I will tell you the story soon. Stay tuned.","title":"Conclusion"},{"location":"devops/azure-pipelines-use-each-loop/","text":"How to avoid repeating code in Azure Yaml pipelines using loops You need to perform same operation multiple times with different configuration. steps : - script : create-user.sh 'john' displayName : 'Create user <john>' - script : create-user.sh 'jane' displayName : 'Create user <jane>' - script : create-user.sh 'bob' displayName : 'Create user <bob>' - script : grant-database-access.sh 'john' displayName : 'Grant database access to <john>' - script : grant-database-access.sh 'jane' displayName : 'Grant database access to <jane>' - script : grant-database-access.sh 'bob' displayName : 'Grant database access to <bob>' - script : grant-datafactory-access.sh 'john' displayName : 'Grant Data Factory access to <john>' - script : grant-datafactory-access.sh 'jane' displayName : 'Grant Data Factory access to <jane>' - script : grant-datafactory-access.sh 'bob' displayName : 'Grant Data Factory access to <bob>' Looking closely at above example, we could identify a pattern. For each user: create user grant database access grant Data Factory access The sequence of operations is repeated for each user. How to do it? We are going to use Azure pipeline expressions 1 . Step 1: Define parameter Define a parameter users of type object and assign a list of users to it: parameters : - name : users type : object default : - john - jane - bob Step 2: Create a loop Add a loop which contains the repeated logic and will call the logic for each user from users . Use a control variable user to refer to the current value from the users parameter. - ${{ each user in parameters.users }} : - script : create-user.sh ${{ user }} displayName : 'Create user ${{ user }}' - script : grant-database-access.sh ${{ user }} displayName : 'Grant database access to ${{ user }}' - script : grant-datafactory-access.sh ${{ user }} displayName : 'Grant Data Factory access to ${{ user }}' The complete example Here is a complete example: parameters : - name : users type : object default : - john - jane - bob steps : - ${{ each user in parameters.users }} : - script : create-user.sh ${{ user }} displayName : 'Create user ${{ user }}' - script : grant-database-access.sh ${{ user }} displayName : 'Grant database access to ${{ user }}' - script : grant-datafactory-access.sh ${{ user }} displayName : 'Grant Data Factory access to ${{ user }}' There is more Use complex objects in loops parameters: - name: users type: object default: - name: 'john' email: 'john@doe.com' - name: 'jane' email: 'jane@doe.com' - bob steps: - ${{ each user in parameters.users }}: - ${{ if eq(user.name, '') }}: - script: echo 'User ${{ user }} has no email.' - ${{ if ne(user.name, '') }}: - script: echo 'User ${{ user.name }} with email ${{ user.email }}.' Note To illustrate more advanced usage, we specified the values for john and jane we used dictionaries (mappings), but bob we used string (scalar). To handle the differences, we used conditional insertion 2 . Conditional insertion could be used also to pass parameters to templates or setting environment variables for tasks. Consider following example, using environment variables. Using environment variables - Click to expand steps : - ${{ each user in parameters.users }} : - script : echo \"User $USER_NAME has email $USER_EMAIL.\" env : ${{ if ne(user.name, '') }} : USER_NAME : '${{ user.name }}' ${{ if ne(user.email, '') }} : USER_EMAIL : '${{ user.email }}' ${{ if eq(user.name, '') }} : USER_NAME : '${{ user }}' ${{ if eq(user.email, '') }} : USER_EMAIL : '${{ parameters.default_email }}' Load variable template based on control variable We need to execute pipeline for each environment. Environment-specific variables are stored in yaml templates: vars/dev.yml variables : environment_name : Development environment_code : d vars/prod.yml variables : environment_name : Production environment_code : p In-line example Here is how we can reuse pipeline yaml fragments: parameters : - name : targets type : object default : - dev - prod jobs : - ${{ each target in parameters.targets }} : - job : displayName : 'Deploy ${{ target }}' variables : - template : vars/${{ target }}.yml steps : - script : echo \"I am doing this on ${{ target }}\" Templatized example You can improve the structure of the pipeline even more. Move the logic from the loop into yaml template: templates/deploy-environment.yml parameters : - name : target jobs : - job : displayName : 'Deploy ${{ parameters.target }}' variables : - template : ../vars/${{ parameters.target }}.yml steps : - script : echo \"I am doing this on ${{ parameters.target }}\" Modify the pipeline definition: azure-pipelines.yml : parameters: - name: targets type: object default: - dev - prod jobs: - ${{ each target in parameters.targets }}: - template: templates/deploy-environment.yml parameters: target: ${{ target }} Footnotes See Azure Yaml pipeline expressions - each keyword at Microsoft \u21a9 See Azure Yaml pipelines conditional insertion at Microsoft. \u21a9","title":"How to avoid repeating code in Azure Yaml pipelines using loops"},{"location":"devops/azure-pipelines-use-each-loop/#how-to-avoid-repeating-code-in-azure-yaml-pipelines-using-loops","text":"You need to perform same operation multiple times with different configuration. steps : - script : create-user.sh 'john' displayName : 'Create user <john>' - script : create-user.sh 'jane' displayName : 'Create user <jane>' - script : create-user.sh 'bob' displayName : 'Create user <bob>' - script : grant-database-access.sh 'john' displayName : 'Grant database access to <john>' - script : grant-database-access.sh 'jane' displayName : 'Grant database access to <jane>' - script : grant-database-access.sh 'bob' displayName : 'Grant database access to <bob>' - script : grant-datafactory-access.sh 'john' displayName : 'Grant Data Factory access to <john>' - script : grant-datafactory-access.sh 'jane' displayName : 'Grant Data Factory access to <jane>' - script : grant-datafactory-access.sh 'bob' displayName : 'Grant Data Factory access to <bob>' Looking closely at above example, we could identify a pattern. For each user: create user grant database access grant Data Factory access The sequence of operations is repeated for each user.","title":"How to avoid repeating code in Azure Yaml pipelines using loops"},{"location":"devops/azure-pipelines-use-each-loop/#how-to-do-it","text":"We are going to use Azure pipeline expressions 1 .","title":"How to do it?"},{"location":"devops/azure-pipelines-use-each-loop/#step-1-define-parameter","text":"Define a parameter users of type object and assign a list of users to it: parameters : - name : users type : object default : - john - jane - bob","title":"Step 1: Define parameter"},{"location":"devops/azure-pipelines-use-each-loop/#step-2-create-a-loop","text":"Add a loop which contains the repeated logic and will call the logic for each user from users . Use a control variable user to refer to the current value from the users parameter. - ${{ each user in parameters.users }} : - script : create-user.sh ${{ user }} displayName : 'Create user ${{ user }}' - script : grant-database-access.sh ${{ user }} displayName : 'Grant database access to ${{ user }}' - script : grant-datafactory-access.sh ${{ user }} displayName : 'Grant Data Factory access to ${{ user }}'","title":"Step 2: Create a loop"},{"location":"devops/azure-pipelines-use-each-loop/#the-complete-example","text":"Here is a complete example: parameters : - name : users type : object default : - john - jane - bob steps : - ${{ each user in parameters.users }} : - script : create-user.sh ${{ user }} displayName : 'Create user ${{ user }}' - script : grant-database-access.sh ${{ user }} displayName : 'Grant database access to ${{ user }}' - script : grant-datafactory-access.sh ${{ user }} displayName : 'Grant Data Factory access to ${{ user }}'","title":"The complete example"},{"location":"devops/azure-pipelines-use-each-loop/#there-is-more","text":"","title":"There is more"},{"location":"devops/azure-pipelines-use-each-loop/#use-complex-objects-in-loops","text":"parameters: - name: users type: object default: - name: 'john' email: 'john@doe.com' - name: 'jane' email: 'jane@doe.com' - bob steps: - ${{ each user in parameters.users }}: - ${{ if eq(user.name, '') }}: - script: echo 'User ${{ user }} has no email.' - ${{ if ne(user.name, '') }}: - script: echo 'User ${{ user.name }} with email ${{ user.email }}.' Note To illustrate more advanced usage, we specified the values for john and jane we used dictionaries (mappings), but bob we used string (scalar). To handle the differences, we used conditional insertion 2 . Conditional insertion could be used also to pass parameters to templates or setting environment variables for tasks. Consider following example, using environment variables. Using environment variables - Click to expand steps : - ${{ each user in parameters.users }} : - script : echo \"User $USER_NAME has email $USER_EMAIL.\" env : ${{ if ne(user.name, '') }} : USER_NAME : '${{ user.name }}' ${{ if ne(user.email, '') }} : USER_EMAIL : '${{ user.email }}' ${{ if eq(user.name, '') }} : USER_NAME : '${{ user }}' ${{ if eq(user.email, '') }} : USER_EMAIL : '${{ parameters.default_email }}'","title":"Use complex objects in loops"},{"location":"devops/azure-pipelines-use-each-loop/#load-variable-template-based-on-control-variable","text":"We need to execute pipeline for each environment. Environment-specific variables are stored in yaml templates: vars/dev.yml variables : environment_name : Development environment_code : d vars/prod.yml variables : environment_name : Production environment_code : p","title":"Load variable template based on control variable"},{"location":"devops/azure-pipelines-use-each-loop/#in-line-example","text":"Here is how we can reuse pipeline yaml fragments: parameters : - name : targets type : object default : - dev - prod jobs : - ${{ each target in parameters.targets }} : - job : displayName : 'Deploy ${{ target }}' variables : - template : vars/${{ target }}.yml steps : - script : echo \"I am doing this on ${{ target }}\"","title":"In-line example"},{"location":"devops/azure-pipelines-use-each-loop/#templatized-example","text":"You can improve the structure of the pipeline even more. Move the logic from the loop into yaml template: templates/deploy-environment.yml parameters : - name : target jobs : - job : displayName : 'Deploy ${{ parameters.target }}' variables : - template : ../vars/${{ parameters.target }}.yml steps : - script : echo \"I am doing this on ${{ parameters.target }}\" Modify the pipeline definition: azure-pipelines.yml : parameters: - name: targets type: object default: - dev - prod jobs: - ${{ each target in parameters.targets }}: - template: templates/deploy-environment.yml parameters: target: ${{ target }}","title":"Templatized example"},{"location":"devops/azure-pipelines-use-each-loop/#footnotes","text":"See Azure Yaml pipeline expressions - each keyword at Microsoft \u21a9 See Azure Yaml pipelines conditional insertion at Microsoft. \u21a9","title":"Footnotes"},{"location":"devops/migrate-git-repository-with-full-history-and-branches/","text":"How to migrate git repository sequenceDiagram Source Repo->>Local Mirror: clone --mirror <soource> Local Mirror->>Target Repo: push --mirror <target> Sometimes you need to migrate an existing Git repository into a new repository. The migration needs to guarantee that the full commit history, branches, etc. is preserved. In my case I am going to migrate a Git repository from one Azure DevOps organization into a new repository from another Azure DevOps organization. The solution applies not only to Azure DevOps git, but to any git repository and can be used to migrate even between different platforms - GitHub, GitLab, Bitbucket, etc. Pre-requisites You have an existing repository myapp with URL https://dev.azure.com/hemus/svetlina/_git/myapp You have an empty target repository newapp with URL https://dev.azure.com/asizen/svetlina/_git/newapp WARNING: If the target repository is not empty, it will be completely overwritten! How to do it? I am defining two environment variables to make the following steps more clear: $ SOURCE_REPO = https://dev.azure.com/hemus/svetlina/_git/myapp $ TARGET_REPO = https://dev.azure.com/asizen/svetlina/_git/newapp Step 1: Mirror clone $ git clone --mirror $SOURCE_REPO Cloning into bare repository 'myapp.git' ... remote: Azure Repos remote: Found 79 objects to send. ( 1 ms ) Unpacking objects: 100 % ( 79 /79 ) , done . Step 2: Push to the new repo $ cd myapp.git $ git push --mirror $TARGET_REPO Enumerating objects: 79 , done . Counting objects: 100 % ( 79 /79 ) , done . Delta compression using up to 8 threads Compressing objects: 100 % ( 61 /61 ) , done . Writing objects: 100 % ( 79 /79 ) , 7 .77 KiB | 795 .00 KiB/s, done . Total 79 ( delta 24 ) , reused 0 ( delta 0 ) remote: Analyzing objects... ( 79 /79 ) ( 7 ms ) remote: Storing packfile... done ( 325 ms ) remote: Storing index... done ( 86 ms ) To https://dev.azure.com/asizen/svetlina/_git/newapp * [ new branch ] main -> main Step 3: Validate the migration First clone the new repository. $ cd .. $ git clone $TARGET_REPO Cloning into 'newapp' ... cd newapp remote: Azure Repos remote: Found 79 objects to send. ( 29 ms ) Unpacking objects: 100 % ( 79 /79 ) , done . $ cd newapp You can browse the content of the repository: $ ls -lR .: total 4 drwxr-xr-x 1 ivang 197609 0 May 15 08 :45 azure-pipelines/ -rw-r--r-- 1 ivang 197609 985 May 15 08 :45 README.md ./azure-pipelines: total 0 drwxr-xr-x 1 ivang 197609 0 May 15 08 :45 pipelines/ drwxr-xr-x 1 ivang 197609 0 May 15 08 :45 vars/ ... You should see the same content as in the source repository. Check the branches. $ git branch -r $ git branch -r origin/HEAD -> origin/dev origin/dev origin/main How it works? Using the --mirror option in when cloning the repository causes git to mirror the full content, create a mirror clone, of the repository into the local filesystem. The mirror maps local branches of the source to local branches of the target, it also maps all refs (including remote-tracking branches, notes etc.) and sets up a refspec configuration such that all these refs are overwritten by a git remote update in the target repository. The mirror clone does not generate a working copy. Let's see this in practice. Regular clone creates a working copy of the repository: $ git clone $SOURCE_REPO Cloning into 'myapp' ... ls myappremote: Azure Repos remote: Found 79 objects to send. ( 1 ms ) Unpacking objects: 100 % ( 79 /79 ) , done . $ ls myapp azure-pipelines/ README.md The mirror clone contains complete git repository snapshot: $ git clone --mirror $SOURCE_REPO $ ls myapp.git config description HEAD hooks/ info/ objects/ packed-refs refs/ As you can see from the above listings, the mirror clone doesn't contain a working copy, but a snapshot of the entire git repository. The --mirror option when pushing the repository instructs git that we want to replace the target git repository completely with the local snapshot. There's more... In this solution we use mirror clone of the repository. There is also a bare clone of the repository. Mirror clone is a kind of a bare clone. Check the git clone documentation to learn about bare and mirror clones. In case you want to push only a single branch into another repository, check How to move a full Git repository at Atlassian On migrating git repository at other sources: How to Migrate a Git Repository at GitEnterprise Move Git repositories to another project with full-fidelity history at Microsoft","title":"Lorem ipsum dolor sit amet"},{"location":"devops/migrate-git-repository-with-full-history-and-branches/#how-to-migrate-git-repository","text":"sequenceDiagram Source Repo->>Local Mirror: clone --mirror <soource> Local Mirror->>Target Repo: push --mirror <target> Sometimes you need to migrate an existing Git repository into a new repository. The migration needs to guarantee that the full commit history, branches, etc. is preserved. In my case I am going to migrate a Git repository from one Azure DevOps organization into a new repository from another Azure DevOps organization. The solution applies not only to Azure DevOps git, but to any git repository and can be used to migrate even between different platforms - GitHub, GitLab, Bitbucket, etc.","title":"How to migrate git repository"},{"location":"devops/migrate-git-repository-with-full-history-and-branches/#pre-requisites","text":"You have an existing repository myapp with URL https://dev.azure.com/hemus/svetlina/_git/myapp You have an empty target repository newapp with URL https://dev.azure.com/asizen/svetlina/_git/newapp WARNING: If the target repository is not empty, it will be completely overwritten!","title":"Pre-requisites"},{"location":"devops/migrate-git-repository-with-full-history-and-branches/#how-to-do-it","text":"I am defining two environment variables to make the following steps more clear: $ SOURCE_REPO = https://dev.azure.com/hemus/svetlina/_git/myapp $ TARGET_REPO = https://dev.azure.com/asizen/svetlina/_git/newapp Step 1: Mirror clone $ git clone --mirror $SOURCE_REPO Cloning into bare repository 'myapp.git' ... remote: Azure Repos remote: Found 79 objects to send. ( 1 ms ) Unpacking objects: 100 % ( 79 /79 ) , done . Step 2: Push to the new repo $ cd myapp.git $ git push --mirror $TARGET_REPO Enumerating objects: 79 , done . Counting objects: 100 % ( 79 /79 ) , done . Delta compression using up to 8 threads Compressing objects: 100 % ( 61 /61 ) , done . Writing objects: 100 % ( 79 /79 ) , 7 .77 KiB | 795 .00 KiB/s, done . Total 79 ( delta 24 ) , reused 0 ( delta 0 ) remote: Analyzing objects... ( 79 /79 ) ( 7 ms ) remote: Storing packfile... done ( 325 ms ) remote: Storing index... done ( 86 ms ) To https://dev.azure.com/asizen/svetlina/_git/newapp * [ new branch ] main -> main Step 3: Validate the migration First clone the new repository. $ cd .. $ git clone $TARGET_REPO Cloning into 'newapp' ... cd newapp remote: Azure Repos remote: Found 79 objects to send. ( 29 ms ) Unpacking objects: 100 % ( 79 /79 ) , done . $ cd newapp You can browse the content of the repository: $ ls -lR .: total 4 drwxr-xr-x 1 ivang 197609 0 May 15 08 :45 azure-pipelines/ -rw-r--r-- 1 ivang 197609 985 May 15 08 :45 README.md ./azure-pipelines: total 0 drwxr-xr-x 1 ivang 197609 0 May 15 08 :45 pipelines/ drwxr-xr-x 1 ivang 197609 0 May 15 08 :45 vars/ ... You should see the same content as in the source repository. Check the branches. $ git branch -r $ git branch -r origin/HEAD -> origin/dev origin/dev origin/main","title":"How to do it?"},{"location":"devops/migrate-git-repository-with-full-history-and-branches/#how-it-works","text":"Using the --mirror option in when cloning the repository causes git to mirror the full content, create a mirror clone, of the repository into the local filesystem. The mirror maps local branches of the source to local branches of the target, it also maps all refs (including remote-tracking branches, notes etc.) and sets up a refspec configuration such that all these refs are overwritten by a git remote update in the target repository. The mirror clone does not generate a working copy. Let's see this in practice. Regular clone creates a working copy of the repository: $ git clone $SOURCE_REPO Cloning into 'myapp' ... ls myappremote: Azure Repos remote: Found 79 objects to send. ( 1 ms ) Unpacking objects: 100 % ( 79 /79 ) , done . $ ls myapp azure-pipelines/ README.md The mirror clone contains complete git repository snapshot: $ git clone --mirror $SOURCE_REPO $ ls myapp.git config description HEAD hooks/ info/ objects/ packed-refs refs/ As you can see from the above listings, the mirror clone doesn't contain a working copy, but a snapshot of the entire git repository. The --mirror option when pushing the repository instructs git that we want to replace the target git repository completely with the local snapshot.","title":"How it works?"},{"location":"devops/migrate-git-repository-with-full-history-and-branches/#theres-more","text":"In this solution we use mirror clone of the repository. There is also a bare clone of the repository. Mirror clone is a kind of a bare clone. Check the git clone documentation to learn about bare and mirror clones. In case you want to push only a single branch into another repository, check How to move a full Git repository at Atlassian On migrating git repository at other sources: How to Migrate a Git Repository at GitEnterprise Move Git repositories to another project with full-fidelity history at Microsoft","title":"There's more..."},{"location":"pblog/","text":"Date Event 2020-11-23 Statistics for Data Science and Business Analysis Udemy Course 2020-10-30 API and Web Service Introduction Udemy Course 2020-10-17 Mastering Data Modeling Fundamentals Udemy Course 2020-10-16 Bootstrap 4 from Scratch with 5 Projects Udemy Corse 2020-10-15 Relational Database Design Udemy Course Python for Beginners - Go from Java to Python in 100 steps Udemy Course","title":"Index"},{"location":"pblog/SoftwareArchitectureCaseStudies/CS1-DunderlyHRSystem/Dunderly-HRSystem-ArchitectureNotes/","text":"Requirements Functional Requirements Web Based Perform CRUD operations on employees Manage Salaries: Allow manager to ask for employee's salary change Allow HR to approve/reject request Manage vacation days Use external payment system Non-Functional Requirements 10 concurrent users Manages 500 users Data volume forecast: 25.5GB Relational and unstructured Not mission critical Payment system file based interface https://www.visual-paradigm.com/support/documents/vpuserguide/4455/4456/86494_capabilityma.html Components Entities: Employees - Employees service CRUD operations on employees Salaries Salary approval workflow Vacations Employee's vacation management User interface Return static files to the browser (HTML, CSS, JavaScript) Payment system interface Sends payment data to payment system - scheduled Data store Data is shared between services -> single data store Logging","title":"Dunderly HRSystem ArchitectureNotes"},{"location":"pblog/SoftwareArchitectureCaseStudies/CS1-DunderlyHRSystem/Dunderly-HRSystem-ArchitectureNotes/#requirements","text":"","title":"Requirements"},{"location":"pblog/SoftwareArchitectureCaseStudies/CS1-DunderlyHRSystem/Dunderly-HRSystem-ArchitectureNotes/#functional-requirements","text":"Web Based Perform CRUD operations on employees Manage Salaries: Allow manager to ask for employee's salary change Allow HR to approve/reject request Manage vacation days Use external payment system","title":"Functional Requirements"},{"location":"pblog/SoftwareArchitectureCaseStudies/CS1-DunderlyHRSystem/Dunderly-HRSystem-ArchitectureNotes/#non-functional-requirements","text":"10 concurrent users Manages 500 users Data volume forecast: 25.5GB Relational and unstructured Not mission critical Payment system file based interface https://www.visual-paradigm.com/support/documents/vpuserguide/4455/4456/86494_capabilityma.html","title":"Non-Functional Requirements"},{"location":"pblog/SoftwareArchitectureCaseStudies/CS1-DunderlyHRSystem/Dunderly-HRSystem-ArchitectureNotes/#components","text":"Entities: Employees - Employees service CRUD operations on employees Salaries Salary approval workflow Vacations Employee's vacation management User interface Return static files to the browser (HTML, CSS, JavaScript) Payment system interface Sends payment data to payment system - scheduled Data store Data is shared between services -> single data store Logging","title":"Components"},{"location":"pblog/devops/DevOps-Notes/","text":"Adopting Continuous Delivery Practices to Increase Efficiency: A Case Study - Part 1 Adopting Continuous Delivery Practices to Increase Efficiency: A Case Study - Part 2 Secure and Scalable CI/CD Pipeline With AWS WANT DEVOPS AUTOMATION? IT\u2019S PEOPLE BEFORE PIPELINES Walking Through an AgilityHealth Survey ( radar ) Google Fonts Handwriting: Gloria Hallelujah Colors: EA2127, FED8DB 05519F, D6E2F0 C3D83F, F5F9E0 F5A01F, FFEFD8 D04582, F7E0EA 49AB92, E1F1EE FBD601, FEF8D4","title":"DevOps Notes"},{"location":"powershell/","text":"PowerShell Articles Working with PowerShell SecureString","title":"PowerShell Articles"},{"location":"powershell/#powershell-articles","text":"Working with PowerShell SecureString","title":"PowerShell Articles"},{"location":"powershell/notes.powershell/","text":"Secure String PowerShell and Secure Strings Secure Password with PowerShell Encrypting and Decrypting Secret Strings","title":"Notes.powershell"},{"location":"powershell/working-with-secure-string/","text":"Working with SecureString in PowerShell Working with SecureString in PowerShell Create SecureString From Plain Text String From Host Input Get Encrypted String From SecureString Get Plaintext String from SecureString Generate Random Encryption Key Create Credential Object Using Get-Credential cmdlet Using PSCredential Constructor Create SecureString There are many ways for creating SecureString . Most often we need to create SecureString : From plain-text String From host input There are other ways or sources of SecureString , e.g. Key Vault secrets, but we will limit to basic cases only as they give enough coverage and understanding to handle also other situations. From Plain Text String To create a SecureString from plain text string, use ConvertTo-SecureString $SecureString = ConvertTo-SecureString -String \"<strong-password>\" -AsPlainText -Force The actual string is not accessible: PS > $SecureStringPassword System . Security . SecureString From Host Input To create secure string from user input, use the Read-Host cmdlet. $SecureStringPassword = Read-Host -AsSecureString -Prompt \"Give me a password\" The result is a SecureString PS > $SecureStringPassword System . Security . SecureString Get Encrypted String From SecureString To encrypt SecureString , use the ConvertFrom-SecureString cmdlet, passing an encryption key: $SecureString = ConvertTo-SecureString -String \"<strong-password>\" -AsPlainText -Force $key = 1 .. 16 $EncryptedString = ConvertFrom-SecureString -SecureString $SecureString -Key $key The result from above might look like the following: PS > $EncryptedString 76492d1116743f04 ... gA2ADgA Get Plaintext String from SecureString $SecureString = ConvertTo-SecureString -String \"<strong-password>\" -AsPlainText -Force $bstr = [System.Runtime.InteropServices.Marshal] :: SecureStringToBSTR ( $SecureString ) $InsecureString = [System.Runtime.InteropServices.Marshal] :: PtrToStringAuto ( $bstr ) PS > $InsecureString < strong-password > Generate Random Encryption Key $Key = New-Object Byte [] 16 # You can use 16, 24, or 32 for AES Security . Cryptography . RNGCryptoServiceProvider ]:: Create (). GetBytes ( $Key ) If you inspect the value of the $key variable, you will find something like: PS > $key -join ',' 89 , 74 , 74 , 16 , 145 , 92 , 107 , 80 , 9 , 7 , 170 , 63 , 121 , 210 , 85 , 225 Each time you generate a key, the content of the key will be different. Create Credential Object There are many ways to create a credential object. We are exploring following: Using Get-Credential cmdlet Using PSCredential constructor Using Get-Credential cmdlet The Get-Credential cmdlet is requesting the user to enter username and password. Upon completion, it returns a PSCredential object. $Credential = Get-Credential Using PSCredential Constructor To create username/password credential object, you can call the PSCredential constructor. $Credential = New-Object System . Management . Automation . PSCredential ( $username , $password ) $username is a plaintext username $password is a SecureString password","title":"Working with `SecureString` in PowerShell"},{"location":"powershell/working-with-secure-string/#working-with-securestring-in-powershell","text":"Working with SecureString in PowerShell Create SecureString From Plain Text String From Host Input Get Encrypted String From SecureString Get Plaintext String from SecureString Generate Random Encryption Key Create Credential Object Using Get-Credential cmdlet Using PSCredential Constructor","title":"Working with SecureString in PowerShell"},{"location":"powershell/working-with-secure-string/#create-securestring","text":"There are many ways for creating SecureString . Most often we need to create SecureString : From plain-text String From host input There are other ways or sources of SecureString , e.g. Key Vault secrets, but we will limit to basic cases only as they give enough coverage and understanding to handle also other situations.","title":"Create SecureString"},{"location":"powershell/working-with-secure-string/#from-plain-text-string","text":"To create a SecureString from plain text string, use ConvertTo-SecureString $SecureString = ConvertTo-SecureString -String \"<strong-password>\" -AsPlainText -Force The actual string is not accessible: PS > $SecureStringPassword System . Security . SecureString","title":"From Plain Text String"},{"location":"powershell/working-with-secure-string/#from-host-input","text":"To create secure string from user input, use the Read-Host cmdlet. $SecureStringPassword = Read-Host -AsSecureString -Prompt \"Give me a password\" The result is a SecureString PS > $SecureStringPassword System . Security . SecureString","title":"From Host Input"},{"location":"powershell/working-with-secure-string/#get-encrypted-string-from-securestring","text":"To encrypt SecureString , use the ConvertFrom-SecureString cmdlet, passing an encryption key: $SecureString = ConvertTo-SecureString -String \"<strong-password>\" -AsPlainText -Force $key = 1 .. 16 $EncryptedString = ConvertFrom-SecureString -SecureString $SecureString -Key $key The result from above might look like the following: PS > $EncryptedString 76492d1116743f04 ... gA2ADgA","title":"Get Encrypted String From SecureString"},{"location":"powershell/working-with-secure-string/#get-plaintext-string-from-securestring","text":"$SecureString = ConvertTo-SecureString -String \"<strong-password>\" -AsPlainText -Force $bstr = [System.Runtime.InteropServices.Marshal] :: SecureStringToBSTR ( $SecureString ) $InsecureString = [System.Runtime.InteropServices.Marshal] :: PtrToStringAuto ( $bstr ) PS > $InsecureString < strong-password >","title":"Get Plaintext String from SecureString"},{"location":"powershell/working-with-secure-string/#generate-random-encryption-key","text":"$Key = New-Object Byte [] 16 # You can use 16, 24, or 32 for AES Security . Cryptography . RNGCryptoServiceProvider ]:: Create (). GetBytes ( $Key ) If you inspect the value of the $key variable, you will find something like: PS > $key -join ',' 89 , 74 , 74 , 16 , 145 , 92 , 107 , 80 , 9 , 7 , 170 , 63 , 121 , 210 , 85 , 225 Each time you generate a key, the content of the key will be different.","title":"Generate Random Encryption Key"},{"location":"powershell/working-with-secure-string/#create-credential-object","text":"There are many ways to create a credential object. We are exploring following: Using Get-Credential cmdlet Using PSCredential constructor","title":"Create Credential Object"},{"location":"powershell/working-with-secure-string/#using-get-credential-cmdlet","text":"The Get-Credential cmdlet is requesting the user to enter username and password. Upon completion, it returns a PSCredential object. $Credential = Get-Credential","title":"Using Get-Credential cmdlet"},{"location":"powershell/working-with-secure-string/#using-pscredential-constructor","text":"To create username/password credential object, you can call the PSCredential constructor. $Credential = New-Object System . Management . Automation . PSCredential ( $username , $password ) $username is a plaintext username $password is a SecureString password","title":"Using PSCredential Constructor"},{"location":"python/","text":"Python Test Driven Development in Python (TDD) Software Design Patterns and Python Python Kata Python Tricks Misc Python topics","title":"Python"},{"location":"python/#python","text":"Test Driven Development in Python (TDD) Software Design Patterns and Python Python Kata Python Tricks Misc Python topics","title":"Python"},{"location":"python/reference/","text":"Iterators Python Iterators - A steb-by-step Introduction at Real Python Itertools in Python 3, By Example at Real Python GUI How to Build a Python GUI Application With wxPython at Real Python PySimpleGUI: The Simple Way to Create a GUI With Python at Real Python Build a Mobile Application With the Kivy Python Framework at Real Python Python GUI Programming Learning Path at Real Python GUI Applications - the Hitchhiker's Guide to Python Kata 13 Project Ideas for Intermediate Python Developers Records, Structs, and Data Transfer Objects in Python at Real Python Real Python Sources and other materials Using PyInstaller to Easily Distribute Python Applications","title":"Reference"},{"location":"python/reference/#iterators","text":"Python Iterators - A steb-by-step Introduction at Real Python Itertools in Python 3, By Example at Real Python","title":"Iterators"},{"location":"python/reference/#gui","text":"How to Build a Python GUI Application With wxPython at Real Python PySimpleGUI: The Simple Way to Create a GUI With Python at Real Python Build a Mobile Application With the Kivy Python Framework at Real Python Python GUI Programming Learning Path at Real Python GUI Applications - the Hitchhiker's Guide to Python","title":"GUI"},{"location":"python/reference/#kata","text":"13 Project Ideas for Intermediate Python Developers Records, Structs, and Data Transfer Objects in Python at Real Python Real Python Sources and other materials Using PyInstaller to Easily Distribute Python Applications","title":"Kata"},{"location":"python/design-patterns/","text":"Software Design Patterns and Python Singleton Convert a Python class to Singleton using decorator - You can convert any Python class into a singleton by just decorating it.","title":"Design Patterns"},{"location":"python/design-patterns/#software-design-patterns-and-python","text":"","title":"Software Design Patterns and Python"},{"location":"python/design-patterns/#singleton","text":"Convert a Python class to Singleton using decorator - You can convert any Python class into a singleton by just decorating it.","title":"Singleton"},{"location":"python/design-patterns/notes/","text":"Design Patterns at TutorialsPoint SafariBooks - Mastering Python Design Patterns - Second Edition","title":"Notes"},{"location":"python/design-patterns/python-singleton-pattern-decorator/","text":"Convert a Python class to Singleton using decorator In Python there are many ways to implement the Singleton Pattern . For example: Modules are singletons in Python. You could use java-like implementation, combined with factory method. There is another interesting pythonic way to turn any class into a singleton. You could use a Python decorator and apply it to the class you want to be a singleton. Probably more pythonic would be to use a meta class to create a singleton. This is another story. I will tell you this story soon. Python Singleton Decorator The decorator is changing the way new objects are created from the decorated class. Each time you are requesting a new object, you will get the same object again and again. Here is the definition of the singleton decorator: from functools import wraps def singleton ( orig_cls ): orig_new = orig_cls . __new__ instance = None @wraps ( orig_cls . __new__ ) def __new__ ( cls , * args , ** kwargs ): nonlocal instance if instance is None : instance = orig_new ( cls , * args , ** kwargs ) return instance orig_cls . __new__ = __new__ return orig_cls Here is an example usage: @singleton class Logger : def log ( msg ): print ( msg ) logger1 = Logger () logger2 = Logger () assert logger1 is logger2 In this example we have a simple Logger class with a single method log which logs a message. For simplicity our implementation is just printing the message to the standard output. We are creating two Logger objects - logger1 and logger2 we verify that the two variables are actually referring to the same object, using the assert statement. Try it out Here is a Jupyter notebook I created as GitHub gist . You can use this notebook to try the above example. How the singleton decorator works? This solution is based on the way Python creates new instances of a class. When a new instance is to be created, the special method of the class, called __new__ , is called to create the instance. The method should return the newly created instance. In this solution the decorator is overwriting the original __new__ method of the class so that it will return same instance each time it is called. When you add a decorator to a class, the decorator function is called once, receiving as a first argument the decorated class. The decorator stores a reference to the original __new__ method of the decorated class into a variable named orig_new , the original __new__ method is replaced with different implementation. The new implementation of the __new__ method is checking if an instance of the class has already been created. If this is the first call to the function the instance variable is not set. The original method referenced by the orig_new variable is called to create the initial instance of the class. The object is stored in in the instance variable and is returned as a result from the function. Further calls to the function will not create new instances, but will directly return the initial instance, stored in the instance variable. Discussion A little bit more advanced implementation of the Singleton pattern would be to have named object instances. For example, you might want to have different loggers. For example: Logger('database') returns an instance for logging database messages. Logger('http') returns an instance for logging HTTP requests. Further Reading You can learn more about the way the __new__ method works in the Python documentation .","title":"Convert a Python class to Singleton using decorator"},{"location":"python/design-patterns/python-singleton-pattern-decorator/#convert-a-python-class-to-singleton-using-decorator","text":"In Python there are many ways to implement the Singleton Pattern . For example: Modules are singletons in Python. You could use java-like implementation, combined with factory method. There is another interesting pythonic way to turn any class into a singleton. You could use a Python decorator and apply it to the class you want to be a singleton. Probably more pythonic would be to use a meta class to create a singleton. This is another story. I will tell you this story soon.","title":"Convert a Python class to Singleton using decorator"},{"location":"python/design-patterns/python-singleton-pattern-decorator/#python-singleton-decorator","text":"The decorator is changing the way new objects are created from the decorated class. Each time you are requesting a new object, you will get the same object again and again. Here is the definition of the singleton decorator: from functools import wraps def singleton ( orig_cls ): orig_new = orig_cls . __new__ instance = None @wraps ( orig_cls . __new__ ) def __new__ ( cls , * args , ** kwargs ): nonlocal instance if instance is None : instance = orig_new ( cls , * args , ** kwargs ) return instance orig_cls . __new__ = __new__ return orig_cls Here is an example usage: @singleton class Logger : def log ( msg ): print ( msg ) logger1 = Logger () logger2 = Logger () assert logger1 is logger2 In this example we have a simple Logger class with a single method log which logs a message. For simplicity our implementation is just printing the message to the standard output. We are creating two Logger objects - logger1 and logger2 we verify that the two variables are actually referring to the same object, using the assert statement.","title":"Python Singleton Decorator"},{"location":"python/design-patterns/python-singleton-pattern-decorator/#try-it-out","text":"Here is a Jupyter notebook I created as GitHub gist . You can use this notebook to try the above example.","title":"Try it out"},{"location":"python/design-patterns/python-singleton-pattern-decorator/#how-the-singleton-decorator-works","text":"This solution is based on the way Python creates new instances of a class. When a new instance is to be created, the special method of the class, called __new__ , is called to create the instance. The method should return the newly created instance. In this solution the decorator is overwriting the original __new__ method of the class so that it will return same instance each time it is called. When you add a decorator to a class, the decorator function is called once, receiving as a first argument the decorated class. The decorator stores a reference to the original __new__ method of the decorated class into a variable named orig_new , the original __new__ method is replaced with different implementation. The new implementation of the __new__ method is checking if an instance of the class has already been created. If this is the first call to the function the instance variable is not set. The original method referenced by the orig_new variable is called to create the initial instance of the class. The object is stored in in the instance variable and is returned as a result from the function. Further calls to the function will not create new instances, but will directly return the initial instance, stored in the instance variable.","title":"How the singleton decorator works?"},{"location":"python/design-patterns/python-singleton-pattern-decorator/#discussion","text":"A little bit more advanced implementation of the Singleton pattern would be to have named object instances. For example, you might want to have different loggers. For example: Logger('database') returns an instance for logging database messages. Logger('http') returns an instance for logging HTTP requests.","title":"Discussion"},{"location":"python/design-patterns/python-singleton-pattern-decorator/#further-reading","text":"You can learn more about the way the __new__ method works in the Python documentation .","title":"Further Reading"},{"location":"python/kata/","text":"Python Kata Kata #1: Pipe and Filter in Python Kata #2: The Galaxy Empire salaries Kata #3: World population Kata #4: Hello Mars!","title":"Kata"},{"location":"python/kata/#python-kata","text":"Kata #1: Pipe and Filter in Python Kata #2: The Galaxy Empire salaries Kata #3: World population Kata #4: Hello Mars!","title":"Python Kata"},{"location":"python/kata/python-kata-galaxy-empire-salaries/","text":"Python Kata #2: The Galaxy Empire Salaries You are given the information about the salaries of all the employees in the Galaxy Empire. The information is a dataset in CSV format with header row. How would you compute the average, the count, and the minimum and the maximum values for the salary column?","title":"Python Kata #2: The Galaxy Empire Salaries"},{"location":"python/kata/python-kata-galaxy-empire-salaries/#python-kata-2-the-galaxy-empire-salaries","text":"You are given the information about the salaries of all the employees in the Galaxy Empire. The information is a dataset in CSV format with header row. How would you compute the average, the count, and the minimum and the maximum values for the salary column?","title":"Python Kata #2: The Galaxy Empire Salaries"},{"location":"python/kata/python-kata-hello-mars/","text":"Python Kata #4: Hello Mars! Me and my friends like to travel to other planets. To keep track on which planets we visited I created a Python VisitTracker class. class VisitTracker : visited = [] def __init__ ( self , name ): self . name = name def visit ( self , place ): if place not in self . visited : self . visited . append ( place ) def list ( self ): print ( \"====== \" + self . name + \"'s visits ======\" ) print ( self . visited ) I put all the Python source code in an Jupyter Notebook (I made it available for you as GitHub gist ) and started using it. Added recent planetary visits John and Jane made: John visited Saturn together with Jane John visited Mars Jane visited Jupiter Here is how I tracked this: johns_visits = VisitTracker ( 'John' ) janes_visits = VisitTracker ( 'Jane' ) #1 John visited Saturn together with Jane johns_visits . visit ( 'Saturn' ) janes_visits . visit ( 'Saturn' ) #2 John visited Mars johns_visits . visit ( 'Mars' ) johns_visits . list () #3 Jane visited Jupiter janes_visits . visit ( 'Jupiter' ) janes_visits . list () And the output was not quite what I was expecting: ====== John's visits ====== ['Saturn', 'Mars'] ====== Jane's visits ====== ['Saturn', 'Mars', 'Jupiter'] I was expecting: ====== John's visits ====== ['Saturn', 'Mars'] ====== Jane's visits ====== ['Saturn', 'Jupiter'] If you do not believe me - check the Jupyter Notebook with the Python code and the results in Colab . What went wrong?","title":"Python Kata #4: Hello Mars!"},{"location":"python/kata/python-kata-hello-mars/#python-kata-4-hello-mars","text":"Me and my friends like to travel to other planets. To keep track on which planets we visited I created a Python VisitTracker class. class VisitTracker : visited = [] def __init__ ( self , name ): self . name = name def visit ( self , place ): if place not in self . visited : self . visited . append ( place ) def list ( self ): print ( \"====== \" + self . name + \"'s visits ======\" ) print ( self . visited ) I put all the Python source code in an Jupyter Notebook (I made it available for you as GitHub gist ) and started using it. Added recent planetary visits John and Jane made: John visited Saturn together with Jane John visited Mars Jane visited Jupiter Here is how I tracked this: johns_visits = VisitTracker ( 'John' ) janes_visits = VisitTracker ( 'Jane' ) #1 John visited Saturn together with Jane johns_visits . visit ( 'Saturn' ) janes_visits . visit ( 'Saturn' ) #2 John visited Mars johns_visits . visit ( 'Mars' ) johns_visits . list () #3 Jane visited Jupiter janes_visits . visit ( 'Jupiter' ) janes_visits . list () And the output was not quite what I was expecting: ====== John's visits ====== ['Saturn', 'Mars'] ====== Jane's visits ====== ['Saturn', 'Mars', 'Jupiter'] I was expecting: ====== John's visits ====== ['Saturn', 'Mars'] ====== Jane's visits ====== ['Saturn', 'Jupiter'] If you do not believe me - check the Jupyter Notebook with the Python code and the results in Colab . What went wrong?","title":"Python Kata #4: Hello Mars!"},{"location":"python/kata/python-kata-pipe-and-filter/","text":"Python Kata #1: Pipe and Filter The Pipe and Filter is a very popular architecture style. It can be found in may software development frameworks, like Spark, JavaScript, etc. Linux command line shell also supports piping. You can run one command and direct the output of that command to another command. The output of the second command can be directed to a third command and so on and so forth. We want to have something similar in Python - piping or chaining together series of transformations on a sequence. Transformations are applied with following patterns: map - takes as an argument a transformation. When the pipeline is executed, the transformation is applied to each element from the input and is placed in the output. flat_map - takes as an argument a transformation. The transformation is expected to produce iterable. When the pipeline is executed, the transformation is applied to each element from the input, each element from the resulting iterable is placed into the output pipeline. filter - takes as an argument a bool function. Function is applied for each element in the input. If the filter function returns true, the element is placed in the output. Otherwise the element is dropped. To implement these patterns you might find useful the Python functions map , filter , `reduce (See Map, Filter and Reduce at Intermediate Python) Do not forget to test your solution! Here is an example what it might look like using the implementation. Example: Input: a list of strings, e.g. ['Lorem ipsum', 'dolorem costum'] Transformation 1: convert all items to lower case, e.g. ['lorem ipsum', 'dolorem costum'] Transformation 2: split each element into, e.g. ['lorem', 'ipsum', 'dolorem', 'costum'] Transformation 3: remove all words ending with 'em', e.g. ['ipsum', 'costum'] Python code: pipe = ( Pipe ([ 'Lorem ipsum' , 'dolorem costum' ]) . map ( lambda x : x . lower ()) . flat_map ( lambda x : x . split ( ' ' )) . filter ( lambda x : not x . endswith ( 'em' ))) for item in pipe : print ( item ) Output: ipsum costum","title":"Python Kata #1: Pipe and Filter"},{"location":"python/kata/python-kata-pipe-and-filter/#python-kata-1-pipe-and-filter","text":"The Pipe and Filter is a very popular architecture style. It can be found in may software development frameworks, like Spark, JavaScript, etc. Linux command line shell also supports piping. You can run one command and direct the output of that command to another command. The output of the second command can be directed to a third command and so on and so forth. We want to have something similar in Python - piping or chaining together series of transformations on a sequence. Transformations are applied with following patterns: map - takes as an argument a transformation. When the pipeline is executed, the transformation is applied to each element from the input and is placed in the output. flat_map - takes as an argument a transformation. The transformation is expected to produce iterable. When the pipeline is executed, the transformation is applied to each element from the input, each element from the resulting iterable is placed into the output pipeline. filter - takes as an argument a bool function. Function is applied for each element in the input. If the filter function returns true, the element is placed in the output. Otherwise the element is dropped. To implement these patterns you might find useful the Python functions map , filter , `reduce (See Map, Filter and Reduce at Intermediate Python) Do not forget to test your solution! Here is an example what it might look like using the implementation. Example: Input: a list of strings, e.g. ['Lorem ipsum', 'dolorem costum'] Transformation 1: convert all items to lower case, e.g. ['lorem ipsum', 'dolorem costum'] Transformation 2: split each element into, e.g. ['lorem', 'ipsum', 'dolorem', 'costum'] Transformation 3: remove all words ending with 'em', e.g. ['ipsum', 'costum'] Python code: pipe = ( Pipe ([ 'Lorem ipsum' , 'dolorem costum' ]) . map ( lambda x : x . lower ()) . flat_map ( lambda x : x . split ( ' ' )) . filter ( lambda x : not x . endswith ( 'em' ))) for item in pipe : print ( item ) Output: ipsum costum","title":"Python Kata #1: Pipe and Filter"},{"location":"python/kata/python-kata-world-population/","text":"Python Kata #3: World population You are involved in a campaign against a lethal virus. You just received the health status information for the whole world population in a CSV file with a header line. You have a to work on number of reports, generated from the dataset. Can you create a Python generator to help you with the reports? Each item returned from the generator is a namedtuple with fields corresponding to the columns of the CSV file.","title":"Python Kata #3: World population"},{"location":"python/kata/python-kata-world-population/#python-kata-3-world-population","text":"You are involved in a campaign against a lethal virus. You just received the health status information for the whole world population in a CSV file with a header line. You have a to work on number of reports, generated from the dataset. Can you create a Python generator to help you with the reports? Each item returned from the generator is a namedtuple with fields corresponding to the columns of the CSV file.","title":"Python Kata #3: World population"},{"location":"python/kata/drafts/python-kata-count-lines-in-multiple-files/","text":"Here is one possible solution .","title":"Python kata count lines in multiple files"},{"location":"python/kata/drafts/python-kata-how-would-you-test-this/","text":"Python Kata #?: How would you test this? You have to implement a function which reads a csv file and returns Python generator of rows. The first line of the csv file is a header. Each row is a namedtuple object with named fields according to the header. File must be closed after the generator is exhausted or stopped. Throws FormatError exception in case of parsing errors. from collections import namedtuple def read_csv ( file_name ): with open ( file_name ) as file : lines = ( line for line in file ) data_rows = ( row for row in csv . reader ( lines )) header_row = next ( data_rows ) Row = namedtuple ( 'Row' , header_row ) rows = ( Row ( * row ) for row in data_rows ) return rows","title":"Python Kata #?: How would you test this?"},{"location":"python/kata/drafts/python-kata-how-would-you-test-this/#python-kata-how-would-you-test-this","text":"You have to implement a function which reads a csv file and returns Python generator of rows. The first line of the csv file is a header. Each row is a namedtuple object with named fields according to the header. File must be closed after the generator is exhausted or stopped. Throws FormatError exception in case of parsing errors. from collections import namedtuple def read_csv ( file_name ): with open ( file_name ) as file : lines = ( line for line in file ) data_rows = ( row for row in csv . reader ( lines )) header_row = next ( data_rows ) Row = namedtuple ( 'Row' , header_row ) rows = ( Row ( * row ) for row in data_rows ) return rows","title":"Python Kata #?: How would you test this?"},{"location":"python/kata/drafts/python-kata-iterate-over-hierarchical-sources%20%28long%20draft%29/","text":"Iterate over hierarchical sources in Python Problem Scenario We are given a list of files and we need to insert the words from files into a database. The sequence of the words should be preserved. Here is one way to achieve this: file_list = [ 'a.txt' , 'b.txt' ] for file_name in file_list : with open ( file_name , 'r' ) as file : for line in file . readline (): for word in line . split ( ' ' ): with db . connection . cursor () as cursor : cursor . execute ( 'INSERT INTO words(word) VALUES (?)' , ( word ,)) cursor . commit () Wow. Not very easy to read already, but it serves the purpose. How about testing? You think - it is pretty straightforward. \"I did couple of exploration tests during development. I am pretty confident it works!\" Well ... wait to see what comes next. Day 1. New requirements: Stop words should be inserted into a table stop_words . Rest of the words are still going into the words table. Day 2. New requirements: Our database administrator has some concerns about the performance and wants that the commit statement happens after 1000 words has been inserted into each table. This means you keep track on number of inserted words in each table separately. When any of the table reaches 1000 you commit inserts for that table and reset the counter for that table. Day 3. New requirement: We should be able to extract words from Word files. Bug: words are not split on punctuation. Bug: multiple spaces cause empty words to be inserted. New requirement: words need to be stored in lower case. Week 5. You worked very hard, but you are still unable to finish the Word files processing. Your manager asks you to deliver without this feature. She also assigns three new developers to your project as new requirements arrive. You need to be able to process PDF files Week 25. Everything works more or less fine. Now the privacy team comes into the picture. Only approved files should be processed. You receive a list of approved files in a CSV file. You need to be able to OCR images You need to create a build pipeline for continuous integration The solution needs to implement unit tests with at least 70% coverage. Unit tests should be executed at build time. Year 1. Privacy team has provided REST API to validate the file eligibility for processing. Can you remember how the whole this hell thing works? Real-time processing! List of files is stored in a database table. It is updated constantly. Our solution needs to process each new file entry as soon as possible. We want that words are inserted not only into the database, but also sent to a message queue. What if? What if our processing looks like this: for word in extract_words (): process_word ( word ) Even better: db_processor = word_process . get_database_processor () extractor = word_extract . get () reader . when_new_word ( processor . process ) Solution Let's start with the original requirements. Can we make the code more readable","title":"Iterate over hierarchical sources in Python"},{"location":"python/kata/drafts/python-kata-iterate-over-hierarchical-sources%20%28long%20draft%29/#iterate-over-hierarchical-sources-in-python","text":"","title":"Iterate over hierarchical sources in Python"},{"location":"python/kata/drafts/python-kata-iterate-over-hierarchical-sources%20%28long%20draft%29/#problem","text":"","title":"Problem"},{"location":"python/kata/drafts/python-kata-iterate-over-hierarchical-sources%20%28long%20draft%29/#scenario","text":"We are given a list of files and we need to insert the words from files into a database. The sequence of the words should be preserved. Here is one way to achieve this: file_list = [ 'a.txt' , 'b.txt' ] for file_name in file_list : with open ( file_name , 'r' ) as file : for line in file . readline (): for word in line . split ( ' ' ): with db . connection . cursor () as cursor : cursor . execute ( 'INSERT INTO words(word) VALUES (?)' , ( word ,)) cursor . commit () Wow. Not very easy to read already, but it serves the purpose. How about testing? You think - it is pretty straightforward. \"I did couple of exploration tests during development. I am pretty confident it works!\" Well ... wait to see what comes next. Day 1. New requirements: Stop words should be inserted into a table stop_words . Rest of the words are still going into the words table. Day 2. New requirements: Our database administrator has some concerns about the performance and wants that the commit statement happens after 1000 words has been inserted into each table. This means you keep track on number of inserted words in each table separately. When any of the table reaches 1000 you commit inserts for that table and reset the counter for that table. Day 3. New requirement: We should be able to extract words from Word files. Bug: words are not split on punctuation. Bug: multiple spaces cause empty words to be inserted. New requirement: words need to be stored in lower case. Week 5. You worked very hard, but you are still unable to finish the Word files processing. Your manager asks you to deliver without this feature. She also assigns three new developers to your project as new requirements arrive. You need to be able to process PDF files Week 25. Everything works more or less fine. Now the privacy team comes into the picture. Only approved files should be processed. You receive a list of approved files in a CSV file. You need to be able to OCR images You need to create a build pipeline for continuous integration The solution needs to implement unit tests with at least 70% coverage. Unit tests should be executed at build time. Year 1. Privacy team has provided REST API to validate the file eligibility for processing. Can you remember how the whole this hell thing works? Real-time processing! List of files is stored in a database table. It is updated constantly. Our solution needs to process each new file entry as soon as possible. We want that words are inserted not only into the database, but also sent to a message queue.","title":"Scenario"},{"location":"python/kata/drafts/python-kata-iterate-over-hierarchical-sources%20%28long%20draft%29/#what-if","text":"What if our processing looks like this: for word in extract_words (): process_word ( word ) Even better: db_processor = word_process . get_database_processor () extractor = word_extract . get () reader . when_new_word ( processor . process )","title":"What if?"},{"location":"python/kata/drafts/python-kata-iterate-over-hierarchical-sources%20%28long%20draft%29/#solution","text":"Let's start with the original requirements. Can we make the code more readable","title":"Solution"},{"location":"python/kata/drafts/python-kata-iterate-over-hierarchical-sources/","text":"Iterate over hierarchical sources in Python Problem We are given a list of files and we need to insert the words from files into a database. The sequence of the words should be preserved. Here is one way to achieve this: file_list = [ 'a.txt' , 'b.txt' ] for file_name in file_list : with open ( file_name , 'r' ) as file : for line in file : for word in line . split ( ' ' ): with db . connection . cursor () as cursor : cursor . execute ( 'INSERT INTO words(word) VALUES (?)' , ( word ,)) cursor . commit () How do you test such code? Maybe you use something I call 'evolutionary testing' - you start with the outer loop. Add a print statement, run a few times, comment the print statement. And you continue until it is done. Ok. This might work at the beginning. But how you test your code continuously? There are situations where you need to verify the code is working correctly. For example: New requirements - e.g. process PDF, image files etc. Dependency upgrade - e.g. changing to a higher version of Python Change the target database Issue fixing Error handling This code is already difficult to read and understand. You want to improve your code - make it testable, more readable and maintainable, more flexible and extensible. Discussion Look at the code. Isn't it doing too much things? Solution Let's start with the responsibilities. Iterate over a list of files Process a file - iterate over the lines from a file Get the lines from a file Process a line - iterate over the words in a line Get the words from a line Process a word - save a word into the database So we might come with an object-oriented solution like this: class WordLoader : def _get_line_words ( self , line ): return line . split ( ' ' ) def _get_file_lines ( file_name ): with open ( file_name , 'r' ) as file : return f def _process_word ( self , word ): with db . connection . cursor () as cursor : cursor . execute ( 'INSERT INTO words(word) VALUES (?)' , ( word ,)) cursor . commit () def _process_line ( self , line ): for word in self . _get_line_words ( line ): self . _process_word ( word ) def _process_file ( self , file_name ): for line in self . _get_file_lines ( file_name ): self . _process_line ( line ) def __call__ ( self , file_list ): for file_name in file_list : self . _process_file ( file_name ) Looks much better now. We can test each method in isolation with unit tests. However I still have some concerns. The WordLoader class we created takes too much responsibilities. What is going on is still not very visible. To understand what is going on, I need to dig into the whole chain of execution. Can I make my code more expressive so that I can read the code from the very beginning like this: \"Insert all the words from a list of files into the database.\" Maybe my main code could look like this? for word in words_from_files : save_word ( word ) Where the words_from_files comes from? files = get_files_from_name_list ( file_list ) lines_from_files = get_lines_from_files ( files ) words_from_files = get_words_from_lines ( lines_from_files ) Wait! You are going to load all these terabytes and petabytes into the memory? Good point. Not necessarily. Instead of returning list, I could use generators. def get_files_from_name_list ( filename_list ): for filename in filename_list : with open ( filename , 'r' ) as file : yield file def get_lines_from_files ( files ): for file in files : for line in file : yield file def get_words_from_lines ( lines ): for line in lines : for word in line . split ( ' ' ) yield word The code is much more expressive now. There are two things I do not like: functions look almost the same functions are doing more than one thing. Let's take get_lines_from_files as an example. It iterates over the files in a list, unpacks the file into lines, using split() method and iterates over the resulting words. def unpack_containers ( containers , unpack ): for container in containers : for item in unpack ( container ): yield item def unpack_filename ( filename ): with open ( filename , 'r' ) as f : yield [ f ] def unpack_file_lines ( file ): return file def unpack_line_words ( line ): for word in line . split ( ' ' ): yeild word files = unpack_containers ( file_list , lambda filename : [ open ( filename , 'r' )]) lines_from_files = get_lines_from_files ( files ) words_from_files = get_words_from_lines ( lines_from_files ) files = map ( open_file , file_list ) lines = chain . from_iterable ( files ) words = chain . from_iterable ( map ( lambda line : line . split ( ' ' ), lines )) def open_file ( filename ): with open ( filename , 'r' ) as f : yield f def get_words_from_line ( line ): return line . split ( ' ' ) def flat_map ( function , iterable , * arg ): arg . insert ( 0 , iterable ) chain . from_iterable ( map ( function , * arg )) lines = map ( open_file , file_list ) words = flat_map ( get_words_from_line , lines ) def list_new_events (): with db . cursor () as c : c . execute ( \"SELECT * FROM events WHERE status='N'\" ) yield c . fetchall () def repeat ( function , before = None , after = None ): while True : if before is not None : before () yield function () if after is not None : after () def reset_process (): pass def process ( event ): pass event_stream = chain . from_iterable ( repeat , after = reset_process ) for event in event_stream : process ( event )","title":"Iterate over hierarchical sources in Python"},{"location":"python/kata/drafts/python-kata-iterate-over-hierarchical-sources/#iterate-over-hierarchical-sources-in-python","text":"","title":"Iterate over hierarchical sources in Python"},{"location":"python/kata/drafts/python-kata-iterate-over-hierarchical-sources/#problem","text":"We are given a list of files and we need to insert the words from files into a database. The sequence of the words should be preserved. Here is one way to achieve this: file_list = [ 'a.txt' , 'b.txt' ] for file_name in file_list : with open ( file_name , 'r' ) as file : for line in file : for word in line . split ( ' ' ): with db . connection . cursor () as cursor : cursor . execute ( 'INSERT INTO words(word) VALUES (?)' , ( word ,)) cursor . commit () How do you test such code? Maybe you use something I call 'evolutionary testing' - you start with the outer loop. Add a print statement, run a few times, comment the print statement. And you continue until it is done. Ok. This might work at the beginning. But how you test your code continuously? There are situations where you need to verify the code is working correctly. For example: New requirements - e.g. process PDF, image files etc. Dependency upgrade - e.g. changing to a higher version of Python Change the target database Issue fixing Error handling This code is already difficult to read and understand. You want to improve your code - make it testable, more readable and maintainable, more flexible and extensible.","title":"Problem"},{"location":"python/kata/drafts/python-kata-iterate-over-hierarchical-sources/#discussion","text":"Look at the code. Isn't it doing too much things?","title":"Discussion"},{"location":"python/kata/drafts/python-kata-iterate-over-hierarchical-sources/#solution","text":"Let's start with the responsibilities. Iterate over a list of files Process a file - iterate over the lines from a file Get the lines from a file Process a line - iterate over the words in a line Get the words from a line Process a word - save a word into the database So we might come with an object-oriented solution like this: class WordLoader : def _get_line_words ( self , line ): return line . split ( ' ' ) def _get_file_lines ( file_name ): with open ( file_name , 'r' ) as file : return f def _process_word ( self , word ): with db . connection . cursor () as cursor : cursor . execute ( 'INSERT INTO words(word) VALUES (?)' , ( word ,)) cursor . commit () def _process_line ( self , line ): for word in self . _get_line_words ( line ): self . _process_word ( word ) def _process_file ( self , file_name ): for line in self . _get_file_lines ( file_name ): self . _process_line ( line ) def __call__ ( self , file_list ): for file_name in file_list : self . _process_file ( file_name ) Looks much better now. We can test each method in isolation with unit tests. However I still have some concerns. The WordLoader class we created takes too much responsibilities. What is going on is still not very visible. To understand what is going on, I need to dig into the whole chain of execution. Can I make my code more expressive so that I can read the code from the very beginning like this: \"Insert all the words from a list of files into the database.\" Maybe my main code could look like this? for word in words_from_files : save_word ( word ) Where the words_from_files comes from? files = get_files_from_name_list ( file_list ) lines_from_files = get_lines_from_files ( files ) words_from_files = get_words_from_lines ( lines_from_files ) Wait! You are going to load all these terabytes and petabytes into the memory? Good point. Not necessarily. Instead of returning list, I could use generators. def get_files_from_name_list ( filename_list ): for filename in filename_list : with open ( filename , 'r' ) as file : yield file def get_lines_from_files ( files ): for file in files : for line in file : yield file def get_words_from_lines ( lines ): for line in lines : for word in line . split ( ' ' ) yield word The code is much more expressive now. There are two things I do not like: functions look almost the same functions are doing more than one thing. Let's take get_lines_from_files as an example. It iterates over the files in a list, unpacks the file into lines, using split() method and iterates over the resulting words. def unpack_containers ( containers , unpack ): for container in containers : for item in unpack ( container ): yield item def unpack_filename ( filename ): with open ( filename , 'r' ) as f : yield [ f ] def unpack_file_lines ( file ): return file def unpack_line_words ( line ): for word in line . split ( ' ' ): yeild word files = unpack_containers ( file_list , lambda filename : [ open ( filename , 'r' )]) lines_from_files = get_lines_from_files ( files ) words_from_files = get_words_from_lines ( lines_from_files ) files = map ( open_file , file_list ) lines = chain . from_iterable ( files ) words = chain . from_iterable ( map ( lambda line : line . split ( ' ' ), lines )) def open_file ( filename ): with open ( filename , 'r' ) as f : yield f def get_words_from_line ( line ): return line . split ( ' ' ) def flat_map ( function , iterable , * arg ): arg . insert ( 0 , iterable ) chain . from_iterable ( map ( function , * arg )) lines = map ( open_file , file_list ) words = flat_map ( get_words_from_line , lines ) def list_new_events (): with db . cursor () as c : c . execute ( \"SELECT * FROM events WHERE status='N'\" ) yield c . fetchall () def repeat ( function , before = None , after = None ): while True : if before is not None : before () yield function () if after is not None : after () def reset_process (): pass def process ( event ): pass event_stream = chain . from_iterable ( repeat , after = reset_process ) for event in event_stream : process ( event )","title":"Solution"},{"location":"python/kata/drafts/python-kata-pipe-and-filter-solution/","text":"Pipe and Filter in Python The Pipe and Filter is a very popular architecture style. It can be found in may software development frameworks, like Spark, JavaScript, etc. Linux command line shell also supports piping. You can run one command and direct the output of that command to another command. The output of the second command can be directed to a third command and so on and so forth. We want to have something similar in Python - piping or chaining together series of transformations on a sequence. Transformations are applied with following patterns: map - takes as an argument a transformation. When the pipeline is executed, the transformation is applied to each element from the input and is placed in the output. flat_map - takes as an argument a transformation. The transformation is expected to produce iterable. When the pipeline is executed, the transformation is applied to each element from the input, each element from the resulting iterable is placed into the output pipeline. Here is an example what it might look like using the implementation. Example: Input: a list of integers [4,6,9] Transformation 1: divide each integer by 2 and convert to integer Transformation 2: convert each item into a sequence, e.g. 1 becomes [1], 2 becomes [2, 2], 3 becomes [3,3,3], etc. Flatten the result. Python code: pipe = ( Pipe ([ 4 , 6 , 9 ]) . map ( lambda x : x / 2 ) . flat_map ( lambda x : str ( x ) * int ( x ))) for item in pipe : print ( item ) Output: 2 2 3 3 3 4 4 4 4","title":"Pipe and Filter in Python"},{"location":"python/kata/drafts/python-kata-pipe-and-filter-solution/#pipe-and-filter-in-python","text":"The Pipe and Filter is a very popular architecture style. It can be found in may software development frameworks, like Spark, JavaScript, etc. Linux command line shell also supports piping. You can run one command and direct the output of that command to another command. The output of the second command can be directed to a third command and so on and so forth. We want to have something similar in Python - piping or chaining together series of transformations on a sequence. Transformations are applied with following patterns: map - takes as an argument a transformation. When the pipeline is executed, the transformation is applied to each element from the input and is placed in the output. flat_map - takes as an argument a transformation. The transformation is expected to produce iterable. When the pipeline is executed, the transformation is applied to each element from the input, each element from the resulting iterable is placed into the output pipeline. Here is an example what it might look like using the implementation. Example: Input: a list of integers [4,6,9] Transformation 1: divide each integer by 2 and convert to integer Transformation 2: convert each item into a sequence, e.g. 1 becomes [1], 2 becomes [2, 2], 3 becomes [3,3,3], etc. Flatten the result. Python code: pipe = ( Pipe ([ 4 , 6 , 9 ]) . map ( lambda x : x / 2 ) . flat_map ( lambda x : str ( x ) * int ( x ))) for item in pipe : print ( item ) Output: 2 2 3 3 3 4 4 4 4","title":"Pipe and Filter in Python"},{"location":"python/kata/drafts/python-kata-public-transportation-solution/","text":"Public Transportation - Python Kata Solution Solution 1 Your solution might look like the following: import os STAGING_DIR = '.' while True : for filename in os . listdir ( STAGING_DIR ): filepath = os . path . join ( STAGING_DIR , filename ) if os . path . isfile ( filepath ): with open ( filepath , 'r' ) as file : for line in file : for word in line . split ( ' ' ): save_word ( word ) Although this is not the worst solution I have seen, it has some weaknesses: Deep statement nesting at 6 nesting levels. This makes the code very difficult. Where should I add the sleep calls? No error handling. If you add try-except blocks, nesting will become even deeper. Difficult to test. You might have followed the \"growing onion layers\" approach which helped in the initial development, but how would you test changes, bug fixes, Python version upgrades? Difficult to reuse. Actually templating or as I prefer calling it - copy/paste/edit - is the only way to reuse such a code. Solution 2 class WordLoader : def _get_line_words ( self , line ): return line . split ( ' ' ) def _get_file_lines ( file_name ): with open ( file_name , 'r' ) as file : yield file def _process_word ( self , word ): with db . connection . cursor () as cursor : cursor . execute ( 'INSERT INTO words(word) VALUES (?)' , ( word ,)) cursor . commit () def _process_line ( self , line ): for word in self . _get_line_words ( line ): self . _process_word ( word ) def _process_file ( self , file_name ): for line in self . _get_file_lines ( file_name ): self . _process_line ( line ) def __call__ ( self , file_list ): for file_name in file_list : self . _process_file ( file_name ) load_file_list = FileLoader () while True : file_list = [ f for f in os . listdir ( STAGING_DIR ) if os . path . isfile ( os . path . join ( STAGING_DIR , f ))] load_file_list ( file_list ) Solution 3 def repeat ( function , before = None , after = None ): while True : try : value = function () except StopIteration : break if before is not None : before ( value ) yield value if after is not None : after ( value ) def pipeline ( iterable , function , before = None , after = None ): for item in terable : if before is not None : item = before ( item ) function ( item ) if after is not None : after ( item ) def ls_staged_files (): file_list = [ f for f in os . listdir ( STAGING_DIR ) if \\ os . path . isfile ( os . path . join ( STAGING_DIR , f ))] return file_list def after_file_list ( item ): time . sleep ( 3 ) def get_lines_from_files ( files ): for filename in files : filepath = os . path . join ( STAGING_DIR , filename ) with open ( filepath , 'r' ) as file : for line in file : yield line def get_words_from_lines ( lines ): for line in lines : for word in line . split ( ' ' ): yield word def open_files ( filenames ): for filename in filenames : filepath = os . path . join ( STAGING_DIR , filename ) with open ( filepath , 'r' ) as file : yield file STAGING_DIR = '.' filelist_sequence = repeat ( ls_staged_files , after = after_file_list ) filename_sequence = itertools . chain . from_iterable ( filelist_sequence ) file_sequence = open_files ( filename_sequence ) file_sequence = itertools . chain . from_iterable ( repeat ( ls_staged_files , after = after_file_list )) line_sequence = get_lines_from_files ( file_sequence ) word_sequence = get_words_from_lines ( line_sequence ) for word in word_sequence : print ( word )","title":"Public Transportation - Python Kata Solution"},{"location":"python/kata/drafts/python-kata-public-transportation-solution/#public-transportation-python-kata-solution","text":"","title":"Public Transportation - Python Kata Solution"},{"location":"python/kata/drafts/python-kata-public-transportation-solution/#solution-1","text":"Your solution might look like the following: import os STAGING_DIR = '.' while True : for filename in os . listdir ( STAGING_DIR ): filepath = os . path . join ( STAGING_DIR , filename ) if os . path . isfile ( filepath ): with open ( filepath , 'r' ) as file : for line in file : for word in line . split ( ' ' ): save_word ( word ) Although this is not the worst solution I have seen, it has some weaknesses: Deep statement nesting at 6 nesting levels. This makes the code very difficult. Where should I add the sleep calls? No error handling. If you add try-except blocks, nesting will become even deeper. Difficult to test. You might have followed the \"growing onion layers\" approach which helped in the initial development, but how would you test changes, bug fixes, Python version upgrades? Difficult to reuse. Actually templating or as I prefer calling it - copy/paste/edit - is the only way to reuse such a code.","title":"Solution 1"},{"location":"python/kata/drafts/python-kata-public-transportation-solution/#solution-2","text":"class WordLoader : def _get_line_words ( self , line ): return line . split ( ' ' ) def _get_file_lines ( file_name ): with open ( file_name , 'r' ) as file : yield file def _process_word ( self , word ): with db . connection . cursor () as cursor : cursor . execute ( 'INSERT INTO words(word) VALUES (?)' , ( word ,)) cursor . commit () def _process_line ( self , line ): for word in self . _get_line_words ( line ): self . _process_word ( word ) def _process_file ( self , file_name ): for line in self . _get_file_lines ( file_name ): self . _process_line ( line ) def __call__ ( self , file_list ): for file_name in file_list : self . _process_file ( file_name ) load_file_list = FileLoader () while True : file_list = [ f for f in os . listdir ( STAGING_DIR ) if os . path . isfile ( os . path . join ( STAGING_DIR , f ))] load_file_list ( file_list )","title":"Solution 2"},{"location":"python/kata/drafts/python-kata-public-transportation-solution/#solution-3","text":"def repeat ( function , before = None , after = None ): while True : try : value = function () except StopIteration : break if before is not None : before ( value ) yield value if after is not None : after ( value ) def pipeline ( iterable , function , before = None , after = None ): for item in terable : if before is not None : item = before ( item ) function ( item ) if after is not None : after ( item ) def ls_staged_files (): file_list = [ f for f in os . listdir ( STAGING_DIR ) if \\ os . path . isfile ( os . path . join ( STAGING_DIR , f ))] return file_list def after_file_list ( item ): time . sleep ( 3 ) def get_lines_from_files ( files ): for filename in files : filepath = os . path . join ( STAGING_DIR , filename ) with open ( filepath , 'r' ) as file : for line in file : yield line def get_words_from_lines ( lines ): for line in lines : for word in line . split ( ' ' ): yield word def open_files ( filenames ): for filename in filenames : filepath = os . path . join ( STAGING_DIR , filename ) with open ( filepath , 'r' ) as file : yield file STAGING_DIR = '.' filelist_sequence = repeat ( ls_staged_files , after = after_file_list ) filename_sequence = itertools . chain . from_iterable ( filelist_sequence ) file_sequence = open_files ( filename_sequence ) file_sequence = itertools . chain . from_iterable ( repeat ( ls_staged_files , after = after_file_list )) line_sequence = get_lines_from_files ( file_sequence ) word_sequence = get_words_from_lines ( line_sequence ) for word in word_sequence : print ( word )","title":"Solution 3"},{"location":"python/kata/drafts/python-kata-public-transportation/","text":"Public Transportation - Python Kata Imagine you live in Amsterdam and you want to travel to Paris. Of course you want to travel as soon as possible, but you have to take in account the railways timetable. Railways work in batch mode. They come on a schedule, pick everybody from the train station and move them. In our software development practice we face similar problems every day. Here is a similar one. Problem Files are being delivered by multiple source systems into a staging folder. These files are our passengers. You need to create a solution which periodically is taking a list of available files and is processing them by extracting the words from the files and inserting into a database table. This is processing solution is our train. In fact what we are creating is a pipeline. The pipeline is using batches - the list of available files. Think about following: How do you test your solution? How do you handle exceptions? How easy is for other people (or you after a year) to read and understand your solution? How would you add pre- and post-processing of words to your pipeline? For example, as pre-processing you might need to convert the word to lower case, strip punctuation characters. Post-processing might be to sleep for 2 seconds. How would you add pre- and post-processing of batches to your pipeline? For example, as post-processing, you might want to sleep for 20 seconds. How would you reuse this solution in other scenarios? For example - your staging is a database table and as processing an email is sent.","title":"Public Transportation - Python Kata"},{"location":"python/kata/drafts/python-kata-public-transportation/#public-transportation-python-kata","text":"Imagine you live in Amsterdam and you want to travel to Paris. Of course you want to travel as soon as possible, but you have to take in account the railways timetable. Railways work in batch mode. They come on a schedule, pick everybody from the train station and move them. In our software development practice we face similar problems every day. Here is a similar one.","title":"Public Transportation - Python Kata"},{"location":"python/kata/drafts/python-kata-public-transportation/#problem","text":"Files are being delivered by multiple source systems into a staging folder. These files are our passengers. You need to create a solution which periodically is taking a list of available files and is processing them by extracting the words from the files and inserting into a database table. This is processing solution is our train. In fact what we are creating is a pipeline. The pipeline is using batches - the list of available files. Think about following: How do you test your solution? How do you handle exceptions? How easy is for other people (or you after a year) to read and understand your solution? How would you add pre- and post-processing of words to your pipeline? For example, as pre-processing you might need to convert the word to lower case, strip punctuation characters. Post-processing might be to sleep for 2 seconds. How would you add pre- and post-processing of batches to your pipeline? For example, as post-processing, you might want to sleep for 20 seconds. How would you reuse this solution in other scenarios? For example - your staging is a database table and as processing an email is sent.","title":"Problem"},{"location":"python/misc/","text":"Misc Python Topics Document REST APIs based on Python Django's Rest Framework on October 12 th , 2020, in Python :: Misc Topics Add OpeanAPI documentation and SwaggerUI to your Django Rest Framework project on October 12 th , 2020, in Python :: Misc Topics Decode and Validate Azure Active Directory Token in Python on August 31 st , 2020, in Python :: Misc Topics Truncate table with pyodbc on August 17 th , 2020, in Python :: Misc Topics","title":"Misc Python Topics"},{"location":"python/misc/#misc-python-topics","text":"Document REST APIs based on Python Django's Rest Framework on October 12 th , 2020, in Python :: Misc Topics Add OpeanAPI documentation and SwaggerUI to your Django Rest Framework project on October 12 th , 2020, in Python :: Misc Topics Decode and Validate Azure Active Directory Token in Python on August 31 st , 2020, in Python :: Misc Topics Truncate table with pyodbc on August 17 th , 2020, in Python :: Misc Topics","title":"Misc Python Topics"},{"location":"python/misc/python-azure-ad-token-decode-validate/","text":"Decode and Validate JWT Token from Azure Active Directory in Python Problem You need to decode and validate JWT Token issued by Azure Active Directory. Example scenario is where you have a Web Application which is using BFF (Backend for Frontend) API. Users are authenticated by the front-end application using Azure AD and the token is forwarded to the BFF API. The BFF API needs to validate the received token since the client is outside of its trust boundary. Solution Use pyjwt and cryptography packages. I have created a small package aadtoken to help with getting the Azure Active Directory public key and decode the token using, using pyjwt and cryptography . Further to decode the token use the jwt.decode function from the pyjwat package. All the sources are available in GitHub . Here I am providing only an example how to use the aadtoken helper package along with jwt.decode : import os import sys import jwt from aadtoken import get_public_key client_id = os . environ . get ( 'CLIENT_ID' , '<your-webapp-id-goes-here>' ) tenant_id = os . environ . get ( 'TENANT_ID' , '<your-tenant-id-goes-here>' ) if len ( sys . argv ) > 1 : token = sys . argv [ 1 ] else : token = os . environ . get ( 'TOKEN' , \"<your-token-goes-here>\" ) issuer = 'https://sts.windows.net/ {tenant_id} /' . format ( tenant_id = tenant_id ) public_key = get_public_key ( token ) decoded = jwt . decode ( token , public_key , verify = True , algorithms = [ 'RS256' ], audience = [ client_id ], issuer = issuer ) print ( decoded ) You need to replace the placeholders with actual values. Alternatively you could use environment variables to define the client id, the tenant id and the token. The token id can also be passed as a command line argument: python demo.py <your-token-goes-here> Discussion This solution is based on the Validating JSON web tokens (JWTs) from Azure AD, in Python publication by Roberto Prevato. The solution defines a package which is responsible for discovering the Azure AD endpoints and getting the Azure Active Directory's public key. Requests to Azure Active Directory discovery and keys endpoints are cached. The most important function exported by the package is get_public_key(<token>, [<tenant_id>]) . For given token and tenant ID the function returns the Azure Active Directory public key. The key is used by the jwt.decode function from the pyjwat package to validate and decode the token.","title":"Python azure ad token decode validate"},{"location":"python/misc/python-azure-ad-token-decode-validate/#decode-and-validate-jwt-token-from-azure-active-directory-in-python","text":"","title":"Decode and Validate JWT Token from Azure Active Directory in Python"},{"location":"python/misc/python-azure-ad-token-decode-validate/#problem","text":"You need to decode and validate JWT Token issued by Azure Active Directory. Example scenario is where you have a Web Application which is using BFF (Backend for Frontend) API. Users are authenticated by the front-end application using Azure AD and the token is forwarded to the BFF API. The BFF API needs to validate the received token since the client is outside of its trust boundary.","title":"Problem"},{"location":"python/misc/python-azure-ad-token-decode-validate/#solution","text":"Use pyjwt and cryptography packages. I have created a small package aadtoken to help with getting the Azure Active Directory public key and decode the token using, using pyjwt and cryptography . Further to decode the token use the jwt.decode function from the pyjwat package. All the sources are available in GitHub . Here I am providing only an example how to use the aadtoken helper package along with jwt.decode : import os import sys import jwt from aadtoken import get_public_key client_id = os . environ . get ( 'CLIENT_ID' , '<your-webapp-id-goes-here>' ) tenant_id = os . environ . get ( 'TENANT_ID' , '<your-tenant-id-goes-here>' ) if len ( sys . argv ) > 1 : token = sys . argv [ 1 ] else : token = os . environ . get ( 'TOKEN' , \"<your-token-goes-here>\" ) issuer = 'https://sts.windows.net/ {tenant_id} /' . format ( tenant_id = tenant_id ) public_key = get_public_key ( token ) decoded = jwt . decode ( token , public_key , verify = True , algorithms = [ 'RS256' ], audience = [ client_id ], issuer = issuer ) print ( decoded ) You need to replace the placeholders with actual values. Alternatively you could use environment variables to define the client id, the tenant id and the token. The token id can also be passed as a command line argument: python demo.py <your-token-goes-here>","title":"Solution"},{"location":"python/misc/python-azure-ad-token-decode-validate/#discussion","text":"This solution is based on the Validating JSON web tokens (JWTs) from Azure AD, in Python publication by Roberto Prevato. The solution defines a package which is responsible for discovering the Azure AD endpoints and getting the Azure Active Directory's public key. Requests to Azure Active Directory discovery and keys endpoints are cached. The most important function exported by the package is get_public_key(<token>, [<tenant_id>]) . For given token and tenant ID the function returns the Azure Active Directory public key. The key is used by the jwt.decode function from the pyjwat package to validate and decode the token.","title":"Discussion"},{"location":"python/misc/python-django-rest-framework-opeanapi-swagger-documentation/","text":"Add OpeanAPI documentation and SwaggerUI to your Django Rest Framework project Here is step-by-step recipe on how to add auto generated documentation for your REST API in OpenAPI format and enable SwaggerUI. The recipe is based on Document your API tutorial from Django Rest Framework. Step 1: Create documentation application python manage.py startapp docs Step 2. Register documentation endpoints Create urls.py in the api_docs application: from django.urls import path from django.views.generic import TemplateView from rest_framework.schemas import get_schema_view from rest_framework import renderers urlpatterns = [ path ( 'docs/' , TemplateView . as_view ( template_name = 'swagger-ui.html' , extra_context = { 'schema_url' : 'openapi-schema-yaml' } ), name = 'swagger-ui' ), path ( 'openapi.yaml' , get_schema_view ( title = \"Best API Service\" , renderer_classes = [ renderers . OpenAPIRenderer ] ), name = 'openapi-schema-yaml' ), path ( 'openapi.json' , get_schema_view ( title = \"Best API Service\" , renderer_classes = [ renderers . JSONOpenAPIRenderer ], ), name = 'openapi-schema-json' ), ] This will register three endpoints: /openapi.json - OpenAPI documentation in JSON format /openapi.yaml - OpenAPI documentation in YAML format /docs - Swagger UI, based on the openapi.yaml Step 3. Create the Swagger UI template Create a swagger-ui.html file in the api_docs application's templates directory: <!DOCTYPE html> < html > < head > < title > Swagger </ title > < meta charset = \"utf-8\" /> < meta name = \"viewport\" content = \"width=device-width, initial-scale=1\" > < link rel = \"stylesheet\" type = \"text/css\" href = \"//unpkg.com/swagger-ui-dist@3/swagger-ui.css\" /> </ head > < body > < div id = \"swagger-ui\" ></ div > < script src = \"//unpkg.com/swagger-ui-dist@3/swagger-ui-bundle.js\" ></ script > < script > const ui = SwaggerUIBundle ({ url : \"{% url schema_url %}\" , dom_id : '#swagger-ui' , presets : [ SwaggerUIBundle . presets . apis , SwaggerUIBundle . SwaggerUIStandalonePreset ], layout : \"BaseLayout\" , requestInterceptor : ( request ) => { request . headers [ 'X-CSRFToken' ] = \"{{ csrf_token }}\" return request ; } }) </ script > </ body > </ html > Step 4. Register the api_docs application with the Django project To register the api_docs application in the Django project you need to modify the project's settings.py : INSTALLED_APPS = [ # ... 'rest_framework' , 'docs' , # ... ] Step 5. Add documentation URLs to Django project urls Modify the urls.py file for your Django project. Make sure in contains include('api_docs.urls') : from django.urls import path , include urlpatterns = [ # ... path ( '' , include ( 'api_docs.urls' )), # ... ]","title":"Add OpeanAPI documentation and SwaggerUI to your Django Rest Framework project"},{"location":"python/misc/python-django-rest-framework-opeanapi-swagger-documentation/#add-opeanapi-documentation-and-swaggerui-to-your-django-rest-framework-project","text":"Here is step-by-step recipe on how to add auto generated documentation for your REST API in OpenAPI format and enable SwaggerUI. The recipe is based on Document your API tutorial from Django Rest Framework.","title":"Add OpeanAPI documentation and SwaggerUI to your Django Rest Framework project"},{"location":"python/misc/python-django-rest-framework-opeanapi-swagger-documentation/#step-1-create-documentation-application","text":"python manage.py startapp docs","title":"Step 1: Create documentation application"},{"location":"python/misc/python-django-rest-framework-opeanapi-swagger-documentation/#step-2-register-documentation-endpoints","text":"Create urls.py in the api_docs application: from django.urls import path from django.views.generic import TemplateView from rest_framework.schemas import get_schema_view from rest_framework import renderers urlpatterns = [ path ( 'docs/' , TemplateView . as_view ( template_name = 'swagger-ui.html' , extra_context = { 'schema_url' : 'openapi-schema-yaml' } ), name = 'swagger-ui' ), path ( 'openapi.yaml' , get_schema_view ( title = \"Best API Service\" , renderer_classes = [ renderers . OpenAPIRenderer ] ), name = 'openapi-schema-yaml' ), path ( 'openapi.json' , get_schema_view ( title = \"Best API Service\" , renderer_classes = [ renderers . JSONOpenAPIRenderer ], ), name = 'openapi-schema-json' ), ] This will register three endpoints: /openapi.json - OpenAPI documentation in JSON format /openapi.yaml - OpenAPI documentation in YAML format /docs - Swagger UI, based on the openapi.yaml","title":"Step 2. Register documentation endpoints"},{"location":"python/misc/python-django-rest-framework-opeanapi-swagger-documentation/#step-3-create-the-swagger-ui-template","text":"Create a swagger-ui.html file in the api_docs application's templates directory: <!DOCTYPE html> < html > < head > < title > Swagger </ title > < meta charset = \"utf-8\" /> < meta name = \"viewport\" content = \"width=device-width, initial-scale=1\" > < link rel = \"stylesheet\" type = \"text/css\" href = \"//unpkg.com/swagger-ui-dist@3/swagger-ui.css\" /> </ head > < body > < div id = \"swagger-ui\" ></ div > < script src = \"//unpkg.com/swagger-ui-dist@3/swagger-ui-bundle.js\" ></ script > < script > const ui = SwaggerUIBundle ({ url : \"{% url schema_url %}\" , dom_id : '#swagger-ui' , presets : [ SwaggerUIBundle . presets . apis , SwaggerUIBundle . SwaggerUIStandalonePreset ], layout : \"BaseLayout\" , requestInterceptor : ( request ) => { request . headers [ 'X-CSRFToken' ] = \"{{ csrf_token }}\" return request ; } }) </ script > </ body > </ html >","title":"Step 3. Create the Swagger UI template"},{"location":"python/misc/python-django-rest-framework-opeanapi-swagger-documentation/#step-4-register-the-api_docs-application-with-the-django-project","text":"To register the api_docs application in the Django project you need to modify the project's settings.py : INSTALLED_APPS = [ # ... 'rest_framework' , 'docs' , # ... ]","title":"Step 4. Register the api_docs application with the Django project"},{"location":"python/misc/python-django-rest-framework-opeanapi-swagger-documentation/#step-5-add-documentation-urls-to-django-project-urls","text":"Modify the urls.py file for your Django project. Make sure in contains include('api_docs.urls') : from django.urls import path , include urlpatterns = [ # ... path ( '' , include ( 'api_docs.urls' )), # ... ]","title":"Step 5. Add documentation URLs to Django project urls"},{"location":"python/misc/python-django-rest-framework-openapi-documentation/","text":"Document REST APIs based on Python Django's Rest Framework Function Views When using object-oriented implementation for your REST API, most of the documentation can be generated automatically from serializers and models you have created. When using function based views, you have almost no flexibility for creating OpenAPI documentation for your REST API. The typical signature for a view function is: from rest_framework.decorators import api_view @api_view ([ 'GET' , 'POST' ]) def hello_world ( request ): ''' get: get greeting post: post a greeting ''' return Response ({ \"message\" : \"Hello, world!\" }) The @api_view decorator wraps your view function into an APIView subclass. You can define description for all the methods, supported by the function in the function's docstring. This is practically all the supported documentation. Fortunately Django Rest Framework provides an extension point through the @schema decorator. I have created a quick and easy solution for OpenAPI generation, based on the @schema decorator. Custom AutoSchema class The solution parses the view function docstring and uses it in the process of the automatic documentation generation. The following AutoDocstringSchema class parses the yaml from the view docstring and returns the operations and components defined. To build the documentation, the AutoSchema methods are invoked first and the result is combined with the documentation from the docstring. If the docstring fails to parse as yaml, it is ignored. from rest_framework.schemas.openapi import AutoSchema class AutoDocstringSchema ( AutoSchema ): @property def documentation ( self ): if not hasattr ( self , '_documentation' ): try : self . _documentation = yaml . safe_load ( self . view . __doc__ ) except yaml . scanner . ScannerError : self . _documentation = {} return self . _documentation def get_components ( self , path , method ): components = super () . get_components ( path , method ) doc_components = self . documentation . get ( 'components' , {}) components . update ( doc_components ) return components def get_operation ( self , path , method ): operation = super () . get_operation ( path , method ) doc_operation = self . documentation . get ( method . lower (), {}) operation . update ( doc_operation ) return operation Here is an example of function views, documented using yaml docstring: from rest_framework.decorators import api_view , schema @api_view ([ 'GET' ]) @schema ( AutoDocstringSchema ()) def list_todos ( request ): ''' get: description: List todo items, stored in the database. summary: List todo items ''' # ... @api_view ([ 'GET' ]) @schema ( AutoDocstringSchema ()) def get_todo_item ( request , id ): ''' get: description: Retrieve a todo item definition summary: Retrieve todo item parameters: - name: id in: path description: ToDo item ID schema: type: string responses: '200': description: Todo item successfully retrieved content: 'application/json': {} ''' # ... Multiple request methods are supported. You can have a view function whish supports GET, POST, etc. requests at the same time. To document any of them, you define corresponding documentation, using lowercase method name. You can also add definitions for components. If your docstring has components section, the components defined in this section will be added to the API components. Reference Django Rest Framework - Schema Django Rest Framework - Document your API OpenAPI Specification v.3.0.2","title":"Document REST APIs based on Python Django's Rest Framework Function Views"},{"location":"python/misc/python-django-rest-framework-openapi-documentation/#document-rest-apis-based-on-python-djangos-rest-framework-function-views","text":"When using object-oriented implementation for your REST API, most of the documentation can be generated automatically from serializers and models you have created. When using function based views, you have almost no flexibility for creating OpenAPI documentation for your REST API. The typical signature for a view function is: from rest_framework.decorators import api_view @api_view ([ 'GET' , 'POST' ]) def hello_world ( request ): ''' get: get greeting post: post a greeting ''' return Response ({ \"message\" : \"Hello, world!\" }) The @api_view decorator wraps your view function into an APIView subclass. You can define description for all the methods, supported by the function in the function's docstring. This is practically all the supported documentation. Fortunately Django Rest Framework provides an extension point through the @schema decorator. I have created a quick and easy solution for OpenAPI generation, based on the @schema decorator.","title":"Document REST APIs based on Python Django's Rest Framework Function Views"},{"location":"python/misc/python-django-rest-framework-openapi-documentation/#custom-autoschema-class","text":"The solution parses the view function docstring and uses it in the process of the automatic documentation generation. The following AutoDocstringSchema class parses the yaml from the view docstring and returns the operations and components defined. To build the documentation, the AutoSchema methods are invoked first and the result is combined with the documentation from the docstring. If the docstring fails to parse as yaml, it is ignored. from rest_framework.schemas.openapi import AutoSchema class AutoDocstringSchema ( AutoSchema ): @property def documentation ( self ): if not hasattr ( self , '_documentation' ): try : self . _documentation = yaml . safe_load ( self . view . __doc__ ) except yaml . scanner . ScannerError : self . _documentation = {} return self . _documentation def get_components ( self , path , method ): components = super () . get_components ( path , method ) doc_components = self . documentation . get ( 'components' , {}) components . update ( doc_components ) return components def get_operation ( self , path , method ): operation = super () . get_operation ( path , method ) doc_operation = self . documentation . get ( method . lower (), {}) operation . update ( doc_operation ) return operation Here is an example of function views, documented using yaml docstring: from rest_framework.decorators import api_view , schema @api_view ([ 'GET' ]) @schema ( AutoDocstringSchema ()) def list_todos ( request ): ''' get: description: List todo items, stored in the database. summary: List todo items ''' # ... @api_view ([ 'GET' ]) @schema ( AutoDocstringSchema ()) def get_todo_item ( request , id ): ''' get: description: Retrieve a todo item definition summary: Retrieve todo item parameters: - name: id in: path description: ToDo item ID schema: type: string responses: '200': description: Todo item successfully retrieved content: 'application/json': {} ''' # ... Multiple request methods are supported. You can have a view function whish supports GET, POST, etc. requests at the same time. To document any of them, you define corresponding documentation, using lowercase method name. You can also add definitions for components. If your docstring has components section, the components defined in this section will be added to the API components.","title":"Custom AutoSchema class"},{"location":"python/misc/python-django-rest-framework-openapi-documentation/#reference","text":"Django Rest Framework - Schema Django Rest Framework - Document your API OpenAPI Specification v.3.0.2","title":"Reference"},{"location":"python/misc/python-pyodbc-truncate-table/","text":"Truncate table with pyodbc Problem You need to truncate a table using pyodbc . Solution Here is an example of a function to truncate a database table, using pyodbc connection. You can find the full source in GitHub . def truncate_table ( table_ref , dbc ): try : with dbc . cursor () as cursor : cursor . execute ( f 'TRUNCATE TABLE { table_ref } ' ) cursor . commit () except Exception as err : dbc . rollback () raise err Testing the solution Although the function is simple, it needs testing. The function should perform two steps: Truncate the table, executing TRUNCATE TABLE sql statement Commit the transaction This is the happy flow. In addition to the happy flow there is an exception flow which happens when pyodbc fails to execute the TRUNCATE TABLE sql statement. Here is a sample implementation of the unit tests that cover above scenarios. import unittest from unittest import mock from pyodbc_functions import truncate_table class Test_function_truncate_table ( unittest . TestCase ): def fix_dbc ( self ): dbc = mock . MagicMock () return dbc def test_truncate_table_calls_proper_methods_given_database_execute_is_successful ( self ): dbc = self . fix_dbc () truncate_table ( 'users' , dbc ) with dbc . cursor () as cursor : cursor . assert_has_calls ([ mock . call . execute ( 'TRUNCATE TABLE users' ), mock . call . commit () ]) def test_truncate_table_calls_rollback_on_and_propagates_exception_given_database_execute_fails ( self ): dbc = self . fix_dbc () with dbc . cursor () as cursor : cursor . execute . side_effect = Exception ( 'bad boy' ) with self . assertRaises ( Exception ) as excinfo : truncate_table ( 'users' , dbc ) self . assertEqual ( 'bad boy' , str ( excinfo . exception )) cursor . asset_has_calls ([ mock . call . execute ( mock . call . ANY ), mock . call . rollback () ]) We use mock database connection. In the happy flow test we pass mock database connection to the truncate_table function. Once the function is executed, we assert that following steps were made in a sequence: execute was called on the database cursor with proper SQL statement as argument commit with no arguments was called on the database cursor. In the exception flow test, we again use a mock database connection, but this time we configure the execute method of the cursor to throw an exception. We make sure the exception is propagated with assertRaises() unittest assert. We also check that the message of the exception is preserved. We verify that the flow is calling: execute method of the cursor - we do not verify the arguments since we already validated this in the happy flow. rollback method on the database connection. Discussion You can call the execute method on a connection cursor directly, but it is always better to move the code into a separate routine: provides reusability - you have a tested piece of code that can be used everywhere. improves the readability of the code - you create your own idioms or dictionary which make your code more expressive. improves the testability of the code - imagine your code truncates the table in the middle of 200+ line code fragment. How would you test it works correctly? How would you cover both scenarios? isolates your code from the external system - one of the benefits of this isolation is that you can unit test your code. You can find the pyodbc documentation here .","title":"Truncate table with pyodbc"},{"location":"python/misc/python-pyodbc-truncate-table/#truncate-table-with-pyodbc","text":"","title":"Truncate table with pyodbc"},{"location":"python/misc/python-pyodbc-truncate-table/#problem","text":"You need to truncate a table using pyodbc .","title":"Problem"},{"location":"python/misc/python-pyodbc-truncate-table/#solution","text":"Here is an example of a function to truncate a database table, using pyodbc connection. You can find the full source in GitHub . def truncate_table ( table_ref , dbc ): try : with dbc . cursor () as cursor : cursor . execute ( f 'TRUNCATE TABLE { table_ref } ' ) cursor . commit () except Exception as err : dbc . rollback () raise err","title":"Solution"},{"location":"python/misc/python-pyodbc-truncate-table/#testing-the-solution","text":"Although the function is simple, it needs testing. The function should perform two steps: Truncate the table, executing TRUNCATE TABLE sql statement Commit the transaction This is the happy flow. In addition to the happy flow there is an exception flow which happens when pyodbc fails to execute the TRUNCATE TABLE sql statement. Here is a sample implementation of the unit tests that cover above scenarios. import unittest from unittest import mock from pyodbc_functions import truncate_table class Test_function_truncate_table ( unittest . TestCase ): def fix_dbc ( self ): dbc = mock . MagicMock () return dbc def test_truncate_table_calls_proper_methods_given_database_execute_is_successful ( self ): dbc = self . fix_dbc () truncate_table ( 'users' , dbc ) with dbc . cursor () as cursor : cursor . assert_has_calls ([ mock . call . execute ( 'TRUNCATE TABLE users' ), mock . call . commit () ]) def test_truncate_table_calls_rollback_on_and_propagates_exception_given_database_execute_fails ( self ): dbc = self . fix_dbc () with dbc . cursor () as cursor : cursor . execute . side_effect = Exception ( 'bad boy' ) with self . assertRaises ( Exception ) as excinfo : truncate_table ( 'users' , dbc ) self . assertEqual ( 'bad boy' , str ( excinfo . exception )) cursor . asset_has_calls ([ mock . call . execute ( mock . call . ANY ), mock . call . rollback () ]) We use mock database connection. In the happy flow test we pass mock database connection to the truncate_table function. Once the function is executed, we assert that following steps were made in a sequence: execute was called on the database cursor with proper SQL statement as argument commit with no arguments was called on the database cursor. In the exception flow test, we again use a mock database connection, but this time we configure the execute method of the cursor to throw an exception. We make sure the exception is propagated with assertRaises() unittest assert. We also check that the message of the exception is preserved. We verify that the flow is calling: execute method of the cursor - we do not verify the arguments since we already validated this in the happy flow. rollback method on the database connection.","title":"Testing the solution"},{"location":"python/misc/python-pyodbc-truncate-table/#discussion","text":"You can call the execute method on a connection cursor directly, but it is always better to move the code into a separate routine: provides reusability - you have a tested piece of code that can be used everywhere. improves the readability of the code - you create your own idioms or dictionary which make your code more expressive. improves the testability of the code - imagine your code truncates the table in the middle of 200+ line code fragment. How would you test it works correctly? How would you cover both scenarios? isolates your code from the external system - one of the benefits of this isolation is that you can unit test your code. You can find the pyodbc documentation here .","title":"Discussion"},{"location":"python/misc/python-run-program-as-service/","text":"How to run a Python program as Linux Service Create Python program $ sudo vi /usr/bin/test_service.py import time import logging logging . basicConfig ( level = logging . INFO , filename = '/var/log/test-py.log' , filemode = 'a' , format = ' %(asctime)s : %(name)s : %(levelname)s : %(message)s ' , datefmt = ' %d -%b-%y %H:%M:%S' ) step = 0 while True : step += 1 logging . info ( f \"Processing step { step } .\" ) time . sleep ( 2 ) Create Service Definition $ sudo vi /lib/systemd/system/test-py.service [Unit] Description=Test Service After=multi-user.target Conflicts=getty@tty1.service [Service] Type=simple ExecStart=python3 /home/root/test_service.py >> /var/log/test-py.log StandardInput=tty-force [Install] WantedBy=multi-user.target Enable and Start the Service First you need to reload the systemctl daemon to read the new service definition. $ sudo systemctl daemon-reload In case you update the service definition, you need to reload the systemctl daemon for updates to take effect. Now you can enable and start the service: $ sudo systemctl enable test-py.service $ sudo systemctl start test-py.service You can monitor the log: $ sudo tail -f /var/log/test-py.log 03 -Jun-21 16 :43:52:root:INFO:Processing step 1 . 03 -Jun-21 16 :43:54:root:INFO:Processing step 2 . 03 -Jun-21 16 :43:56:root:INFO:Processing step 3 . 03 -Jun-21 16 :43:58:root:INFO:Processing step 4 . ... Manage the Service You can start, stop, restart the service or get the service status: $ sudo systemctl start test-py.service $ sudo systemctl status test-py.service $ sudo systemctl restart test-py.service $ sudo systemctl stop test-py.service","title":"How to run a Python program as Linux Service"},{"location":"python/misc/python-run-program-as-service/#how-to-run-a-python-program-as-linux-service","text":"","title":"How to run a Python program as Linux Service"},{"location":"python/misc/python-run-program-as-service/#create-python-program","text":"$ sudo vi /usr/bin/test_service.py import time import logging logging . basicConfig ( level = logging . INFO , filename = '/var/log/test-py.log' , filemode = 'a' , format = ' %(asctime)s : %(name)s : %(levelname)s : %(message)s ' , datefmt = ' %d -%b-%y %H:%M:%S' ) step = 0 while True : step += 1 logging . info ( f \"Processing step { step } .\" ) time . sleep ( 2 )","title":"Create Python program"},{"location":"python/misc/python-run-program-as-service/#create-service-definition","text":"$ sudo vi /lib/systemd/system/test-py.service [Unit] Description=Test Service After=multi-user.target Conflicts=getty@tty1.service [Service] Type=simple ExecStart=python3 /home/root/test_service.py >> /var/log/test-py.log StandardInput=tty-force [Install] WantedBy=multi-user.target","title":"Create Service Definition"},{"location":"python/misc/python-run-program-as-service/#enable-and-start-the-service","text":"First you need to reload the systemctl daemon to read the new service definition. $ sudo systemctl daemon-reload In case you update the service definition, you need to reload the systemctl daemon for updates to take effect. Now you can enable and start the service: $ sudo systemctl enable test-py.service $ sudo systemctl start test-py.service You can monitor the log: $ sudo tail -f /var/log/test-py.log 03 -Jun-21 16 :43:52:root:INFO:Processing step 1 . 03 -Jun-21 16 :43:54:root:INFO:Processing step 2 . 03 -Jun-21 16 :43:56:root:INFO:Processing step 3 . 03 -Jun-21 16 :43:58:root:INFO:Processing step 4 . ...","title":"Enable and Start the Service"},{"location":"python/misc/python-run-program-as-service/#manage-the-service","text":"You can start, stop, restart the service or get the service status: $ sudo systemctl start test-py.service $ sudo systemctl status test-py.service $ sudo systemctl restart test-py.service $ sudo systemctl stop test-py.service","title":"Manage the Service"},{"location":"python/tdd/","text":"Python Test Driven Developmen Assert custom objects are equal in Python unit test on August 15th, 2020, in Python :: Test Driven Development Unit testing for Python database applications on August 14th, 2020, in Python :: Test Driven Development","title":"Test Driven Development (TDD)"},{"location":"python/tdd/#python-test-driven-developmen","text":"Assert custom objects are equal in Python unit test on August 15th, 2020, in Python :: Test Driven Development Unit testing for Python database applications on August 14th, 2020, in Python :: Test Driven Development","title":"Python Test Driven Developmen"},{"location":"python/tdd/python-unittest-assert-custom-objects-are-equal/","text":"Assert custom objects are equal in Python unit test Problem You are creating a Python unit test, using unittest . Your test needs to assert that two custom objects are equal. For example, you might have following User class defined: class User : id : int name : str def __init__ ( self , id , name ): self . id = id self . name = name Trying to compare two User objects for equality, using assertEqual fails: >>> import unittest >>> test_case = unittest.TestCase() >>> expected = User(1, 'John') >>> actual = User(1, 'John') test_case.assertEqual(expected, actual) Traceback (most recent call last): ... AssertionError: <__main__.User object at 0x000002BC202AD888> != <__main__.User object at 0x000002BC2000B348> >>> Solution I will present you 3 different solutions to the problem. Implement equality interface Use addTypeEqualityFunc Use matcher Each solution is applicable to different situations. Implement equality interface Because unittest will try to perform Python's equality operator on your User objects, if the User class implements the equality operator interface, the equality assertion will work. The equality operator interface requires that the class implements a __eq__ method. class User : id : int name : str def __init__ ( self , id , name ): self . id = id self . name = name def __eq__ ( self , other ): return self . id == other . id and \\ self . name == other . name Now it is ok to use assertEqual : >>> import unittest >>> test_case = unittest.TestCase() >>> expected = User(1, 'John') >>> actual = User(1, 'John') >>> test_case.assertEqual(expected, actual) assertNotEqual also works fine: >>> import unittest >>> test_case = unittest.TestCase() >>> expected = User(1, 'John') >>> actual = User(1, 'Jane') >>> test_case.assertNotEqual(expected, actual) Using this approach solves our problem, but it has some limitations: We need to modify the User class source There is no way to have different equality assertions for different situations. For example, we might have situations where equality doesn't include the id attribute. Use addTypeEqualityFunc method Unittest TestCase class provides convenient way to override default Python equality operator by using the addTypeEqualityFunc : >>> import unittest >>> test_case = unittest.TestCase() >>> test_case.addTypeEqualityFunc(User, lambda first, second, msg: first.name == second.name ) >>> expected = User(1, 'John') actual = User(2, 'John') >>> test_case.assertEqual(expected, actual) The limitations of this approach: We have to use the same comparison function for a given type in all tests. For example, we might have tests where equality check requires id fields to be equal and other tests where id fields should not be compared. Both parameters to assertEqual have to be objects of the same type. Use matcher Another approach would be to create custom matcher object. class UserMatcher : expected : User def __init__ ( self , expected ): self . expected = expected def __eq__ ( self , other ): return self . expected . id == other . id and \\ self . expected . name == other . name Custom matcher could be any class that implements the equality operator. In other words, having the __eq__ method implemented. >>> test_case = unittest.TestCase() >>> expected = User(1, 'John') >>> actual = User(1, 'John') >>> test_case.assertEqual(UserMatcher(expected), actual) Although this is the most flexible approach, it is more verbose. Discussion In case of equality assertion fails, the output is not very useful: >>> test_case.assertEqual(UserMatcher(expected), actual) Traceback (most recent call last): ... AssertionError: <__main__.UserMatcher object at 0x000002BC20309BC8> != <__main__.User object at 0x000002BC20314388> You could improve this by implementing a __repr__ method of your custom class: class User : id : int name : str def __init__ ( self , id , name ): self . id = id self . name = name def __repr__ ( self ): return f \"User(id= { repr ( self . id ) } , name= { repr ( self . name ) } )\" In case you are using matcher, the matcher should also implement the __repr__ method: class UserMatcher : expected : User def __init__ ( self , expected ): self . expected = expected def __repr__ ( self ): return repr ( self . expected ) def __eq__ ( self , other ): return self . expected . id == other . id and \\ self . expected . name == other . name Now we will get much more readable and meaningful assertion failure message: >>> test_case.assertEqual(UserMatcher(expected), actual) ... AssertionError: User(id=1, name='John') != User(id=2, name='John')","title":"Assert custom objects are equal in Python unit test"},{"location":"python/tdd/python-unittest-assert-custom-objects-are-equal/#assert-custom-objects-are-equal-in-python-unit-test","text":"","title":"Assert custom objects are equal in Python unit test"},{"location":"python/tdd/python-unittest-assert-custom-objects-are-equal/#problem","text":"You are creating a Python unit test, using unittest . Your test needs to assert that two custom objects are equal. For example, you might have following User class defined: class User : id : int name : str def __init__ ( self , id , name ): self . id = id self . name = name Trying to compare two User objects for equality, using assertEqual fails: >>> import unittest >>> test_case = unittest.TestCase() >>> expected = User(1, 'John') >>> actual = User(1, 'John') test_case.assertEqual(expected, actual) Traceback (most recent call last): ... AssertionError: <__main__.User object at 0x000002BC202AD888> != <__main__.User object at 0x000002BC2000B348> >>>","title":"Problem"},{"location":"python/tdd/python-unittest-assert-custom-objects-are-equal/#solution","text":"I will present you 3 different solutions to the problem. Implement equality interface Use addTypeEqualityFunc Use matcher Each solution is applicable to different situations.","title":"Solution"},{"location":"python/tdd/python-unittest-assert-custom-objects-are-equal/#implement-equality-interface","text":"Because unittest will try to perform Python's equality operator on your User objects, if the User class implements the equality operator interface, the equality assertion will work. The equality operator interface requires that the class implements a __eq__ method. class User : id : int name : str def __init__ ( self , id , name ): self . id = id self . name = name def __eq__ ( self , other ): return self . id == other . id and \\ self . name == other . name Now it is ok to use assertEqual : >>> import unittest >>> test_case = unittest.TestCase() >>> expected = User(1, 'John') >>> actual = User(1, 'John') >>> test_case.assertEqual(expected, actual) assertNotEqual also works fine: >>> import unittest >>> test_case = unittest.TestCase() >>> expected = User(1, 'John') >>> actual = User(1, 'Jane') >>> test_case.assertNotEqual(expected, actual) Using this approach solves our problem, but it has some limitations: We need to modify the User class source There is no way to have different equality assertions for different situations. For example, we might have situations where equality doesn't include the id attribute.","title":"Implement equality interface"},{"location":"python/tdd/python-unittest-assert-custom-objects-are-equal/#use-addtypeequalityfunc-method","text":"Unittest TestCase class provides convenient way to override default Python equality operator by using the addTypeEqualityFunc : >>> import unittest >>> test_case = unittest.TestCase() >>> test_case.addTypeEqualityFunc(User, lambda first, second, msg: first.name == second.name ) >>> expected = User(1, 'John') actual = User(2, 'John') >>> test_case.assertEqual(expected, actual) The limitations of this approach: We have to use the same comparison function for a given type in all tests. For example, we might have tests where equality check requires id fields to be equal and other tests where id fields should not be compared. Both parameters to assertEqual have to be objects of the same type.","title":"Use addTypeEqualityFunc method"},{"location":"python/tdd/python-unittest-assert-custom-objects-are-equal/#use-matcher","text":"Another approach would be to create custom matcher object. class UserMatcher : expected : User def __init__ ( self , expected ): self . expected = expected def __eq__ ( self , other ): return self . expected . id == other . id and \\ self . expected . name == other . name Custom matcher could be any class that implements the equality operator. In other words, having the __eq__ method implemented. >>> test_case = unittest.TestCase() >>> expected = User(1, 'John') >>> actual = User(1, 'John') >>> test_case.assertEqual(UserMatcher(expected), actual) Although this is the most flexible approach, it is more verbose.","title":"Use matcher"},{"location":"python/tdd/python-unittest-assert-custom-objects-are-equal/#discussion","text":"In case of equality assertion fails, the output is not very useful: >>> test_case.assertEqual(UserMatcher(expected), actual) Traceback (most recent call last): ... AssertionError: <__main__.UserMatcher object at 0x000002BC20309BC8> != <__main__.User object at 0x000002BC20314388> You could improve this by implementing a __repr__ method of your custom class: class User : id : int name : str def __init__ ( self , id , name ): self . id = id self . name = name def __repr__ ( self ): return f \"User(id= { repr ( self . id ) } , name= { repr ( self . name ) } )\" In case you are using matcher, the matcher should also implement the __repr__ method: class UserMatcher : expected : User def __init__ ( self , expected ): self . expected = expected def __repr__ ( self ): return repr ( self . expected ) def __eq__ ( self , other ): return self . expected . id == other . id and \\ self . expected . name == other . name Now we will get much more readable and meaningful assertion failure message: >>> test_case.assertEqual(UserMatcher(expected), actual) ... AssertionError: User(id=1, name='John') != User(id=2, name='John')","title":"Discussion"},{"location":"python/tdd/python-unittest-database-applications/","text":"Unit testing for Python database applications Problem You are building an application that uses database in Python. For example, you might have created following function, which uses pyodbc to insert a list of rows into a database table. Each row is a dictionary. def insert_rows ( rows , table_name , dbc ): field_names = rows [ 0 ] . keys () field_names_str = ', ' . join ( field_names ) placeholder_str = ',' . join ( '?' * len ( field_names )) insert_sql = f 'INSERT INTO { table_name } ( { field_names_str } ) VALUES ( { placeholder_str } )' saved_autocommit = dbc . autocommit with dbc . cursor () as cursor : try : dbc . autocommit = False tuples = [ tuple (( row [ field_name ] for field_name in field_names )) for row in rows ] cursor . executemany ( insert_sql , tuples ) cursor . commit () except Exception as exc : cursor . rollback () raise exc finally : dbc . autocommit = saved_autocommit How I can unit test such a function? Solution Use unittest.mock to generate mock database connection. It is as simple as: dbc = mock . MagicMock () The very first test could be to verify that our function calls the cursor() method of the database connection. import unittest from unittest import mock class Test_insert_rows ( unittest . TestCase ): def fix_dbc ( self ): dbc = mock . MagicMock ( spec = [ 'cursor' ]) dbc . autocommit = True return dbc def fix_rows ( self ): rows = [{ 'id' : 1 , 'name' : 'John' }, { 'id' : 2 , 'name' : 'Jane' },] return rows def test_insert_rows_calls_cursor_method ( self ): dbc = self . fix_dbc () rows = self . fix_rows () insert_rows ( rows , 'users' , dbc ) self . assertTrue ( dbc . cursor . called ) if __name__ == '__main__' : unittest . main ( argv = [ '' , '-v' ]) Some highlights on what I have done in this test: The database connection, used for testing is created using a method. This is because I am gonna need this connection again and again in the test methods I am going to create. The rows fixture is also created using a method. For the same reason. At the end there is if __name__ == '__main__':... . This will run unittest if I execute the file as python script. Here is the result of the execution: (.venv37) sandbox>python test_insert_rows.py -v test_insert_rows_calls_cursor_method (test_insert_rows.Test_insert_rows) ... ok ---------------------------------------------------------------------- Ran 1 test in 0.002s OK The test I have created so far is not very exciting. To create a better test, let's look closely at what this function does: generates insert statement generates a list of tuples from the rows list calls the executemany method of the database cursor commits the transaction So my test could verify that the function calls the executemany method with correct arguments and commits the transaction. Let's implement this test. def fix_tuples ( self ): tuples = [( 1 , 'John' ), ( 2 , 'Jane' ),] return tuples def test_insert_rows_calls_executemany_and_commit_passing_correct_arguments ( self ): dbc = self . fix_dbc () rows = self . fix_rows () insert_rows ( rows , 'users' , dbc ) with dbc . cursor () as cursor : expect_sql = 'INSERT INTO users(id, name) VALUES (?,?)' expect_tuples = self . fix_tuples () calls = [ mock . call . executemany ( expect_sql , expect_tuples ), mock . call . commit (),] cursor . assert_has_calls ( calls ) self . assertTrue ( dbc . autocommit ) In this test I use the assert_has_calls assertion of the mock object to verify that specific calls has been made with expected arguments and in expected order. At the end of the test I verify that autocommit property of the database connection is restored to True . Ok. Great. So far we tested the happy path. What happens if something fails? In the following test I am gonna make the database connection raise an exception to test the behavior of my function. We have to verify: Transaction is rolled back after executemany is called. Database connection autocommit property has been restored. Exception is propagated. def test_insert_rows_rollsback_transaction_on_databse_exception ( self ): dbc = self . fix_dbc () rows = self . fix_rows () with dbc . cursor () as cursor : cursor . executemany . side_effect = Exception ( 'Some DB error' ) with self . assertRaises ( Exception ) as exc : insert_rows ( rows , 'users' , dbc ) calls = [ mock . call . executemany ( mock . ANY , mock . ANY ), mock . call . rollback (),] cursor . assert_has_calls ( calls ) self . assertTrue ( dbc . autocommit ) self . assertEqual ( 'Some DB error' , str ( exc . exception )) Discussion It might seem easier using real database, but it has drawbacks: Actual database might not be available or you might not have connectivity to it. You are not testing your code in isolation. If, for example, the database connection is unstable, you will start getting wired, unpredictable results. Database requests add significant latency. You need to reset the database state before each state to a fixed well-known state. This might be challenging, especially if the database is shared. Looking at our function, we could see a smell. It does more than one thing: Generate INSERT statement Generate tuples list Insert the tuples into the database There are also some conditions that might have been handled better. What if I pass empty list of rows? The good news is, we could refactor our code and make sure our code still works properly. We know we haven't broken anything. Just because we have thorough unit tests. Refactoring - Extract function Let's move the code for generating INSERT statement into a new function. We are going to follow these steps: Run our tests and make sure they pass (green). Create a test the new function. Run our tests and make sure they fail (red). Create a new function and copy the code we want to extract. Run our tests and make sure they pass (green). Replace the old code with a call to the new function. Run our tests and make sure they still pass (green). Let's implement the test: def test_make_insert_produces_correct_statement ( self ): fields = [ 'id' , 'name' ] actual = make_insert_statement ( fields , 'users' ) expected = 'INSERT INTO users(id, name) VALUES (?,?)' self . assertEqual ( expected , actual ) Tests are failing now: (.venv37) sandbox>python test_insert_rows.py test_insert_rows_calls_cursor_method (__main__.Test_insert_rows) ... ok test_insert_rows_calls_executemany_and_commit_passing_correct_arguments (__main__.Test_insert_rows) ... ok test_insert_rows_rollsback_transaction_on_databse_exception (__main__.Test_insert_rows) ... ok test_make_insert_produces_correct_statement (__main__.Test_insert_rows) ... ERROR ====================================================================== ERROR: test_make_insert_produces_correct_statement (__main__.Test_insert_rows) ---------------------------------------------------------------------- Traceback (most recent call last): File \"c:/Sandbox/Learn/Python/TestDrivenPythonDevelopment/play/sandbox/test_insert_rows.py\", line 42, in test_make_insert_produces_correct_statement actual = make_insert_statement(fields, 'users') NameError: name 'make_insert_statement' is not defined ---------------------------------------------------------------------- Ran 4 tests in 0.017s FAILED (errors=1) Now we implement the function: def make_insert_statement ( field_names , table_name ): field_names_str = ', ' . join ( field_names ) placeholder_str = ',' . join ( '?' * len ( field_names )) insert_sql = f 'INSERT INTO { table_name } ( { field_names_str } ) VALUES ( { placeholder_str } )' return insert_sql Tests are passing. We update the insert_rows function to call the new make_insert_statement function and run the tests to see them passing. def insert_rows ( rows , table_name , dbc ): field_names = rows [ 0 ] . keys () insert_sql = make_insert_statement ( field_names , table_name ) saved_autocommit = dbc . autocommit with dbc . cursor () as cursor : try : dbc . autocommit = False tuples = [ tuple (( row [ field_name ] for field_name in field_names )) for row in rows ] cursor . executemany ( insert_sql , tuples ) cursor . commit () except Exception as exc : cursor . rollback () raise exc finally : dbc . autocommit = saved_autocommit The new version of the function is not much shorter, but has some advantages: improved readability - it is much easier to understand what the code is doing improved reusability - it is very likely that we might need the INSERT statement generation in another situation better testability - we could test the generation of the INSERT statement in isolation. Introducing new test cases for this functionality is easy. It doesn't require database connection, for example. If we follow the principles of the test driven development (TDD), we should remove the check for the generated statement in the call to the insertmany . We could achieve this by patching the make_insert_statement function. @mock . patch ( '__main__.make_insert_statement' ) def test_insert_rows_calls_executemany_and_commit_passing_correct_arguments ( self , make_insert_mock ): dbc = self . fix_dbc () rows = self . fix_rows () make_insert_mock . return_value = 'MY PRECIOUS' insert_rows ( rows , 'users' , dbc ) with dbc . cursor () as cursor : expect_sql = 'MY PRECIOUS' expect_tuples = self . fix_tuples () calls = [ mock . call . executemany ( expect_sql , expect_tuples ), mock . call . commit (),] cursor . assert_has_calls ( calls ) self . assertTrue ( dbc . autocommit ) Wait! What is going here? I used the patch decorator to replace the make_insert_statement with a mock object. The mock object is automatically added as second argument to my test method. I have also defined that the make_insert_statement mock returns a fixed value MY PRECIOS . It is not valid SQL, but our mock database connection doesn't care. The important thing is that we see the result from the make_insert_statement passed to the executemany method.","title":"Unit testing for Python database applications"},{"location":"python/tdd/python-unittest-database-applications/#unit-testing-for-python-database-applications","text":"","title":"Unit testing for Python database applications"},{"location":"python/tdd/python-unittest-database-applications/#problem","text":"You are building an application that uses database in Python. For example, you might have created following function, which uses pyodbc to insert a list of rows into a database table. Each row is a dictionary. def insert_rows ( rows , table_name , dbc ): field_names = rows [ 0 ] . keys () field_names_str = ', ' . join ( field_names ) placeholder_str = ',' . join ( '?' * len ( field_names )) insert_sql = f 'INSERT INTO { table_name } ( { field_names_str } ) VALUES ( { placeholder_str } )' saved_autocommit = dbc . autocommit with dbc . cursor () as cursor : try : dbc . autocommit = False tuples = [ tuple (( row [ field_name ] for field_name in field_names )) for row in rows ] cursor . executemany ( insert_sql , tuples ) cursor . commit () except Exception as exc : cursor . rollback () raise exc finally : dbc . autocommit = saved_autocommit How I can unit test such a function?","title":"Problem"},{"location":"python/tdd/python-unittest-database-applications/#solution","text":"Use unittest.mock to generate mock database connection. It is as simple as: dbc = mock . MagicMock () The very first test could be to verify that our function calls the cursor() method of the database connection. import unittest from unittest import mock class Test_insert_rows ( unittest . TestCase ): def fix_dbc ( self ): dbc = mock . MagicMock ( spec = [ 'cursor' ]) dbc . autocommit = True return dbc def fix_rows ( self ): rows = [{ 'id' : 1 , 'name' : 'John' }, { 'id' : 2 , 'name' : 'Jane' },] return rows def test_insert_rows_calls_cursor_method ( self ): dbc = self . fix_dbc () rows = self . fix_rows () insert_rows ( rows , 'users' , dbc ) self . assertTrue ( dbc . cursor . called ) if __name__ == '__main__' : unittest . main ( argv = [ '' , '-v' ]) Some highlights on what I have done in this test: The database connection, used for testing is created using a method. This is because I am gonna need this connection again and again in the test methods I am going to create. The rows fixture is also created using a method. For the same reason. At the end there is if __name__ == '__main__':... . This will run unittest if I execute the file as python script. Here is the result of the execution: (.venv37) sandbox>python test_insert_rows.py -v test_insert_rows_calls_cursor_method (test_insert_rows.Test_insert_rows) ... ok ---------------------------------------------------------------------- Ran 1 test in 0.002s OK The test I have created so far is not very exciting. To create a better test, let's look closely at what this function does: generates insert statement generates a list of tuples from the rows list calls the executemany method of the database cursor commits the transaction So my test could verify that the function calls the executemany method with correct arguments and commits the transaction. Let's implement this test. def fix_tuples ( self ): tuples = [( 1 , 'John' ), ( 2 , 'Jane' ),] return tuples def test_insert_rows_calls_executemany_and_commit_passing_correct_arguments ( self ): dbc = self . fix_dbc () rows = self . fix_rows () insert_rows ( rows , 'users' , dbc ) with dbc . cursor () as cursor : expect_sql = 'INSERT INTO users(id, name) VALUES (?,?)' expect_tuples = self . fix_tuples () calls = [ mock . call . executemany ( expect_sql , expect_tuples ), mock . call . commit (),] cursor . assert_has_calls ( calls ) self . assertTrue ( dbc . autocommit ) In this test I use the assert_has_calls assertion of the mock object to verify that specific calls has been made with expected arguments and in expected order. At the end of the test I verify that autocommit property of the database connection is restored to True . Ok. Great. So far we tested the happy path. What happens if something fails? In the following test I am gonna make the database connection raise an exception to test the behavior of my function. We have to verify: Transaction is rolled back after executemany is called. Database connection autocommit property has been restored. Exception is propagated. def test_insert_rows_rollsback_transaction_on_databse_exception ( self ): dbc = self . fix_dbc () rows = self . fix_rows () with dbc . cursor () as cursor : cursor . executemany . side_effect = Exception ( 'Some DB error' ) with self . assertRaises ( Exception ) as exc : insert_rows ( rows , 'users' , dbc ) calls = [ mock . call . executemany ( mock . ANY , mock . ANY ), mock . call . rollback (),] cursor . assert_has_calls ( calls ) self . assertTrue ( dbc . autocommit ) self . assertEqual ( 'Some DB error' , str ( exc . exception ))","title":"Solution"},{"location":"python/tdd/python-unittest-database-applications/#discussion","text":"It might seem easier using real database, but it has drawbacks: Actual database might not be available or you might not have connectivity to it. You are not testing your code in isolation. If, for example, the database connection is unstable, you will start getting wired, unpredictable results. Database requests add significant latency. You need to reset the database state before each state to a fixed well-known state. This might be challenging, especially if the database is shared. Looking at our function, we could see a smell. It does more than one thing: Generate INSERT statement Generate tuples list Insert the tuples into the database There are also some conditions that might have been handled better. What if I pass empty list of rows? The good news is, we could refactor our code and make sure our code still works properly. We know we haven't broken anything. Just because we have thorough unit tests.","title":"Discussion"},{"location":"python/tdd/python-unittest-database-applications/#refactoring-extract-function","text":"Let's move the code for generating INSERT statement into a new function. We are going to follow these steps: Run our tests and make sure they pass (green). Create a test the new function. Run our tests and make sure they fail (red). Create a new function and copy the code we want to extract. Run our tests and make sure they pass (green). Replace the old code with a call to the new function. Run our tests and make sure they still pass (green). Let's implement the test: def test_make_insert_produces_correct_statement ( self ): fields = [ 'id' , 'name' ] actual = make_insert_statement ( fields , 'users' ) expected = 'INSERT INTO users(id, name) VALUES (?,?)' self . assertEqual ( expected , actual ) Tests are failing now: (.venv37) sandbox>python test_insert_rows.py test_insert_rows_calls_cursor_method (__main__.Test_insert_rows) ... ok test_insert_rows_calls_executemany_and_commit_passing_correct_arguments (__main__.Test_insert_rows) ... ok test_insert_rows_rollsback_transaction_on_databse_exception (__main__.Test_insert_rows) ... ok test_make_insert_produces_correct_statement (__main__.Test_insert_rows) ... ERROR ====================================================================== ERROR: test_make_insert_produces_correct_statement (__main__.Test_insert_rows) ---------------------------------------------------------------------- Traceback (most recent call last): File \"c:/Sandbox/Learn/Python/TestDrivenPythonDevelopment/play/sandbox/test_insert_rows.py\", line 42, in test_make_insert_produces_correct_statement actual = make_insert_statement(fields, 'users') NameError: name 'make_insert_statement' is not defined ---------------------------------------------------------------------- Ran 4 tests in 0.017s FAILED (errors=1) Now we implement the function: def make_insert_statement ( field_names , table_name ): field_names_str = ', ' . join ( field_names ) placeholder_str = ',' . join ( '?' * len ( field_names )) insert_sql = f 'INSERT INTO { table_name } ( { field_names_str } ) VALUES ( { placeholder_str } )' return insert_sql Tests are passing. We update the insert_rows function to call the new make_insert_statement function and run the tests to see them passing. def insert_rows ( rows , table_name , dbc ): field_names = rows [ 0 ] . keys () insert_sql = make_insert_statement ( field_names , table_name ) saved_autocommit = dbc . autocommit with dbc . cursor () as cursor : try : dbc . autocommit = False tuples = [ tuple (( row [ field_name ] for field_name in field_names )) for row in rows ] cursor . executemany ( insert_sql , tuples ) cursor . commit () except Exception as exc : cursor . rollback () raise exc finally : dbc . autocommit = saved_autocommit The new version of the function is not much shorter, but has some advantages: improved readability - it is much easier to understand what the code is doing improved reusability - it is very likely that we might need the INSERT statement generation in another situation better testability - we could test the generation of the INSERT statement in isolation. Introducing new test cases for this functionality is easy. It doesn't require database connection, for example. If we follow the principles of the test driven development (TDD), we should remove the check for the generated statement in the call to the insertmany . We could achieve this by patching the make_insert_statement function. @mock . patch ( '__main__.make_insert_statement' ) def test_insert_rows_calls_executemany_and_commit_passing_correct_arguments ( self , make_insert_mock ): dbc = self . fix_dbc () rows = self . fix_rows () make_insert_mock . return_value = 'MY PRECIOUS' insert_rows ( rows , 'users' , dbc ) with dbc . cursor () as cursor : expect_sql = 'MY PRECIOUS' expect_tuples = self . fix_tuples () calls = [ mock . call . executemany ( expect_sql , expect_tuples ), mock . call . commit (),] cursor . assert_has_calls ( calls ) self . assertTrue ( dbc . autocommit ) Wait! What is going here? I used the patch decorator to replace the make_insert_statement with a mock object. The mock object is automatically added as second argument to my test method. I have also defined that the make_insert_statement mock returns a fixed value MY PRECIOS . It is not valid SQL, but our mock database connection doesn't care. The important thing is that we see the result from the make_insert_statement passed to the executemany method.","title":"Refactoring - Extract function"},{"location":"python/tricks/","text":"Python tricks Unpacking a Python sequence into variables on August 19th, 2020, in Python :: Tricks","title":"Tricks"},{"location":"python/tricks/#python-tricks","text":"Unpacking a Python sequence into variables on August 19th, 2020, in Python :: Tricks","title":"Python tricks"},{"location":"python/tricks/python-trick-unpack-a-sequence-into-variables/","text":"Unpacking a Python sequence into variables Problem You have a list, tuple or another sequence that you want to unpack into a collection of variables. Solution You use assignment operator. On the left side of the operator you define a tuple with the variables. On the right side of the operator is the sequence. The only requirement is that the number of variables and structures match the sequence. Example: >>> p = ( 5 , 6 ) >>> x , y = p >>> x 5 >>> y 6 Elements of the sequence could be any objects - integer, sequence, etc. Here is an example: >>> data = [ 'ACME' , 50 , 91.1 , ( 2012 , 12 , 21 )] >>> name , shares , price , date = data >>> name 'ACME' >>> shares 50 >>> price 91.1 >>> date ( 2012 , 12 , 21 ) Discussion In fact unpacking works with any iterable Python object. This includes strings, files, iterators, and generators. Sometimes in your assignment, you want to ignore some of the values at certain places from the sequence. Python doesn't have a special syntax for this. Common practice is to use special variable name, e.g. underscore _ , for places you want to ignore. >>> data = [ 'ACME' , 50 , 91.1 , ( 2012 , 12 , 21 )] >>> name , _ , _ , date = data >>> name 'ACME' >>> date ( 2012 , 12 , 21 )","title":"Unpacking a Python sequence into variables"},{"location":"python/tricks/python-trick-unpack-a-sequence-into-variables/#unpacking-a-python-sequence-into-variables","text":"","title":"Unpacking a Python sequence into variables"},{"location":"python/tricks/python-trick-unpack-a-sequence-into-variables/#problem","text":"You have a list, tuple or another sequence that you want to unpack into a collection of variables.","title":"Problem"},{"location":"python/tricks/python-trick-unpack-a-sequence-into-variables/#solution","text":"You use assignment operator. On the left side of the operator you define a tuple with the variables. On the right side of the operator is the sequence. The only requirement is that the number of variables and structures match the sequence. Example: >>> p = ( 5 , 6 ) >>> x , y = p >>> x 5 >>> y 6 Elements of the sequence could be any objects - integer, sequence, etc. Here is an example: >>> data = [ 'ACME' , 50 , 91.1 , ( 2012 , 12 , 21 )] >>> name , shares , price , date = data >>> name 'ACME' >>> shares 50 >>> price 91.1 >>> date ( 2012 , 12 , 21 )","title":"Solution"},{"location":"python/tricks/python-trick-unpack-a-sequence-into-variables/#discussion","text":"In fact unpacking works with any iterable Python object. This includes strings, files, iterators, and generators. Sometimes in your assignment, you want to ignore some of the values at certain places from the sequence. Python doesn't have a special syntax for this. Common practice is to use special variable name, e.g. underscore _ , for places you want to ignore. >>> data = [ 'ACME' , 50 , 91.1 , ( 2012 , 12 , 21 )] >>> name , _ , _ , date = data >>> name 'ACME' >>> date ( 2012 , 12 , 21 )","title":"Discussion"},{"location":"sqlserver/","text":"SQL Server (MSSQL) Zone Recipes List Blocking Locks in SQL Server List roles assigned to a principal / user in SQL Server (MSSQL) Database","title":"SQL Server"},{"location":"sqlserver/#sql-server-mssql-zone","text":"","title":"SQL Server (MSSQL) Zone"},{"location":"sqlserver/#recipes","text":"List Blocking Locks in SQL Server List roles assigned to a principal / user in SQL Server (MSSQL) Database","title":"Recipes"},{"location":"sqlserver/sql-server-list-blocking-locks/","text":"List blocking locks in SQL Server (MSSQL) Database Problem You use SQL Server and you need to know which sessions are blocked and for what reason. Solution Use the following query to get a list of blocked sessions. SELECT p . cmd , p . * FROM sys . sysprocesses p WHERE blocked > 0 You can find this solution also as GitHub gist list-blocking-locks.sql .","title":"List blocking locks in SQL Server (MSSQL) Database"},{"location":"sqlserver/sql-server-list-blocking-locks/#list-blocking-locks-in-sql-server-mssql-database","text":"","title":"List blocking locks in SQL Server (MSSQL) Database"},{"location":"sqlserver/sql-server-list-blocking-locks/#problem","text":"You use SQL Server and you need to know which sessions are blocked and for what reason.","title":"Problem"},{"location":"sqlserver/sql-server-list-blocking-locks/#solution","text":"Use the following query to get a list of blocked sessions. SELECT p . cmd , p . * FROM sys . sysprocesses p WHERE blocked > 0 You can find this solution also as GitHub gist list-blocking-locks.sql .","title":"Solution"},{"location":"sqlserver/sql-server-list-find-roles-assigned-user-principalblocking-locks/","text":"List or find roles assigned to a principal / user in SQL Server (MSSQL) Database Problem You use SQL Server. You to know which roles were granted to which users (database principals). Solution To find all the role assignments to users in SQL Server database, you can use the following query. SELECT r . name role_principal_name , m . name AS member_principal_name FROM sys . database_role_members rm JOIN sys . database_principals r ON rm . role_principal_id = r . principal_id JOIN sys . database_principals m ON rm . member_principal_id = m . principal_id WHERE r . type = 'R' ; You can also limit the list of roles to only the roles, assigned to a particular user or principal by adding a filtering condition to the WHERE clause. DECLARE @ PrincipalName VARCHAR ( 128 ) = 'principal-name-here' SELECT r . name role_principal_name , m . name AS member_principal_name FROM sys . database_role_members rm JOIN sys . database_principals r ON rm . role_principal_id = r . principal_id JOIN sys . database_principals m ON rm . member_principal_id = m . principal_id WHERE r . type = 'R' AND m . name = @ PrincipalName ; You can find this solution also as GitHub gist list-principal-roles.sql .","title":"List or find roles assigned to a principal / user in SQL Server (MSSQL) Database"},{"location":"sqlserver/sql-server-list-find-roles-assigned-user-principalblocking-locks/#list-or-find-roles-assigned-to-a-principal-user-in-sql-server-mssql-database","text":"","title":"List or find roles assigned to a principal / user in SQL Server (MSSQL) Database"},{"location":"sqlserver/sql-server-list-find-roles-assigned-user-principalblocking-locks/#problem","text":"You use SQL Server. You to know which roles were granted to which users (database principals).","title":"Problem"},{"location":"sqlserver/sql-server-list-find-roles-assigned-user-principalblocking-locks/#solution","text":"To find all the role assignments to users in SQL Server database, you can use the following query. SELECT r . name role_principal_name , m . name AS member_principal_name FROM sys . database_role_members rm JOIN sys . database_principals r ON rm . role_principal_id = r . principal_id JOIN sys . database_principals m ON rm . member_principal_id = m . principal_id WHERE r . type = 'R' ; You can also limit the list of roles to only the roles, assigned to a particular user or principal by adding a filtering condition to the WHERE clause. DECLARE @ PrincipalName VARCHAR ( 128 ) = 'principal-name-here' SELECT r . name role_principal_name , m . name AS member_principal_name FROM sys . database_role_members rm JOIN sys . database_principals r ON rm . role_principal_id = r . principal_id JOIN sys . database_principals m ON rm . member_principal_id = m . principal_id WHERE r . type = 'R' AND m . name = @ PrincipalName ; You can find this solution also as GitHub gist list-principal-roles.sql .","title":"Solution"}]}